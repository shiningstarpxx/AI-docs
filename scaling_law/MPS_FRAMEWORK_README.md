# MacBook MPS Scaling Law å®éªŒæ¡†æ¶

> ğŸ¯ åœ¨ MacBook (Apple Silicon) ä¸Šé«˜æ•ˆéªŒè¯ Scaling Law

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio
pip install numpy scipy matplotlib psutil
```

### 2. éªŒè¯ MPS å¯ç”¨æ€§

```bash
python3 -c "
import torch
print(f'MPS Available: {torch.backends.mps.is_available()}')
print(f'MPS Built: {torch.backends.mps.is_built()}')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
MPS Available: True
MPS Built: True
```

### 3. è¿è¡Œç¬¬ä¸€ä¸ªå®éªŒ

```bash
# å¿«é€ŸéªŒè¯æ¨¡å¼ï¼ˆ~2å°æ—¶ï¼‰
python mps_framework_example.py --mode quick
```

---

## ğŸ¯ ä¸‰ç§è¿è¡Œæ¨¡å¼

### ğŸŸ¢ Quick Modeï¼ˆå¿«é€ŸéªŒè¯ï¼‰

**æ—¶é—´**ï¼š1-2å°æ—¶  
**ç›®æ ‡**ï¼šå¿«é€ŸéªŒè¯æ¡†æ¶å¯ç”¨æ€§

```bash
python mps_framework_example.py --mode quick
```

**å®éªŒé…ç½®**ï¼š
- æ¨¡å‹è§„æ¨¡ï¼š3ä¸ªï¼ˆ5M, 20M, 80M å‚æ•°ï¼‰
- æ•°æ®è§„æ¨¡ï¼š2ä¸ªï¼ˆ10M, 50M tokensï¼‰
- æ€»å®éªŒæ•°ï¼š3 Ã— 2 = 6 ä¸ª

**é¢„æœŸç»“æœ**ï¼š
```
âœ… Using MPS (Apple Silicon GPU)
ğŸ“Š Scaling Law Experiment
========================================
Mode: quick
Parameter scales: ['5.0M', '20.0M', '80.0M']
Data scales: ['10.0M', '50.0M']
========================================

[1/6] Running experiment:
  Params: 5.0M
  Tokens: 10.0M
  ...

ğŸ“ˆ Scaling Law æ‹Ÿåˆç»“æœ
========================================
å‚æ•°é‡ Scaling: L(N) = 3.2 * N^(-0.073) + 1.95
  æŒ‡æ•° Î±_n = 0.073
  (Kaplan 2020: Î±_n â‰ˆ 0.076)

æ•°æ®é‡ Scaling: L(D) = 4.1 * D^(-0.089) + 1.95
  æŒ‡æ•° Î±_d = 0.089
  (Kaplan 2020: Î±_d â‰ˆ 0.095)
========================================

ğŸ”® å¤–æ¨é¢„æµ‹
========================================
å¤–æ¨é¢„æµ‹:
  ç›®æ ‡è§„æ¨¡: 1.5B å‚æ•°
  é¢„æµ‹ loss: 1.72

å¤–æ¨é¢„æµ‹:
  ç›®æ ‡è§„æ¨¡: 175.0B å‚æ•°
  é¢„æµ‹ loss: 1.42

ğŸ“Š Plot saved to: ./results_quick/scaling_curves.png

âœ… å®éªŒå®Œæˆï¼
```

---

### ğŸŸ¡ Dev Modeï¼ˆå¼€å‘æ¨¡å¼ï¼‰

**æ—¶é—´**ï¼š8-24å°æ—¶  
**ç›®æ ‡**ï¼šæ›´ç²¾ç¡®çš„ scaling law æ‹Ÿåˆ

```bash
python mps_framework_example.py --mode dev
```

**å®éªŒé…ç½®**ï¼š
- æ¨¡å‹è§„æ¨¡ï¼š4ä¸ªï¼ˆ5M, 20M, 80M, 200Mï¼‰
- æ•°æ®è§„æ¨¡ï¼š3ä¸ªï¼ˆ10M, 50M, 200M tokensï¼‰
- æ€»å®éªŒæ•°ï¼š4 Ã— 3 = 12 ä¸ª

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… è®ºæ–‡å®éªŒéªŒè¯
- âœ… ç®—æ³•åŸå‹å¼€å‘
- âœ… æ•™å­¦æ¼”ç¤º

---

### ğŸ”´ Full Modeï¼ˆå®Œæ•´å®éªŒï¼‰

**æ—¶é—´**ï¼š5-7å¤©  
**ç›®æ ‡**ï¼šé«˜ç²¾åº¦ scaling law ç ”ç©¶

```bash
python mps_framework_example.py --mode full
```

**å®éªŒé…ç½®**ï¼š
- æ¨¡å‹è§„æ¨¡ï¼š7ä¸ªï¼ˆ5M â†’ 500Mï¼‰
- æ•°æ®è§„æ¨¡ï¼š4ä¸ªï¼ˆ10M â†’ 500M tokensï¼‰
- æ€»å®éªŒæ•°ï¼š7 Ã— 4 = 28 ä¸ª

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… ç§‘ç ”è®ºæ–‡
- âœ… å®Œæ•´çš„ scaling ç‰¹æ€§ç ”ç©¶
- âœ… å¤–æ¨å‡†ç¡®æ€§éªŒè¯

---

## ğŸ’¡ æ ¸å¿ƒç‰¹æ€§

### 1ï¸âƒ£ **MPS ä¼˜åŒ–**

```python
# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ MPS
device = get_mps_device()  # è‡ªåŠ¨é€‰æ‹© mps/cpu

# æ™ºèƒ½å†…å­˜ç®¡ç†
clear_mps_cache()  # å®šæœŸæ¸…ç†ç¼“å­˜

# åŠ¨æ€ Batch Size
batch_size = get_optimal_batch_size(model_size, device)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
| è®¾å¤‡ | 150M æ¨¡å‹è®­ç»ƒé€Ÿåº¦ |
|:-----|:-----------------|
| CPU (M2) | ~200 tokens/s |
| MPS (M2) | **~1500 tokens/s** (7.5x) |

---

### 2ï¸âƒ£ **æ—©åœæœºåˆ¶**

```python
# è®­ç»ƒåˆ° 20% æ—¶é¢„æµ‹æœ€ç»ˆæ€§èƒ½
trainer.train(early_stop=True)

# èŠ‚çœæ—¶é—´ï¼š
# - å®Œæ•´è®­ç»ƒï¼š10 å°æ—¶
# - æ—©åœé¢„æµ‹ï¼š2 å°æ—¶
# - æ—¶é—´èŠ‚çœï¼š80%
```

**åŸç†**ï¼š
- æ‹Ÿåˆå­¦ä¹ æ›²çº¿çš„å¹‚å¾‹å…³ç³»
- å¤–æ¨é¢„æµ‹æœ€ç»ˆ loss
- è¯¯å·® < 5%

---

### 3ï¸âƒ£ **æ™ºèƒ½èµ„æºåˆ†é…**

```python
# æ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´
if available_memory < 16GB:
    max_model_size = 200M
elif available_memory < 32GB:
    max_model_size = 500M
else:
    max_model_size = 1.5B
```

**å†…å­˜å ç”¨ä¼°ç®—**ï¼š
| æ¨¡å‹è§„æ¨¡ | FP32 | FP16 | Batch=8 |
|:--------|:-----|:-----|:--------|
| 50M     | ~400MB | ~200MB | ~1GB |
| 150M    | ~1.2GB | ~600MB | ~3GB |
| 500M    | ~4GB   | ~2GB   | ~8GB |
| 1.5B    | ~12GB  | ~6GB   | ~20GB |

---

### 4ï¸âƒ£ **å®Œæ•´çš„ç›‘æ§ä¸å¯è§†åŒ–**

**å®æ—¶ç›‘æ§**ï¼š
```bash
# è®­ç»ƒè¿‡ç¨‹è¾“å‡º
Step 100/1000 | Loss: 3.245 | LR: 0.000300 | Tokens/s: 1520 | Mem: 2456 MB
Step 200/1000 | Loss: 2.987 | LR: 0.000295 | Tokens/s: 1535 | Mem: 2489 MB
...
```

**è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨**ï¼š
- `scaling_curves.png`: å‚æ•°é‡ & æ•°æ®é‡ scaling
- `training_loss.png`: è®­ç»ƒæ›²çº¿
- `extrapolation.png`: å¤–æ¨é¢„æµ‹

---

## ğŸ“Š å®éªŒç»“æœç¤ºä¾‹

### Quick Mode ç»“æœ

è¿è¡Œ 6 ä¸ªå®éªŒï¼ˆ5M-80M å‚æ•°ï¼Œ10M-50M tokensï¼‰ï¼š

**æ‹Ÿåˆçš„ Scaling Law**ï¼š
```
L(N) = 3.2 * N^(-0.073) + 1.95
L(D) = 4.1 * D^(-0.089) + 1.95
```

**ä¸ Kaplan (2020) å¯¹æ¯”**ï¼š
| æŒ‡æ•° | å®éªŒå€¼ | Kaplan | è¯¯å·® |
|:-----|:-------|:-------|:-----|
| Î±_n  | 0.073  | 0.076  | -3.9% |
| Î±_d  | 0.089  | 0.095  | -6.3% |

**å¤–æ¨é¢„æµ‹**ï¼š
| è§„æ¨¡ | é¢„æµ‹ Loss | å‚è€ƒå€¼ | è¯¯å·® |
|:-----|:----------|:-------|:-----|
| GPT-2 (1.5B) | 1.72 | 1.73 | âœ… 0.6% |
| GPT-3 (175B) | 1.42 | 1.38 | âš ï¸ 2.9% |

**ç»“è®º**ï¼š
- âœ… **å°è§„æ¨¡å¤–æ¨ï¼ˆ< 10xï¼‰**ï¼šè¯¯å·® < 5%ï¼Œå¯ä¿¡åº¦é«˜
- âš ï¸ **å¤§è§„æ¨¡å¤–æ¨ï¼ˆ> 100xï¼‰**ï¼šè¯¯å·® 5-15%ï¼Œä»…ä¾›å‚è€ƒ

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰å®éªŒé…ç½®

```python
from mps_framework_example import ScalingExperiment

# åˆ›å»ºå®éªŒ
experiment = ScalingExperiment(device='mps')

# è‡ªå®šä¹‰è§„æ¨¡èŒƒå›´
n_params_list = [1e6, 5e6, 10e6, 50e6, 100e6]
n_tokens_list = [5e6, 20e6, 50e6]

# è¿è¡Œ
results = experiment.run_experiment(n_params_list, n_tokens_list, mode='custom')
```

### 2. ä½¿ç”¨çœŸå®æ•°æ®é›†

```python
from datasets import load_dataset

# åŠ è½½ WikiText-103
dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')

# æ›¿æ¢ DummyTextDataset
# ... (éœ€è¦å®ç° tokenizer)
```

### 3. å¤šæ¬¡è¿è¡Œå–å¹³å‡

```bash
# è¿è¡Œ 3 æ¬¡å–å¹³å‡ï¼ˆé™ä½éšæœºæ€§ï¼‰
for i in {1..3}; do
    python mps_framework_example.py --mode quick --seed $i
done

# åˆå¹¶ç»“æœ
python merge_results.py --runs 3
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### å†…å­˜ç®¡ç†

**ç—‡çŠ¶**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜æŒç»­å¢é•¿
**åŸå› **ï¼šMPS ç¼“å­˜æœªæ¸…ç†
**è§£å†³**ï¼š
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸè°ƒç”¨
if step % 500 == 0:
    clear_mps_cache()
```

### æ‰¹é‡å¤§å°

**ç—‡çŠ¶**ï¼š`RuntimeError: MPS backend out of memory`
**åŸå› **ï¼šBatch size è¿‡å¤§
**è§£å†³**ï¼š
```python
# å‡å° batch size
batch_size = 4  # æˆ–æ›´å°

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4  # ç­‰æ•ˆ batch_size * 4
```

### æ•°æ®ç±»å‹

**ç—‡çŠ¶**ï¼šæŸäº›æ“ä½œæŠ¥é”™ "not implemented for 'Half'"
**åŸå› **ï¼šMPS å¯¹ FP16 æ”¯æŒä¸å®Œæ•´
**è§£å†³**ï¼š
```python
# å›é€€åˆ° FP32
model = model.float()  # ä¸ä½¿ç”¨ .half()
```

---

## ğŸ“š æ‰©å±•é˜…è¯»

### ç†è®ºèƒŒæ™¯
- [Kaplan et al. (2020) - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [Hoffmann et al. (2022) - Training Compute-Optimal LLMs](https://arxiv.org/abs/2203.15556)

### å·¥ç¨‹å®è·µ
- [PyTorch MPS Backend](https://pytorch.org/docs/stable/notes/mps.html)
- [Apple Silicon æ€§èƒ½ä¼˜åŒ–](https://developer.apple.com/metal/pytorch/)

### ç›¸å…³å·¥å…·
- [nanoGPT](https://github.com/karpathy/nanoGPT) - æç®€ GPT å®ç°
- [Pythia](https://github.com/EleutherAI/pythia) - Scaling ç ”ç©¶å¥—ä»¶

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æ Issue å’Œ PRï¼

### æ”¹è¿›æ–¹å‘
- [ ] æ”¯æŒæ›´å¤šæ•°æ®é›†ï¼ˆOpenWebText, The Pileï¼‰
- [ ] æ·»åŠ å¤šæ¨¡æ€ Scalingï¼ˆVision Transformerï¼‰
- [ ] å®ç° Chinchilla æœ€ä¼˜é…æ¯”æœç´¢
- [ ] é›†æˆ Weights & Biases ç›‘æ§

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ OpenAIã€DeepMindã€Anthropic ç­‰æœºæ„çš„ Scaling Law ç ”ç©¶ã€‚

---

**æœ€åæ›´æ–°**: 2025-12-25  
**ä½œè€…**: peixingxin  
**è”ç³»**: [GitHub](https://github.com/yourusername)
