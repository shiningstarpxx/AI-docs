"""
MacBook MPS Scaling Law å®éªŒæ¡†æ¶
==================================

ä½œè€…: peixingxin
æ—¥æœŸ: 2025-12-25
ç›®æ ‡: åœ¨ MacBook (Apple Silicon) ä¸Šé«˜æ•ˆéªŒè¯ Scaling Law

ç‰¹ç‚¹:
- âœ… å……åˆ†åˆ©ç”¨ MPS åŠ é€Ÿ
- âœ… æ™ºèƒ½å†…å­˜ç®¡ç†
- âœ… æ—©åœä¸å¤–æ¨ç»“åˆ
- âœ… å®Œæ•´çš„ç›‘æ§ä¸å¯è§†åŒ–
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
import time
from dataclasses import dataclass
from typing import List, Dict, Optional
import gc
from scipy.optimize import curve_fit

# ============================================================================
# 1. MPS è®¾å¤‡ç®¡ç†
# ============================================================================

def get_mps_device():
    """è·å– MPS è®¾å¤‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if torch.backends.mps.is_available():
        if not torch.backends.mps.is_built():
            print("âš ï¸ MPS not available because PyTorch was not built with MPS enabled.")
            return torch.device("cpu")
        print("âœ… Using MPS (Apple Silicon GPU)")
        return torch.device("mps")
    else:
        print("âš ï¸ MPS device not found, using CPU")
        return torch.device("cpu")

def clear_mps_cache():
    """æ¸…ç† MPS ç¼“å­˜"""
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()

def get_memory_usage():
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    import psutil
    process = psutil.Process()
    mem_mb = process.memory_info().rss / (1024 * 1024)
    return f"{mem_mb:.0f} MB"

# ============================================================================
# 2. æ¨¡å‹å®šä¹‰
# ============================================================================

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    vocab_size: int = 50257  # GPT-2 vocab size
    max_seq_len: int = 256   # è¾ƒçŸ­çš„åºåˆ—ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    n_layers: int = 6
    d_model: int = 384
    n_heads: int = 6
    d_ff: int = 1536
    dropout: float = 0.1
    
    @property
    def n_params(self):
        """ä¼°ç®—å‚æ•°é‡"""
        # ç²—ç•¥ä¼°è®¡
        embed_params = self.vocab_size * self.d_model
        layer_params = (
            4 * self.d_model * self.d_model +  # Attention QKV + O
            2 * self.d_model * self.d_ff       # FFN
        ) * self.n_layers
        return embed_params + layer_params

class SimpleGPT(nn.Module):
    """ç®€åŒ–çš„ GPT æ¨¡å‹ï¼ˆç”¨äº Scaling å®éªŒï¼‰"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # Token + Position embeddings
        self.token_embed = nn.Embedding(config.vocab_size, config.d_model)
        self.pos_embed = nn.Embedding(config.max_seq_len, config.d_model)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        
        # Output
        self.ln_f = nn.LayerNorm(config.d_model)
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        
        # Initialize
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, input_ids, targets=None):
        B, T = input_ids.shape
        
        # Embeddings
        tok_emb = self.token_embed(input_ids)
        pos_emb = self.pos_embed(torch.arange(T, device=input_ids.device))
        x = tok_emb + pos_emb
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # Output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        # Loss
        loss = None
        if targets is not None:
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)), 
                targets.view(-1)
            )
        
        return logits, loss

class TransformerBlock(nn.Module):
    """Transformer block"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.d_model)
        self.attn = nn.MultiheadAttention(
            config.d_model, 
            config.n_heads, 
            dropout=config.dropout,
            batch_first=True
        )
        self.ln2 = nn.LayerNorm(config.d_model)
        self.mlp = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.GELU(),
            nn.Linear(config.d_ff, config.d_model),
            nn.Dropout(config.dropout)
        )
    
    def forward(self, x):
        # Self-attention
        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]
        # FFN
        x = x + self.mlp(self.ln2(x))
        return x

# ============================================================================
# 3. æ•°æ®åŠ è½½
# ============================================================================

class DummyTextDataset(Dataset):
    """è™šæ‹Ÿæ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"""
    
    def __init__(self, n_tokens: int, seq_len: int, vocab_size: int):
        self.n_samples = n_tokens // seq_len
        self.seq_len = seq_len
        self.vocab_size = vocab_size
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        # ç”Ÿæˆéšæœºåºåˆ—
        tokens = torch.randint(0, self.vocab_size, (self.seq_len + 1,))
        return {
            'input_ids': tokens[:-1],
            'labels': tokens[1:]
        }

# ============================================================================
# 4. è®­ç»ƒå™¨
# ============================================================================

class MPSTrainer:
    """é’ˆå¯¹ MPS ä¼˜åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(
        self, 
        model: nn.Module,
        device: torch.device,
        config: Dict
    ):
        self.model = model.to(device)
        self.device = device
        self.config = config
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.get('lr', 3e-4),
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.get('max_steps', 10000)
        )
        
        # History
        self.history = {'loss': [], 'step': []}
        
    def train(
        self, 
        train_loader: DataLoader, 
        max_steps: Optional[int] = None,
        eval_interval: int = 100,
        early_stop: bool = True
    ):
        """è®­ç»ƒå¾ªç¯"""
        self.model.train()
        step = 0
        max_steps = max_steps or self.config.get('max_steps', 10000)
        
        print(f"ğŸš€ Starting training (max {max_steps} steps)")
        print(f"   Device: {self.device}")
        print(f"   Memory: {get_memory_usage()}")
        
        start_time = time.time()
        
        while step < max_steps:
            for batch in train_loader:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward
                _, loss = self.model(input_ids, targets=labels)
                
                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.optimizer.step()
                self.scheduler.step()
                
                # è®°å½•
                self.history['loss'].append(loss.item())
                self.history['step'].append(step)
                
                # æ—¥å¿—
                if step % eval_interval == 0:
                    elapsed = time.time() - start_time
                    tokens_per_sec = (step * input_ids.numel()) / elapsed
                    print(f"Step {step}/{max_steps} | "
                          f"Loss: {loss.item():.4f} | "
                          f"LR: {self.scheduler.get_last_lr()[0]:.6f} | "
                          f"Tokens/s: {tokens_per_sec:.0f} | "
                          f"Mem: {get_memory_usage()}")
                
                # æ—©åœæ£€æŸ¥
                if early_stop and step > 0.2 * max_steps:
                    if self._should_early_stop():
                        print(f"âš¡ Early stopping at step {step}")
                        return self._get_results()
                
                step += 1
                if step >= max_steps:
                    break
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if step % 500 == 0:
                    clear_mps_cache()
        
        print(f"âœ… Training completed in {time.time() - start_time:.1f}s")
        return self._get_results()
    
    def _should_early_stop(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœ"""
        if len(self.history['loss']) < 100:
            return False
        
        # æ£€æŸ¥æœ€è¿‘çš„ loss æ˜¯å¦æ”¶æ•›
        recent_losses = self.history['loss'][-100:]
        loss_std = np.std(recent_losses)
        loss_mean = np.mean(recent_losses)
        
        # å¦‚æœæ ‡å‡†å·®å¾ˆå°ï¼Œè¯´æ˜å·²ç»æ”¶æ•›
        return loss_std / loss_mean < 0.01
    
    def _get_results(self) -> Dict:
        """è·å–è®­ç»ƒç»“æœ"""
        final_loss = np.mean(self.history['loss'][-100:])
        return {
            'final_loss': final_loss,
            'history': self.history,
            'n_steps': len(self.history['step'])
        }

# ============================================================================
# 5. Scaling Law å®éªŒ
# ============================================================================

class ScalingExperiment:
    """Scaling Law å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, device: torch.device, save_dir: str = "./results"):
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        self.results = []
    
    def run_experiment(
        self,
        n_params_list: List[int],
        n_tokens_list: List[int],
        mode: str = 'quick'
    ):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("=" * 80)
        print("ğŸ“Š Scaling Law Experiment")
        print("=" * 80)
        print(f"Mode: {mode}")
        print(f"Parameter scales: {[f'{n/1e6:.1f}M' for n in n_params_list]}")
        print(f"Data scales: {[f'{n/1e6:.1f}M' for n in n_tokens_list]}")
        print("=" * 80)
        
        total_experiments = len(n_params_list) * len(n_tokens_list)
        current = 0
        
        for n_params in n_params_list:
            for n_tokens in n_tokens_list:
                current += 1
                print(f"\n[{current}/{total_experiments}] Running experiment:")
                print(f"  Params: {n_params/1e6:.1f}M")
                print(f"  Tokens: {n_tokens/1e6:.1f}M")
                
                # è¿è¡Œå•æ¬¡å®éªŒ
                result = self._run_single_experiment(n_params, n_tokens, mode)
                result['n_params'] = n_params
                result['n_tokens'] = n_tokens
                self.results.append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_results()
                
                # æ¸…ç†å†…å­˜
                clear_mps_cache()
        
        print("\n" + "=" * 80)
        print("âœ… All experiments completed!")
        print("=" * 80)
        
        return self.results
    
    def _run_single_experiment(
        self, 
        n_params: int, 
        n_tokens: int,
        mode: str
    ) -> Dict:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        # 1. åˆ›å»ºæ¨¡å‹é…ç½®
        config = self._params_to_config(n_params)
        model = SimpleGPT(config)
        
        print(f"  Model: {config.n_layers} layers, {config.d_model} dim")
        print(f"  Actual params: {config.n_params/1e6:.1f}M")
        
        # 2. åˆ›å»ºæ•°æ®
        dataset = DummyTextDataset(
            n_tokens=int(n_tokens),
            seq_len=config.max_seq_len,
            vocab_size=config.vocab_size
        )
        dataloader = DataLoader(
            dataset,
            batch_size=self._get_batch_size(n_params, mode),
            shuffle=True,
            num_workers=0  # MPS ä¸æ”¯æŒå¤šè¿›ç¨‹
        )
        
        # 3. è®­ç»ƒ
        trainer = MPSTrainer(
            model=model,
            device=self.device,
            config={
                'lr': 3e-4,
                'weight_decay': 0.01,
                'max_steps': self._get_max_steps(n_params, n_tokens, mode)
            }
        )
        
        result = trainer.train(
            dataloader,
            eval_interval=100,
            early_stop=(mode != 'full')
        )
        
        return result
    
    def _params_to_config(self, n_params: int) -> ModelConfig:
        """æ ¹æ®å‚æ•°é‡ç”Ÿæˆé…ç½®"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        if n_params < 10e6:  # < 10M
            return ModelConfig(n_layers=4, d_model=256, n_heads=4)
        elif n_params < 50e6:  # < 50M
            return ModelConfig(n_layers=6, d_model=384, n_heads=6)
        elif n_params < 150e6:  # < 150M
            return ModelConfig(n_layers=8, d_model=512, n_heads=8)
        elif n_params < 500e6:  # < 500M
            return ModelConfig(n_layers=12, d_model=768, n_heads=12)
        else:  # >= 500M
            return ModelConfig(n_layers=16, d_model=1024, n_heads=16)
    
    def _get_batch_size(self, n_params: int, mode: str) -> int:
        """æ ¹æ®æ¨¡å‹å¤§å°åŠ¨æ€è°ƒæ•´ batch size"""
        if mode == 'quick':
            base_bs = 32
        elif mode == 'dev':
            base_bs = 16
        else:
            base_bs = 8
        
        # å¤§æ¨¡å‹ç”¨å° batch
        if n_params > 100e6:
            return max(1, base_bs // 4)
        elif n_params > 50e6:
            return max(2, base_bs // 2)
        else:
            return base_bs
    
    def _get_max_steps(self, n_params: int, n_tokens: int, mode: str) -> int:
        """è®¡ç®—è®­ç»ƒæ­¥æ•°"""
        batch_size = self._get_batch_size(n_params, mode)
        seq_len = 256
        
        # æ€» token æ•° = steps * batch_size * seq_len
        # steps = n_tokens / (batch_size * seq_len)
        steps = int(n_tokens / (batch_size * seq_len))
        
        # é™åˆ¶æœ€å¤§æ­¥æ•°
        if mode == 'quick':
            return min(steps, 1000)
        elif mode == 'dev':
            return min(steps, 5000)
        else:
            return steps
    
    def _save_results(self):
        """ä¿å­˜ç»“æœ"""
        with open(self.save_dir / 'results.json', 'w') as f:
            json.dump(self.results, f, indent=2)

# ============================================================================
# 6. ç»“æœåˆ†æ
# ============================================================================

class ScalingAnalyzer:
    """Scaling Law åˆ†æå™¨"""
    
    def __init__(self, results: List[Dict]):
        self.results = results
    
    def fit_power_law(self):
        """æ‹Ÿåˆå¹‚å¾‹"""
        # æå–æ•°æ®
        N = np.array([r['n_params'] for r in self.results])
        D = np.array([r['n_tokens'] for r in self.results])
        L = np.array([r['final_loss'] for r in self.results])
        
        # æ‹Ÿåˆå‚æ•°é‡çš„ scaling
        N_unique = np.unique(N)
        L_vs_N = []
        for n in N_unique:
            mask = N == n
            L_vs_N.append(np.mean(L[mask]))
        L_vs_N = np.array(L_vs_N)
        
        def power_law(x, a, alpha, c):
            return a * x**(-alpha) + c
        
        params_N, _ = curve_fit(power_law, N_unique, L_vs_N, p0=[100, 0.1, 2.0])
        a_n, alpha_n, c_n = params_N
        
        # æ‹Ÿåˆæ•°æ®é‡çš„ scaling
        D_unique = np.unique(D)
        L_vs_D = []
        for d in D_unique:
            mask = D == d
            L_vs_D.append(np.mean(L[mask]))
        L_vs_D = np.array(L_vs_D)
        
        params_D, _ = curve_fit(power_law, D_unique, L_vs_D, p0=[100, 0.1, 2.0])
        a_d, alpha_d, c_d = params_D
        
        print("\n" + "=" * 80)
        print("ğŸ“ˆ Scaling Law æ‹Ÿåˆç»“æœ")
        print("=" * 80)
        print(f"å‚æ•°é‡ Scaling: L(N) = {a_n:.2f} * N^(-{alpha_n:.3f}) + {c_n:.3f}")
        print(f"  æŒ‡æ•° Î±_n = {alpha_n:.3f}")
        print(f"  (Kaplan 2020: Î±_n â‰ˆ 0.076)")
        print()
        print(f"æ•°æ®é‡ Scaling: L(D) = {a_d:.2f} * D^(-{alpha_d:.3f}) + {c_d:.3f}")
        print(f"  æŒ‡æ•° Î±_d = {alpha_d:.3f}")
        print(f"  (Kaplan 2020: Î±_d â‰ˆ 0.095)")
        print("=" * 80)
        
        return {
            'N_scaling': params_N,
            'D_scaling': params_D
        }
    
    def extrapolate(self, target_params: float):
        """å¤–æ¨åˆ°ç›®æ ‡è§„æ¨¡"""
        params = self.fit_power_law()
        a_n, alpha_n, c_n = params['N_scaling']
        
        predicted_loss = a_n * target_params**(-alpha_n) + c_n
        
        print(f"\nå¤–æ¨é¢„æµ‹:")
        print(f"  ç›®æ ‡è§„æ¨¡: {target_params/1e9:.1f}B å‚æ•°")
        print(f"  é¢„æµ‹ loss: {predicted_loss:.3f}")
        
        return predicted_loss
    
    def plot(self, save_path: str = 'scaling_curves.png'):
        """ç»˜åˆ¶ scaling æ›²çº¿"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # æå–æ•°æ®
        N = np.array([r['n_params'] for r in self.results])
        D = np.array([r['n_tokens'] for r in self.results])
        L = np.array([r['final_loss'] for r in self.results])
        
        # å·¦å›¾ï¼šå‚æ•°é‡ scaling
        axes[0].scatter(N, L, alpha=0.6, s=50)
        axes[0].set_xscale('log')
        axes[0].set_yscale('log')
        axes[0].set_xlabel('Parameters (N)', fontsize=12)
        axes[0].set_ylabel('Loss (L)', fontsize=12)
        axes[0].set_title('Parameter Scaling', fontsize=14)
        axes[0].grid(True, alpha=0.3)
        
        # æ‹Ÿåˆæ›²çº¿
        N_sorted = np.sort(np.unique(N))
        params = self.fit_power_law()
        a_n, alpha_n, c_n = params['N_scaling']
        L_fit = a_n * N_sorted**(-alpha_n) + c_n
        axes[0].plot(N_sorted, L_fit, 'r--', linewidth=2, 
                     label=f'L(N) âˆ N^(-{alpha_n:.3f})')
        axes[0].legend()
        
        # å³å›¾ï¼šæ•°æ®é‡ scaling
        axes[1].scatter(D, L, alpha=0.6, s=50, color='green')
        axes[1].set_xscale('log')
        axes[1].set_yscale('log')
        axes[1].set_xlabel('Training Tokens (D)', fontsize=12)
        axes[1].set_ylabel('Loss (L)', fontsize=12)
        axes[1].set_title('Data Scaling', fontsize=14)
        axes[1].grid(True, alpha=0.3)
        
        # æ‹Ÿåˆæ›²çº¿
        D_sorted = np.sort(np.unique(D))
        a_d, alpha_d, c_d = params['D_scaling']
        L_fit = a_d * D_sorted**(-alpha_d) + c_d
        axes[1].plot(D_sorted, L_fit, 'r--', linewidth=2,
                     label=f'L(D) âˆ D^(-{alpha_d:.3f})')
        axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š Plot saved to: {save_path}")

# ============================================================================
# 7. ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['quick', 'dev', 'full'], default='quick')
    args = parser.parse_args()
    
    # 1. è®¾å¤‡
    device = get_mps_device()
    
    # 2. å®éªŒé…ç½®
    if args.mode == 'quick':
        print("ğŸš€ Quick mode: å¿«é€ŸéªŒè¯ï¼ˆ~2å°æ—¶ï¼‰")
        n_params_list = [5e6, 20e6, 80e6]  # 5M, 20M, 80M
        n_tokens_list = [10e6, 50e6]       # 10M, 50M
    elif args.mode == 'dev':
        print("ğŸš€ Dev mode: å¼€å‘æ¨¡å¼ï¼ˆ~1å¤©ï¼‰")
        n_params_list = [5e6, 20e6, 80e6, 200e6]  # 5M, 20M, 80M, 200M
        n_tokens_list = [10e6, 50e6, 200e6]       # 10M, 50M, 200M
    else:
        print("ğŸš€ Full mode: å®Œæ•´å®éªŒï¼ˆ~1å‘¨ï¼‰")
        n_params_list = [5e6, 10e6, 20e6, 50e6, 100e6, 200e6, 500e6]
        n_tokens_list = [10e6, 50e6, 200e6, 500e6]
    
    # 3. è¿è¡Œå®éªŒ
    experiment = ScalingExperiment(device=device, save_dir=f'./results_{args.mode}')
    results = experiment.run_experiment(n_params_list, n_tokens_list, mode=args.mode)
    
    # 4. åˆ†æç»“æœ
    analyzer = ScalingAnalyzer(results)
    analyzer.fit_power_law()
    
    # 5. å¤–æ¨é¢„æµ‹
    print("\n" + "=" * 80)
    print("ğŸ”® å¤–æ¨é¢„æµ‹")
    print("=" * 80)
    analyzer.extrapolate(1.5e9)   # GPT-2 XL
    analyzer.extrapolate(175e9)   # GPT-3
    
    # 6. å¯è§†åŒ–
    analyzer.plot(save_path=f'./results_{args.mode}/scaling_curves.png')
    
    print("\nâœ… å®éªŒå®Œæˆï¼")

if __name__ == '__main__':
    main()
