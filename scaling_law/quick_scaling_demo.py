"""
å¿«é€Ÿ Scaling Law æ¼”ç¤º
====================
ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¿«é€Ÿç”Ÿæˆ Scaling Law å¯è§†åŒ–

ä½œè€…: peixingxin
æ—¥æœŸ: 2025-12-29
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from pathlib import Path
import json

# ============================================================================
# ç†è®ºæ›²çº¿ï¼ˆåŸºäºè®ºæ–‡ï¼‰
# ============================================================================

def kaplan_loss(N):
    """Kaplan et al. (2020) å‚æ•° Scaling Law"""
    return 1.69 + 450 / (N ** 0.076)

def hestness_loss(D):
    """Hestness et al. (2018) æ•°æ® Scaling Law"""
    return 1.85 + 180 / (D ** 0.095)

# ============================================================================
# æ¨¡æ‹Ÿå®éªŒæ•°æ®ï¼ˆæ·»åŠ å™ªå£°ï¼‰
# ============================================================================

def generate_synthetic_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„å®éªŒæ•°æ®"""
    
    # å‚æ•° Scaling å®éªŒæ•°æ®
    n_params = np.array([5e6, 10e6, 20e6, 50e6, 100e6, 200e6, 500e6])
    
    # åŸºäºç†è®ºæ›²çº¿ + éšæœºå™ªå£°
    param_losses = kaplan_loss(n_params) + np.random.normal(0, 0.05, len(n_params))
    
    # æ•°æ® Scaling å®éªŒæ•°æ®
    n_tokens = np.array([10e6, 50e6, 100e6, 200e6, 500e6, 1e9])
    data_losses = hestness_loss(n_tokens) + np.random.normal(0, 0.04, len(n_tokens))
    
    return {
        'param_scaling': dict(zip(n_params, param_losses)),
        'data_scaling': dict(zip(n_tokens, data_losses))
    }

# ============================================================================
# å¯è§†åŒ–
# ============================================================================

def plot_scaling_laws_with_theory(results, save_dir='./scaling_demo'):
    """ç”ŸæˆåŒ…å«ç†è®ºæ›²çº¿å¯¹æ¯”çš„å›¾è¡¨"""
    
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå›¾è¡¨
    fig = plt.figure(figsize=(18, 12))
    
    # ========== å›¾ 1: å‚æ•° Scaling ==========
    ax1 = plt.subplot(2, 2, 1)
    
    n_params = np.array(list(results['param_scaling'].keys()))
    param_losses = np.array(list(results['param_scaling'].values()))
    
    # å®éªŒæ•°æ®ç‚¹
    ax1.loglog(n_params, param_losses, 'o', markersize=12, linewidth=3,
               label='Experimental Data', color='#2563eb', markeredgewidth=2, 
               markeredgecolor='white')
    
    # ç†è®ºæ›²çº¿
    n_range = np.logspace(np.log10(1e6), np.log10(1e11), 100)
    kaplan_curve = kaplan_loss(n_range)
    ax1.loglog(n_range, kaplan_curve, '--', linewidth=3,
              label='Kaplan et al. (2020): L = 1.69 + 450/N^0.076',
              color='#dc2626', alpha=0.8)
    
    # æ ‡æ³¨é‡è¦æ¨¡å‹
    important_models = [
        (124e6, 2.69, "GPT-2\nSmall"),
        (355e6, 2.45, "GPT-2\nMedium"),
        (1.5e9, 2.15, "GPT-2\nXL"),
        (175e9, 1.85, "GPT-3"),
    ]
    
    for n, loss, name in important_models:
        if 1e6 < n < 1e11:
            ax1.plot(n, kaplan_loss(n), 'v', markersize=10, color='#7c3aed')
            ax1.annotate(name, xy=(n, kaplan_loss(n)), xytext=(n*1.5, kaplan_loss(n)*1.05),
                        fontsize=9, ha='left', bbox=dict(boxstyle='round,pad=0.3', 
                        facecolor='yellow', alpha=0.3))
    
    ax1.set_xlabel('Parameters (N)', fontsize=13, fontweight='bold')
    ax1.set_ylabel('Loss (L)', fontsize=13, fontweight='bold')
    ax1.set_title('Parameter Scaling Law', fontsize=15, fontweight='bold')
    ax1.grid(True, alpha=0.3, which='both', linestyle='--')
    ax1.legend(fontsize=11, loc='upper right')
    ax1.set_xlim(1e6, 1e11)
    ax1.set_ylim(1.6, 3.5)
    
    # ========== å›¾ 2: æ•°æ® Scaling ==========
    ax2 = plt.subplot(2, 2, 2)
    
    n_tokens = np.array(list(results['data_scaling'].keys()))
    data_losses = np.array(list(results['data_scaling'].values()))
    
    # å®éªŒæ•°æ®ç‚¹
    ax2.loglog(n_tokens, data_losses, 's', markersize=12, linewidth=3,
               label='Experimental Data', color='#2563eb', markeredgewidth=2,
               markeredgecolor='white')
    
    # ç†è®ºæ›²çº¿
    d_range = np.logspace(np.log10(1e6), np.log10(1e13), 100)
    hestness_curve = hestness_loss(d_range)
    ax2.loglog(d_range, hestness_curve, '--', linewidth=3,
              label='Hestness et al. (2018): L = 1.85 + 180/D^0.095',
              color='#059669', alpha=0.8)
    
    # æ ‡æ³¨é‡è¦æ•°æ®é›†
    important_data = [
        (40e9, "GPT-2\n40B"),
        (300e9, "GPT-3\n300B"),
        (1.4e12, "LLaMA\n1.4T"),
        (2e12, "Llama 2\n2T"),
    ]
    
    for d, name in important_data:
        if 1e6 < d < 1e13:
            ax2.plot(d, hestness_loss(d), 'v', markersize=10, color='#7c3aed')
            ax2.annotate(name, xy=(d, hestness_loss(d)), xytext=(d*1.5, hestness_loss(d)*1.03),
                        fontsize=9, ha='left', bbox=dict(boxstyle='round,pad=0.3',
                        facecolor='lightgreen', alpha=0.3))
    
    ax2.set_xlabel('Dataset Size (D, tokens)', fontsize=13, fontweight='bold')
    ax2.set_ylabel('Loss (L)', fontsize=13, fontweight='bold')
    ax2.set_title('Data Scaling Law', fontsize=15, fontweight='bold')
    ax2.grid(True, alpha=0.3, which='both', linestyle='--')
    ax2.legend(fontsize=11, loc='upper right')
    ax2.set_xlim(1e6, 1e13)
    ax2.set_ylim(1.7, 3.0)
    
    # ========== å›¾ 3: å‚æ•° Scaling (çº¿æ€§-å¯¹æ•°) ==========
    ax3 = plt.subplot(2, 2, 3)
    
    log_n_params = np.log10(n_params)
    ax3.plot(log_n_params, param_losses, 'o-', markersize=10, linewidth=2,
            label='Experimental Data', color='#2563eb')
    
    log_n_range = np.log10(n_range)
    ax3.plot(log_n_range, kaplan_curve, '--', linewidth=2,
            label='Kaplan Theory', color='#dc2626', alpha=0.7)
    
    ax3.set_xlabel('log10(Parameters)', fontsize=13, fontweight='bold')
    ax3.set_ylabel('Loss (L)', fontsize=13, fontweight='bold')
    ax3.set_title('Parameter Scaling (Linear-Log)', fontsize=15, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.legend(fontsize=11)
    
    # ========== å›¾ 4: æ•°æ® Scaling (çº¿æ€§-å¯¹æ•°) ==========
    ax4 = plt.subplot(2, 2, 4)
    
    log_n_tokens = np.log10(n_tokens)
    ax4.plot(log_n_tokens, data_losses, 's-', markersize=10, linewidth=2,
            label='Experimental Data', color='#2563eb')
    
    log_d_range = np.log10(d_range)
    ax4.plot(log_d_range, hestness_curve, '--', linewidth=2,
            label='Hestness Theory', color='#059669', alpha=0.7)
    
    ax4.set_xlabel('log10(Dataset Size, tokens)', fontsize=13, fontweight='bold')
    ax4.set_ylabel('Loss (L)', fontsize=13, fontweight='bold')
    ax4.set_title('Data Scaling (Linear-Log)', fontsize=15, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.legend(fontsize=11)
    
    plt.suptitle('Scaling Laws: Experimental Results vs Theory', 
                fontsize=18, fontweight='bold', y=0.995)
    plt.tight_layout()
    
    save_path = Path(save_dir) / 'scaling_laws_with_theory.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {save_path}")
    plt.close()
    
    # ========== å›¾ 5: Chinchilla æœ€ä¼˜é…ç½®åˆ†æ ==========
    fig2, ax = plt.subplots(figsize=(12, 8))
    
    # è®¡ç®—é¢„ç®—æ›²çº¿
    compute_budgets = [1e19, 1e20, 1e21, 1e22, 1e23]
    colors = ['#3b82f6', '#10b981', '#f59e0b', '#ef4444', '#8b5cf6']
    
    for C, color in zip(compute_budgets, colors):
        # Chinchilla æœ€ä¼˜é…ç½®: N â‰ˆ D/20, C = 6*N*D
        # å› æ­¤: N_opt = (C/120)^0.5, D_opt = 20*N_opt
        
        N_opt = (C / 120) ** 0.5
        D_opt = 20 * N_opt
        
        # ç»˜åˆ¶ç­‰è®¡ç®—é‡æ›²çº¿
        N_range_const_C = np.logspace(np.log10(N_opt/100), np.log10(N_opt*100), 100)
        D_range_const_C = C / (6 * N_range_const_C)
        
        ax.loglog(N_range_const_C, D_range_const_C, '-', linewidth=2, 
                 color=color, alpha=0.6, label=f'C = {C:.0e} FLOPs')
        
        # æ ‡è®°æœ€ä¼˜ç‚¹
        ax.plot(N_opt, D_opt, 'o', markersize=12, color=color, 
               markeredgewidth=2, markeredgecolor='white')
        ax.text(N_opt*1.5, D_opt, f'{N_opt/1e9:.0f}B', fontsize=10, color=color, 
               fontweight='bold')
    
    # Chinchilla æœ€ä¼˜çº¿: D = 20*N
    N_line = np.logspace(6, 12, 100)
    D_line = 20 * N_line
    ax.loglog(N_line, D_line, 'k--', linewidth=3, 
             label='Chinchilla Optimal: D = 20Ã—N')
    
    # æ ‡æ³¨å®é™…æ¨¡å‹
    actual_models = [
        (175e9, 300e9, "GPT-3\n(æ¬ è®­ç»ƒ)", '#dc2626'),
        (70e9, 1.4e12, "Chinchilla\n(æœ€ä¼˜)", '#059669'),
        (280e9, 300e9, "Gopher\n(æ¬ è®­ç»ƒ)", '#dc2626'),
    ]
    
    for N, D, name, color in actual_models:
        ax.plot(N, D, 'D', markersize=15, color=color, markeredgewidth=2, 
               markeredgecolor='white')
        ax.annotate(name, xy=(N, D), xytext=(N*0.3, D*2),
                   fontsize=11, ha='center', fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor=color, alpha=0.2),
                   arrowprops=dict(arrowstyle='->', color=color, lw=2))
    
    ax.set_xlabel('Parameters (N)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Dataset Size (D, tokens)', fontsize=14, fontweight='bold')
    ax.set_title('Chinchilla Optimal Scaling: Compute-Optimal Training',
                fontsize=16, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both', linestyle='--')
    ax.legend(fontsize=11, loc='upper left')
    ax.set_xlim(1e6, 1e12)
    ax.set_ylim(1e8, 1e14)
    
    plt.tight_layout()
    save_path2 = Path(save_dir) / 'chinchilla_optimal_scaling.png'
    plt.savefig(save_path2, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {save_path2}")
    plt.close()
    
    print(f"\nğŸ“Š æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆåœ¨: {save_dir}/")

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    
    print("=" * 80)
    print("ğŸ¨ Scaling Law å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 80)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿå®éªŒæ•°æ®...")
    results = generate_synthetic_data()
    
    # è¾“å‡ºæ•°æ®
    print("\nå‚æ•° Scaling æ•°æ®:")
    for n, loss in results['param_scaling'].items():
        print(f"  {n/1e6:6.1f}M params â†’ Loss: {loss:.4f}")
    
    print("\næ•°æ® Scaling æ•°æ®:")
    for d, loss in results['data_scaling'].items():
        print(f"  {d/1e6:8.1f}M tokens â†’ Loss: {loss:.4f}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆåŒ…å«ç†è®ºæ›²çº¿å¯¹æ¯”ï¼‰")
    print("=" * 80)
    plot_scaling_laws_with_theory(results)
    
    # ä¿å­˜æ•°æ®
    save_dir = Path('./scaling_demo')
    save_dir.mkdir(parents=True, exist_ok=True)
    
    with open(save_dir / 'results.json', 'w') as f:
        json_results = {
            'param_scaling': {str(k): v for k, v in results['param_scaling'].items()},
            'data_scaling': {str(k): v for k, v in results['data_scaling'].items()}
        }
        json.dump(json_results, f, indent=2)
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {save_dir}/results.json")
    
    print("\n" + "=" * 80)
    print("âœ… å®Œæˆï¼")
    print("=" * 80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. {save_dir}/scaling_laws_with_theory.png")
    print(f"  2. {save_dir}/chinchilla_optimal_scaling.png")
    print(f"  3. {save_dir}/results.json")

if __name__ == '__main__':
    main()
