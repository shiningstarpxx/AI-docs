# Tasks: CartPole 对比实验

## 实验状态总结

**完成日期**: 2025-12-23  
**结论**: CartPole 不适合 World Model 演示，但实验过程很有教学价值

---

## 1. 环境准备
- [x] 1.1 确认 Gymnasium CartPole-v1 环境可用
- [x] 1.2 定义统一的评估协议（成功标准、评估次数）
- [x] 1.3 设置随机种子保证可复现

## 2. DQN Baseline 实现 ✅
- [x] 2.1 实现 DQN (经验回放 + 目标网络)
- [x] 2.2 训练并记录学习曲线
- [x] 2.3 记录达到 195 分所需的交互步数
- **结果**: 193.3 ± 128.9 (800 episodes, 不够稳定但可用)
- **文件**: `experiments/1_baseline_dqn.py`

## 3. Simple World Model 实现 ❌ (失败但有价值)
- [x] 3.1 跳过 VAE（低维状态不需要）
- [x] 3.2 实现 LSTM 动态模型 (s, a) → s'
- [x] 3.3 实现奖励预测器
- [x] 3.4 在想象中训练 policy (CMA-ES 进化策略)
- [x] 3.5 记录样本效率
- **V1 结果**: 真实 17.06 vs 梦境 103.16（模型偏差严重）
- **V2 结果**: 真实 9.28 vs 梦境 46.94（改进后更差）
- **根本原因**: CartPole 动态太敏感，LSTM 无法准确建模
- **文件**: `experiments/2_simple_world_model_v2.py`

## 4. Mini-Dreamer 实现 ❌ (未开始)
- [ ] 4.1 实现简化版 RSSM (确定性 h + 随机性 z)
- [ ] 4.2 实现 Actor-Critic 网络
- [ ] 4.3 实现想象轨迹生成和 λ-Returns
- [ ] 4.4 训练并记录样本效率
- **决定**: 不继续实现，因为 Simple WM 已证明 CartPole 不适合

## 5. 对比分析 ✅
- [x] 5.1 绘制学习曲线对比图
- [x] 5.2 绘制样本效率对比柱状图
- [x] 5.3 分析各方法优缺点
- **深入分析**: World Model 适用场景、模型偏差、数据质量影响

## 6. 文档输出 ✅
- [x] 6.1 创建实验报告文档 (`CURRENT_STATUS.md`)
- [x] 6.2 整理可视化结果 (训练曲线图)
- [x] 6.3 总结实验结论

---

## 核心经验教训

### World Model 适用场景

**适合** ✅:
- 视觉输入（VAE 压缩）
- 平滑动态（允许误差）
- 稀疏奖励（探索困难）

**不适合** ❌:
- 低维状态（CartPole 只有 4 维）
- 敏感动态（微小误差 → 崩溃）
- 密集奖励（直接学习更高效）

### 关键发现

1. **数据质量 ≠ 模型准确性**: V2 用 180 分数据训练，但梦境只有 47 分
2. **模型容量的边界**: LSTM(256) 仍无法准确建模 CartPole
3. **隐状态不匹配**: 训练时有正确 hidden，梦境时从零开始累积误差

### 教学价值

尽管性能失败，这个实验很有价值：
- ✅ 理解 World Model 的适用边界
- ✅ 模型偏差问题的真实案例
- ✅ 数据质量对 Model-Based RL 的影响
- ✅ Train-test mismatch 的实例

---

## 建议后续方向

### 方案 A: 接受现状 (已采纳)
- 将失败案例作为教学材料
- 重点讲清楚 World Model 原理和挑战
- 时间成本：0（已完成）

### 方案 B: Dyna-Q 补充 (可选)
- 实现更简单的 Model-Based RL
- 在 CartPole 上更容易成功
- 时间成本：2-3 小时

### 方案 C: 换到 MountainCar (不推荐)
- 更适合 World Model 的任务
- 但需要重新实现所有方法
- 时间成本：1-2 天
