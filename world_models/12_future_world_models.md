# 新一代世界模型：从 Dreamer 到 Genie 与 JEPA

> 探索世界模型的前沿演进方向

## 目录

1. [Dreamer 的局限与未解问题](#dreamer-的局限与未解问题)
2. [视频生成作为世界模型](#视频生成作为世界模型)
3. [Genie：从视频学习可控世界模型](#genie从视频学习可控世界模型)
4. [LeCun 的 JEPA 路线](#lecun-的-jepa-路线)
5. [统一架构的探索](#统一架构的探索)
6. [未来方向](#未来方向)

---

## Dreamer 的局限与未解问题

### 回顾：三阶段割裂

```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│   观察      │  →   │   建模      │  →   │   决策      │
│ Encoder     │      │   RSSM      │      │ Actor-Critic│
└─────────────┘      └─────────────┘      └─────────────┘
     ↓                     ↓                    ↓
  重建准确？           预测准确？           奖励最大？

问题：三个目标不完全一致
- 重建准确的细节可能对决策无用
- 预测准确的动态可能不是关键因素
```

### 核心问题

**1. 目标不对齐**

```
世界模型目标：最小化重建误差 + KL 散度
策略目标：最大化累积奖励

重建误差 ↓ ≠ 策略性能 ↑

例子：完美重建背景草地的纹理
      但这对"躲避敌人"毫无帮助
```

**2. 表示瓶颈**

```
RSSM 的瓶颈：
- 固定维度的潜在空间 (如 32×32)
- 必须把所有信息压缩进去
- 简单任务浪费，复杂任务不够

问：能否学习"与任务相关"的表示？
```

**3. 泛化困难**

```
Dreamer 在每个任务上从零学习
- 新任务需要重新收集数据
- 新环境需要重新训练世界模型

问：能否从大量视频预训练一个"通用世界模型"？
```

---

## 视频生成作为世界模型

### 核心洞察

```
视频 = 世界的时序展开
视频生成模型 = 某种意义上的世界模型

如果模型能"预测下一帧"，它就理解了：
- 物理规律（物体运动）
- 因果关系（动作→结果）
- 场景一致性
```

### 视频预测 vs 世界模型

| 方面 | 传统视频预测 | 世界模型 |
|:---|:---|:---|
| 输入 | 过去帧 | 过去帧 + 动作 |
| 输出 | 未来帧 | 未来帧 + 奖励 |
| 可控性 | 无 | 有（通过动作） |
| 应用 | 视频生成 | 决策/规划 |

**关键差异：动作条件**

```
视频预测：   p(x_{t+1} | x_{1:t})
世界模型：   p(x_{t+1} | x_{1:t}, a_t)  ← 动作条件
                              ↑
                        这是核心！
```

### Sora 与世界模型

OpenAI 的 Sora 展示了惊人的视频生成能力：

```
Sora 能做到：
- 物理一致性（物体碰撞、流体运动）
- 长期一致性（角色保持、场景连贯）
- 复杂场景理解

但 Sora 缺少：
- 动作条件（不可控）
- 奖励预测
- 与 RL 的集成
```

**问题：能否让视频生成模型"可控"，变成真正的世界模型？**

---

## Genie：从视频学习可控世界模型

> Google DeepMind, 2024
> "Genie: Generative Interactive Environments"

### 核心创新

**从无标签视频学习动作！**

```
传统世界模型：需要 (观测, 动作, 奖励) 三元组
Genie：只需要视频，自动推断"潜在动作"

数据来源：YouTube 游戏视频、网络视频
规模：200k+ 小时视频
```

### 架构

```
┌─────────────────────────────────────────────────────────┐
│                        Genie                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  输入: 视频帧序列 [x_1, x_2, ..., x_T]                  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐   │
│  │           Video Tokenizer (VQ-VAE)              │   │
│  │           将视频压缩为离散 token                 │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  ┌─────────────────────────────────────────────────┐   │
│  │           Latent Action Model                   │   │
│  │           推断帧之间的"潜在动作"                 │   │
│  │           a_t = f(x_t, x_{t+1})                 │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  ┌─────────────────────────────────────────────────┐   │
│  │           Dynamics Model (Transformer)          │   │
│  │           预测: x_{t+1} = g(x_t, a_t)           │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 潜在动作 (Latent Action) 的魔力

```
训练时：
  给定连续两帧 x_t, x_{t+1}
  学习一个"动作" a_t 解释它们的变化
  a_t = Encoder(x_t, x_{t+1})

推理时：
  给定当前帧 x_t 和动作 a_t
  生成下一帧 x_{t+1} = Decoder(x_t, a_t)

关键：a_t 是离散的 (8 个选项)
      对应方向键 + 有限操作
```

### 为什么这很重要？

```
1. 无需人工标注动作
   - 从视频自动发现"可控维度"
   - 扩展到海量网络视频

2. 学习到的动作是"有意义的"
   - 不是随机维度
   - 对应人类直觉的操作（移动、跳跃）

3. 生成的环境可以交互
   - 用户给动作，模型生成下一帧
   - 可以"玩"从视频学到的游戏
```

### Genie 的效果

```
训练数据：2D 平台游戏视频 (无动作标签)
结果：
- 学到了合理的动作空间（左、右、跳等）
- 可以生成一致的游戏体验
- 用户可以交互控制

更惊人的是：
- 输入一张草图，生成可玩的游戏！
- 世界模型从"理解视频"到"创造世界"
```

### Genie vs Dreamer

| 方面 | Dreamer | Genie |
|:---|:---|:---|
| 训练数据 | (obs, action, reward) | 纯视频 |
| 动作空间 | 预定义 | 自动学习 |
| 预训练 | 无 | 大规模视频 |
| 任务特定 | 是 | 通用 |
| 可交互 | 是 | 是 |

---

## LeCun 的 JEPA 路线

> Joint Embedding Predictive Architecture
> "A Path Towards Autonomous Machine Intelligence" (LeCun, 2022)

### LeCun 对当前方法的批评

```
生成模型的问题：
  预测像素 → 浪费计算在无关细节
  "预测草地的每片叶子" vs "理解有个敌人在逼近"

Dreamer/Genie 都在像素空间或压缩后的 token 空间预测
LeCun 认为这是错误的方向
```

### JEPA 的核心思想

**不预测像素，预测表示！**

```
传统 (生成式):
  x_t → Encoder → z_t → Decoder → x̂_{t+1}
  损失: ||x_{t+1} - x̂_{t+1}||²
  问题: 必须预测所有细节

JEPA (判别式):
  x_t → Encoder → z_t → Predictor → ẑ_{t+1}
  x_{t+1} → Encoder → z_{t+1}  (目标)
  损失: ||z_{t+1} - ẑ_{t+1}||²
  优势: 只预测"重要的"抽象特征
```

### 架构图

```
┌─────────────────────────────────────────────────────────┐
│                        JEPA                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│     x_t                               x_{t+1}           │
│      ↓                                   ↓              │
│  ┌────────┐                         ┌────────┐         │
│  │Encoder │                         │Encoder │ (EMA)   │
│  └────────┘                         └────────┘         │
│      ↓                                   ↓              │
│     z_t                                z_{t+1}          │
│      ↓                                   ↓              │
│  ┌──────────┐                           │              │
│  │Predictor │ ───────────────────→  比较损失           │
│  └──────────┘                                          │
│      ↓                                                  │
│    ẑ_{t+1}                                             │
│                                                         │
│  关键: 不重建 x_{t+1}，只预测其表示 z_{t+1}             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### I-JEPA (图像) 和 V-JEPA (视频)

**I-JEPA (2023):**

```
任务: 从图像的一部分预测另一部分的表示

┌─────────────┐
│  ████░░░░░░ │  输入: 可见部分
│  ████░░░░░░ │  目标: 预测被遮挡部分的表示
│  ░░░░░░░░░░ │
└─────────────┘

不是预测像素值，而是预测高级特征
学到的表示对下游任务很有用
```

**V-JEPA (2024):**

```
扩展到视频:
- 输入: 视频的部分帧
- 目标: 预测其他帧的表示

特点:
- 不需要预训练的图像编码器
- 不需要负样本
- 学到时空理解能力
```

### JEPA vs 生成式世界模型

| 方面 | 生成式 (Dreamer/Genie) | JEPA |
|:---|:---|:---|
| 预测目标 | 像素/token | 表示 |
| 解码器 | 需要 | 不需要 |
| 计算效率 | 较低 | 较高 |
| 信息保留 | 全部细节 | 只保留"有用的" |
| 当前状态 | 已验证有效 | 仍在探索 |

### JEPA 用于 RL 的挑战

```
JEPA 的问题:
- 如何加入动作条件？
- 如何预测奖励？
- 如何做规划？

目前 JEPA 主要用于自监督学习
用于 RL 的世界模型还在研究中
```

---

## 统一架构的探索

### 问题回顾

我们之前讨论的"三阶段割裂"：

```
观察 (Encoder) → 建模 (Dynamics) → 决策 (Policy)

每个阶段有不同的目标和损失
联合优化是困难的
```

### 方向 1: Transformer 统一一切

```
┌─────────────────────────────────────────────────────────┐
│                  Unified Transformer                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  输入序列: [obs_1, act_1, rew_1, obs_2, act_2, ...]    │
│                                                         │
│                    ↓ Transformer ↓                      │
│                                                         │
│  输出: 预测下一个 token (obs/act/rew)                   │
│                                                         │
│  优势:                                                  │
│  - 统一的架构，统一的损失                               │
│  - 可以利用大规模预训练                                 │
│  - In-context learning 潜力                            │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

代表工作：
- **Decision Transformer** (2021): 把 RL 变成序列建模
- **Gato** (2022): 一个模型做所有任务
- **RT-2** (2023): 视觉-语言-动作模型

### 方向 2: 目标条件世界模型

```
传统: 世界模型预测 "会发生什么"
新方向: 世界模型预测 "如何达到目标"

┌─────────────────────────────────────────────────────────┐
│              Goal-Conditioned World Model               │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  输入: 当前状态 s, 目标状态 g                           │
│  输出: 达到目标的动作序列                               │
│                                                         │
│  不是 p(s' | s, a)                                     │
│  而是 p(a | s, g) 或 p(trajectory | s, g)              │
│                                                         │
│  优势: 直接为决策服务                                   │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 方向 3: 层次化世界模型

```
现实世界是多尺度的:
- 毫秒级: 肌肉控制
- 秒级: 动作执行
- 分钟级: 子目标
- 小时级: 高层计划

┌─────────────────────────────────────────────────────────┐
│            Hierarchical World Model                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  高层: 抽象状态转移 (分钟级)                            │
│    ↓                                                    │
│  中层: 子目标序列 (秒级)                                │
│    ↓                                                    │
│  底层: 具体动作 (毫秒级)                                │
│                                                         │
│  每层有自己的世界模型                                   │
│  高层指导低层                                           │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 未来方向

### 1. 基础世界模型 (Foundation World Model)

```
类比:
  GPT = 语言的基础模型
  ??? = 物理世界的基础模型

目标:
  在海量视频/交互数据上预训练
  学习通用的物理规律和因果关系
  微调适应具体任务

挑战:
  - 数据：需要带动作的交互数据
  - 评估：如何衡量"世界理解"？
  - 泛化：从视频到真实机器人
```

### 2. 语言 + 世界模型

```
人类用语言思考和规划
能否让世界模型也有语言接口？

┌─────────────────────────────────────────────────────────┐
│          Language-Augmented World Model                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  语言指令: "把红色方块放到蓝色盒子上"                   │
│                 ↓                                       │
│  世界模型: 预测执行后的状态                             │
│                 ↓                                       │
│  语言反馈: "方块会掉下来因为盒子太小"                   │
│                                                         │
└─────────────────────────────────────────────────────────┘

代表工作:
- RT-2: 把机器人控制变成视觉语言问题
- PaLM-E: 多模态具身语言模型
```

### 3. 因果世界模型

```
当前模型学的是相关性，不是因果性

例子:
  观察: 太阳升起 → 公鸡打鸣
  相关模型: 太阳升起导致公鸡打鸣 ✓
            公鸡打鸣导致太阳升起 ✓ (错误！)

因果模型:
  学习 do(action) 的效果
  能回答反事实问题
  更好的泛化和规划
```

### 4. 具身智能与真实世界

```
从模拟到现实 (Sim2Real):
  在模拟器中用世界模型训练
  迁移到真实机器人

挑战:
  - 模拟器与真实的差距 (domain gap)
  - 真实世界的复杂性和不确定性
  - 安全性（不能在真实世界随意探索）

解决方向:
  - Domain Randomization
  - 持续在线学习
  - 人在回路 (Human in the loop)
```

---

## 总结：世界模型的演进脉络

```
第一代 (2018): World Models
  - 证明了"在梦中学习"可行
  - 但分阶段训练，简单控制器

第二代 (2020-2023): Dreamer 系列
  - 端到端训练
  - 强大的 Actor-Critic
  - 离散潜在空间
  - 固定超参数

第三代 (2024+): ???
  - 大规模视频预训练 (Genie)
  - 抽象表示预测 (JEPA)
  - 语言集成
  - 因果推理
```

### 核心演进趋势

| 趋势 | 早期 | 现在 | 未来 |
|:---|:---|:---|:---|
| 数据 | 任务特定交互 | 大规模视频 | 多模态融合 |
| 表示 | 像素重建 | 离散 token | 抽象表示 |
| 架构 | 专用模块 | Transformer | 统一基础模型 |
| 训练 | 分阶段 | 端到端 | 预训练+微调 |
| 目标 | 单任务 | 多任务 | 通用智能 |

---

## 开放问题

1. **什么是"好的"世界模型？**
   - 重建准确？决策有用？泛化能力？

2. **需要多少先验知识？**
   - 物理规律应该学习还是内置？

3. **如何评估世界理解？**
   - 不只是预测准确率，而是"理解"

4. **计算效率 vs 表示能力的平衡？**
   - JEPA 高效但信息可能丢失
   - 生成式完整但计算昂贵

5. **与大语言模型的关系？**
   - LLM 是否已经是某种世界模型？
   - 如何结合语言和物理世界模型？

---

## 参考文献

1. Ha & Schmidhuber. "World Models" (2018)
2. Hafner et al. "DreamerV3" (2023)
3. Bruce et al. "Genie: Generative Interactive Environments" (2024)
4. LeCun. "A Path Towards Autonomous Machine Intelligence" (2022)
5. Assran et al. "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture" (I-JEPA, 2023)
6. Bardes et al. "V-JEPA" (2024)
7. Reed et al. "Gato: A Generalist Agent" (2022)
8. Brohan et al. "RT-2: Vision-Language-Action Models" (2023)

---

> 世界模型的终极目标：让机器像人一样理解世界、想象未来、做出决策
