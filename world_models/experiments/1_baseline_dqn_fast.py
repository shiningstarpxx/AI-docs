"""
CartPole-v1 DQN - å¿«é€Ÿæµ‹è¯•ç‰ˆ
===========================
ä¼˜åŒ–å‚æ•°ç¡®ä¿å¿«é€Ÿæ”¶æ•›åˆ° 450+
"""

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import deque
import random
import json
from pathlib import Path

# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"
    
    # è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    episodes = 800  # å¢åŠ è®­ç»ƒè½®æ•°
    max_steps = 500
    batch_size = 64
    gamma = 0.99
    
    # æ¢ç´¢ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 0.9985  # æ›´æ…¢çš„è¡°å‡ï¼Œä¿è¯å……åˆ†æ¢ç´¢
    
    # ç½‘ç»œ
    hidden_size = 256  # å¢å¤§ç½‘ç»œå®¹é‡
    learning_rate = 1e-4  # æ›´å°çš„å­¦ä¹ ç‡
    
    # ç»éªŒå›æ”¾
    buffer_size = 20000  # å¢å¤§ buffer
    min_buffer_size = 1000  # æœ€å° buffer å†å¼€å§‹è®­ç»ƒ
    target_update_freq = 5  # æ›´é¢‘ç¹æ›´æ–°ç›®æ ‡ç½‘ç»œ
    
    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ—¥å¿—
    log_interval = 10
    save_dir = "./results_dqn"
    
    # æ—©åœï¼ˆæ”¶æ•›ååœæ­¢ï¼‰
    early_stop_threshold = 475  # è¿ç»­ N æ¬¡è¾¾åˆ°æ­¤åˆ†æ•°
    early_stop_window = 20


# ========== DQN ç½‘ç»œ ==========
class DQN(nn.Module):
    """Q ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_size=256):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),  # é¢å¤–ä¸€å±‚
            nn.ReLU(),
            nn.Linear(hidden_size, action_dim)
        )
    
    def forward(self, state):
        return self.network(state)


# ========== ç»éªŒå›æ”¾ ==========
class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )
    
    def __len__(self):
        return len(self.buffer)


# ========== DQN Agent ==========
class DQNAgent:
    """DQN æ™ºèƒ½ä½“"""
    def __init__(self, state_dim, action_dim, config):
        self.config = config
        self.action_dim = action_dim
        
        # Q ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim, config.hidden_size).to(config.device)
        self.target_network = DQN(state_dim, action_dim, config.hidden_size).to(config.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config.learning_rate)
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(config.buffer_size)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = config.epsilon_start
        
    def select_action(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰"""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.config.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax(1).item()
    
    def update(self):
        """æ›´æ–° Q ç½‘ç»œ"""
        if len(self.replay_buffer) < self.config.min_buffer_size:
            return 0.0
        
        # é‡‡æ ·
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(
            self.config.batch_size
        )
        
        # è½¬æ¢ä¸º tensor
        states = torch.FloatTensor(states).to(self.config.device)
        actions = torch.LongTensor(actions).to(self.config.device)
        rewards = torch.FloatTensor(rewards).to(self.config.device)
        next_states = torch.FloatTensor(next_states).to(self.config.device)
        dones = torch.FloatTensor(dones).to(self.config.device)
        
        # å½“å‰ Q å€¼
        q_values = self.q_network(states)
        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼ï¼ˆDouble DQNï¼‰
        with torch.no_grad():
            # ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_actions = self.q_network(next_states).argmax(1)
            # ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®— Q å€¼
            next_q_values = self.target_network(next_states)
            next_q_value = next_q_values.gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q_value = rewards + (1 - dones) * self.config.gamma * next_q_value
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(q_value, target_q_value)
        
        # æ›´æ–°ç½‘ç»œ
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.config.epsilon_end, 
                          self.epsilon * self.config.epsilon_decay)


# ========== è®­ç»ƒå‡½æ•° ==========
def train():
    """è®­ç»ƒ DQN"""
    config = Config()
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(config.env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(state_dim, action_dim, config)
    
    # è®°å½•
    episode_rewards = []
    episode_lengths = []
    moving_avg_rewards = []
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ DQNï¼ˆä¼˜åŒ–ç‰ˆï¼‰on {config.device}")
    print(f"ç¯å¢ƒ: {config.env_name}")
    print("=" * 50)
    
    total_steps = 0
    best_reward = 0
    convergence_count = 0
    
    for episode in range(1, config.episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        
        for step in range(config.max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒ
            agent.replay_buffer.push(state, action, reward, next_state, done)
            
            # æ›´æ–°ç½‘ç»œ
            if len(agent.replay_buffer) >= config.min_buffer_size:
                agent.update()
            
            episode_reward += reward
            episode_length += 1
            total_steps += 1
            state = next_state
            
            if done:
                break
        
        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % config.target_update_freq == 0:
            agent.update_target_network()
        
        # è®°å½•
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = 10
        if len(episode_rewards) >= window:
            moving_avg = np.mean(episode_rewards[-window:])
            moving_avg_rewards.append(moving_avg)
        else:
            moving_avg_rewards.append(np.mean(episode_rewards))
        
        # æ—©åœæ£€æŸ¥
        if episode_reward >= config.early_stop_threshold:
            convergence_count += 1
            if convergence_count >= config.early_stop_window:
                print(f"\nâœ… æå‰æ”¶æ•›ï¼è¿ç»­ {config.early_stop_window} æ¬¡è¾¾åˆ° {config.early_stop_threshold}+")
                break
        else:
            convergence_count = 0
        
        # è®°å½•æœ€ä½³
        if episode_reward > best_reward:
            best_reward = episode_reward
        
        # æ—¥å¿—
        if episode % config.log_interval == 0:
            avg_reward = np.mean(episode_rewards[-config.log_interval:])
            avg_length = np.mean(episode_lengths[-config.log_interval:])
            print(f"Episode {episode}/{config.episodes} | "
                  f"Avg Reward: {avg_reward:.2f} | "
                  f"Avg Length: {avg_length:.2f} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"Steps: {total_steps} | "
                  f"Best: {best_reward:.0f}")
    
    env.close()
    
    # ä¿å­˜ç»“æœ
    Path(config.save_dir).mkdir(exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    torch.save(agent.q_network.state_dict(), f"{config.save_dir}/model_final.pth")
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    training_data = {
        "episode_rewards": episode_rewards,
        "episode_lengths": episode_lengths,
        "moving_avg_rewards": moving_avg_rewards,
        "config": {
            "episodes": episode,
            "epsilon_decay": config.epsilon_decay,
            "learning_rate": config.learning_rate,
            "hidden_size": config.hidden_size,
        }
    }
    
    with open(f"{config.save_dir}/training_data.json", "w") as f:
        json.dump(training_data, f, indent=2)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - æ€» Episodes: {episode}")
    print(f"   - æ€»æ­¥æ•°: {total_steps}")
    print(f"   - æœ€ä½³å¥–åŠ±: {best_reward:.1f}")
    print(f"   - æœ€å10è½®å¹³å‡: {np.mean(episode_rewards[-10:]):.1f}")
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {config.save_dir}")
    
    return episode_rewards, episode_lengths


if __name__ == "__main__":
    train()
