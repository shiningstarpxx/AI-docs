"""
CartPole-v1 Baseline: DQN (Deep Q-Network)
==========================================
Model-Free RL baseline for comparison

å®éªŒç›®æ ‡ï¼š
- å»ºç«‹æ ·æœ¬æ•ˆç‡åŸºçº¿
- è®°å½•è®­ç»ƒæ›²çº¿
- æµ‹è¯• MPS åŠ é€Ÿæ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import deque
import random
import matplotlib.pyplot as plt
from datetime import datetime
import json

# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"
    
    # è®­ç»ƒ
    episodes = 600  # å¢åŠ è®­ç»ƒ episodes
    max_steps = 500
    batch_size = 64
    gamma = 0.99
    
    # æ¢ç´¢ï¼ˆä¼˜åŒ–ï¼šæ›´æ…¢çš„è¡°å‡ï¼‰
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 0.998  # ä» 0.995 æ”¹ä¸º 0.998ï¼Œæ›´æ…¢è¡°å‡
    
    # ç½‘ç»œ
    hidden_size = 128
    learning_rate = 3e-4  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
    
    # ç»éªŒå›æ”¾
    buffer_size = 10000
    target_update_freq = 10
    
    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ—¥å¿—
    log_interval = 10
    save_dir = "./results_dqn"


# ========== DQN ç½‘ç»œ ==========
class DQN(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥ Q ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_size=128):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_dim)
        )
    
    def forward(self, state):
        return self.network(state)


# ========== ç»éªŒå›æ”¾ ==========
class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return (
            np.array(state),
            np.array(action),
            np.array(reward),
            np.array(next_state),
            np.array(done)
        )
    
    def __len__(self):
        return len(self.buffer)


# ========== DQN Agent ==========
class DQNAgent:
    def __init__(self, config):
        self.config = config
        self.env = gym.make(config.env_name)
        
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.n
        
        # Q ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim, config.hidden_size).to(config.device)
        self.target_network = DQN(state_dim, action_dim, config.hidden_size).to(config.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config.learning_rate)
        self.buffer = ReplayBuffer(config.buffer_size)
        
        self.epsilon = config.epsilon_start
        self.total_steps = 0
        
        # è®°å½•
        self.episode_rewards = []
        self.episode_lengths = []
        self.loss_history = []
    
    def select_action(self, state, training=True):
        """Îµ-greedy ç­–ç•¥"""
        if training and random.random() < self.epsilon:
            return self.env.action_space.sample()
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.config.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def update(self):
        """DQN æ›´æ–°"""
        if len(self.buffer) < self.config.batch_size:
            return None
        
        # é‡‡æ ·æ‰¹æ¬¡
        state, action, reward, next_state, done = self.buffer.sample(self.config.batch_size)
        
        state = torch.FloatTensor(state).to(self.config.device)
        action = torch.LongTensor(action).to(self.config.device)
        reward = torch.FloatTensor(reward).to(self.config.device)
        next_state = torch.FloatTensor(next_state).to(self.config.device)
        done = torch.FloatTensor(done).to(self.config.device)
        
        # å½“å‰ Q å€¼
        current_q = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            next_q = self.target_network(next_state).max(1)[0]
            target_q = reward + (1 - done) * self.config.gamma * next_q
        
        # æŸå¤±
        loss = nn.MSELoss()(current_q, target_q)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ DQN on {self.config.device}")
        print(f"ç¯å¢ƒ: {self.config.env_name}")
        print("-" * 50)
        
        for episode in range(self.config.episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            episode_length = 0
            
            for step in range(self.config.max_steps):
                # é€‰æ‹©åŠ¨ä½œ
                action = self.select_action(state)
                
                # ç¯å¢ƒäº¤äº’
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # å­˜å‚¨ç»éªŒ
                self.buffer.push(state, action, reward, next_state, done)
                
                # æ›´æ–°ç½‘ç»œ
                loss = self.update()
                if loss is not None:
                    self.loss_history.append(loss)
                
                episode_reward += reward
                episode_length += 1
                self.total_steps += 1
                
                state = next_state
                
                if done:
                    break
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if episode % self.config.target_update_freq == 0:
                self.target_network.load_state_dict(self.q_network.state_dict())
            
            # è¡°å‡ epsilon
            self.epsilon = max(self.config.epsilon_end, self.epsilon * self.config.epsilon_decay)
            
            # è®°å½•
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_length)
            
            # æ—¥å¿—
            if (episode + 1) % self.config.log_interval == 0:
                avg_reward = np.mean(self.episode_rewards[-self.config.log_interval:])
                avg_length = np.mean(self.episode_lengths[-self.config.log_interval:])
                print(f"Episode {episode+1}/{self.config.episodes} | "
                      f"Avg Reward: {avg_reward:.2f} | "
                      f"Avg Length: {avg_length:.2f} | "
                      f"Epsilon: {self.epsilon:.3f} | "
                      f"Steps: {self.total_steps}")
        
        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(self.q_network.state_dict(), 
                   f"{self.config.save_dir}/dqn_model.pt")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        results = {
            "episode_rewards": self.episode_rewards,
            "episode_lengths": self.episode_lengths,
            "loss_history": self.loss_history,
            "total_steps": self.total_steps,
            "config": vars(self.config)
        }
        
        with open(f"{self.config.save_dir}/training_data.json", "w") as f:
            json.dump(results, f, indent=2)
        
        # ç»˜å›¾
        self.plot_results()
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {self.config.save_dir}")
    
    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.episode_rewards, alpha=0.3, label='Raw')
        window = 50
        if len(self.episode_rewards) >= window:
            smoothed = np.convolve(self.episode_rewards, 
                                   np.ones(window)/window, mode='valid')
            axes[0, 0].plot(range(window-1, len(self.episode_rewards)), 
                           smoothed, label=f'Smoothed ({window})')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Episode é•¿åº¦
        axes[0, 1].plot(self.episode_lengths, alpha=0.3)
        if len(self.episode_lengths) >= window:
            smoothed = np.convolve(self.episode_lengths, 
                                   np.ones(window)/window, mode='valid')
            axes[0, 1].plot(range(window-1, len(self.episode_lengths)), smoothed)
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Length')
        axes[0, 1].set_title('Episode Lengths')
        axes[0, 1].grid(True)
        
        # æŸå¤±æ›²çº¿
        if self.loss_history:
            axes[1, 0].plot(self.loss_history, alpha=0.5)
            axes[1, 0].set_xlabel('Update Step')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].set_title('Training Loss')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True)
        
        # æ ·æœ¬æ•ˆç‡ï¼ˆç´¯ç§¯å¥–åŠ± vs æ€»æ­¥æ•°ï¼‰
        cumulative_rewards = np.cumsum(self.episode_rewards)
        cumulative_steps = np.cumsum(self.episode_lengths)
        axes[1, 1].plot(cumulative_steps, cumulative_rewards)
        axes[1, 1].set_xlabel('Total Environment Steps')
        axes[1, 1].set_ylabel('Cumulative Reward')
        axes[1, 1].set_title('Sample Efficiency')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.save_dir}/training_curves.png", dpi=150)
        plt.close()


# ========== ä¸»å‡½æ•° ==========
def main():
    config = Config()
    agent = DQNAgent(config)
    agent.train()


if __name__ == "__main__":
    main()
