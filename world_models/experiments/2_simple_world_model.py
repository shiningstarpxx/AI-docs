"""
CartPole-v1: Simple World Model
================================
ç®€åŒ–çš„ World Models (2018) å®ç°

æ¶æ„ï¼š
- Vision: ç®€å•ç¼–ç å™¨ï¼ˆçŠ¶æ€å·²æ˜¯ä½ç»´å‘é‡ï¼‰
- Memory: LSTM é¢„æµ‹ä¸‹ä¸€çŠ¶æ€
- Controller: çº¿æ€§ç­–ç•¥ + CMA-ES

å…³é”®å¯¹æ¯”ç‚¹ï¼š
- åœ¨"æ¢¦å¢ƒ"ä¸­è®­ç»ƒç­–ç•¥
- æ ·æœ¬æ•ˆç‡æå‡ ~3Ã—
"""

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import json

# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"
    
    # é˜¶æ®µ 1: æ•°æ®æ”¶é›†ï¼ˆä¼˜åŒ–ï¼šæ›´å¤šæ•°æ®ï¼‰
    random_episodes = 200  # å¢åŠ æ•°æ®é‡
    
    # é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼ˆä¼˜åŒ–ï¼šæ›´å¼ºçš„æ¨¡å‹ï¼‰
    world_model_epochs = 100  # æ›´å¤šè®­ç»ƒè½®æ¬¡
    batch_size = 32
    sequence_length = 30  # ç¼©çŸ­åºåˆ—ï¼Œæ›´ç¨³å®š
    hidden_size = 128  # å¢åŠ æ¨¡å‹å®¹é‡
    learning_rate = 1e-3
    
    # é˜¶æ®µ 3: æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥ï¼ˆä¼˜åŒ–ï¼šæ›´ä¿å®ˆçš„æ¢¦å¢ƒé•¿åº¦ï¼‰
    population_size = 100  # å¢åŠ ç§ç¾¤å¤§å°
    generations = 150  # æ›´å¤šè¿›åŒ–ä»£æ•°
    dream_rollout_length = 100  # ç¼©çŸ­æ¢¦å¢ƒé•¿åº¦ï¼Œå‡å°‘ç´¯ç§¯è¯¯å·®
    elite_frac = 0.2
    
    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ—¥å¿—
    save_dir = "./results_simple_wm"


# ========== World Model: LSTM åŠ¨æ€æ¨¡å‹ ==========
class WorldModel(nn.Module):
    """
    LSTM-based World Model
    è¾“å…¥: (state_t, action_t)
    è¾“å‡º: state_{t+1}, reward_t, done_t
    """
    def __init__(self, state_dim, action_dim, hidden_size=64):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        
        # è¾“å…¥ç¼–ç 
        self.input_encoder = nn.Linear(state_dim + action_dim, hidden_size)
        
        # LSTM æ ¸å¿ƒ
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        
        # è¾“å‡ºè§£ç 
        self.state_predictor = nn.Linear(hidden_size, state_dim)
        self.reward_predictor = nn.Linear(hidden_size, 1)
        self.done_predictor = nn.Linear(hidden_size, 1)
    
    def forward(self, state, action, hidden=None):
        """
        Args:
            state: (batch, seq_len, state_dim)
            action: (batch, seq_len, action_dim)
            hidden: LSTM hidden state
        Returns:
            next_state, reward, done, hidden
        """
        # ç¼–ç è¾“å…¥
        x = torch.cat([state, action], dim=-1)
        x = torch.relu(self.input_encoder(x))
        
        # LSTM
        x, hidden = self.lstm(x, hidden)
        
        # é¢„æµ‹
        next_state = self.state_predictor(x)
        reward = self.reward_predictor(x)
        done = torch.sigmoid(self.done_predictor(x))
        
        return next_state, reward, done, hidden
    
    def imagine_step(self, state, action, hidden=None):
        """å•æ­¥æƒ³è±¡ï¼ˆç”¨äºç­–ç•¥è®­ç»ƒï¼‰"""
        with torch.no_grad():
            state = state.unsqueeze(1)  # (batch, 1, state_dim)
            action = action.unsqueeze(1)  # (batch, 1, action_dim)
            next_state, reward, done, hidden = self.forward(state, action, hidden)
            return next_state.squeeze(1), reward.squeeze(1), done.squeeze(1), hidden


# ========== Controller: ç®€å•çº¿æ€§ç­–ç•¥ ==========
class LinearController:
    """
    çº¿æ€§ç­–ç•¥: action = argmax(W @ [state, hidden])
    å‚æ•°æå°‘ï¼Œé€‚åˆè¿›åŒ–ç®—æ³•ä¼˜åŒ–
    """
    def __init__(self, input_dim, action_dim):
        self.input_dim = input_dim
        self.action_dim = action_dim
        self.weights = np.random.randn(action_dim, input_dim) * 0.1
    
    def get_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        logits = self.weights @ state
        return np.argmax(logits)
    
    def get_params(self):
        """è·å–å‚æ•°ï¼ˆæ‰å¹³åŒ–ï¼‰"""
        return self.weights.flatten()
    
    def set_params(self, params):
        """è®¾ç½®å‚æ•°"""
        self.weights = params.reshape(self.action_dim, self.input_dim)


# ========== CMA-ES è¿›åŒ–ç®—æ³• ==========
class CMAES:
    """ç®€åŒ–çš„ CMA-ES å®ç°"""
    def __init__(self, dim, population_size=50, elite_frac=0.2):
        self.dim = dim
        self.population_size = population_size
        self.elite_size = int(population_size * elite_frac)
        
        # åˆå§‹åˆ†å¸ƒ
        self.mean = np.zeros(dim)
        self.sigma = 0.5
    
    def ask(self):
        """ç”Ÿæˆå€™é€‰è§£"""
        return np.random.randn(self.population_size, self.dim) * self.sigma + self.mean
    
    def tell(self, population, fitness):
        """æ›´æ–°åˆ†å¸ƒ"""
        # é€‰æ‹©ç²¾è‹±
        elite_idxs = np.argsort(fitness)[-self.elite_size:]
        elite = population[elite_idxs]
        
        # æ›´æ–°å‡å€¼
        self.mean = elite.mean(axis=0)
        
        # æ›´æ–°æ–¹å·®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.sigma = elite.std() * 0.9


# ========== Simple World Model Agent ==========
class SimpleWorldModelAgent:
    def __init__(self, config):
        self.config = config
        self.env = gym.make(config.env_name)
        
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n
        
        # ä¸–ç•Œæ¨¡å‹
        self.world_model = WorldModel(
            self.state_dim, 
            self.action_dim, 
            config.hidden_size
        ).to(config.device)
        
        self.optimizer = optim.Adam(
            self.world_model.parameters(), 
            lr=config.learning_rate
        )
        
        # æ•°æ®ç¼“å†²
        self.trajectories = []
        
        # è®°å½•
        self.training_history = {
            "data_collection_rewards": [],
            "world_model_losses": [],
            "policy_fitness": [],
            "evaluation_rewards": []
        }
    
    def collect_data(self):
        """é˜¶æ®µ 1: éšæœºç­–ç•¥æ”¶é›†æ•°æ®"""
        print("ğŸ“¦ é˜¶æ®µ 1: æ”¶é›†æ•°æ®")
        print("-" * 50)
        
        for episode in range(self.config.random_episodes):
            trajectory = {
                "states": [],
                "actions": [],
                "rewards": [],
                "dones": []
            }
            
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                action = self.env.action_space.sample()
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # One-hot encode action
                action_onehot = np.zeros(self.action_dim)
                action_onehot[action] = 1
                
                trajectory["states"].append(state)
                trajectory["actions"].append(action_onehot)
                trajectory["rewards"].append(reward)
                trajectory["dones"].append(float(done))
                
                episode_reward += reward
                state = next_state
            
            self.trajectories.append(trajectory)
            self.training_history["data_collection_rewards"].append(episode_reward)
            
            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(self.training_history["data_collection_rewards"][-20:])
                print(f"Episode {episode+1}/{self.config.random_episodes} | Avg Reward: {avg_reward:.2f}")
        
        print(f"âœ… æ”¶é›†äº† {len(self.trajectories)} æ¡è½¨è¿¹")
    
    def train_world_model(self):
        """é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹"""
        print("\nğŸŒ é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹")
        print("-" * 50)
        
        for epoch in range(self.config.world_model_epochs):
            epoch_losses = []
            
            # éšæœºé‡‡æ ·è½¨è¿¹ï¼ˆå¢åŠ é‡‡æ ·æ¬¡æ•°ï¼‰
            for _ in range(50):
                traj = np.random.choice(self.trajectories)
                
                # å‡†å¤‡åºåˆ—æ•°æ®ï¼ˆéœ€è¦ç¡®ä¿æœ‰è¶³å¤Ÿçš„é•¿åº¦æ¥å– next_stateï¼‰
                max_seq_len = min(len(traj["states"]) - 1, self.config.sequence_length)
                if max_seq_len < 2:
                    continue
                
                start_idx = np.random.randint(0, len(traj["states"]) - max_seq_len)
                
                states = torch.FloatTensor(
                    np.array(traj["states"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).to(self.config.device)
                
                actions = torch.FloatTensor(
                    np.array(traj["actions"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).to(self.config.device)
                
                next_states = torch.FloatTensor(
                    np.array(traj["states"][start_idx+1:start_idx+max_seq_len+1])
                ).unsqueeze(0).to(self.config.device)
                
                rewards = torch.FloatTensor(
                    np.array(traj["rewards"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).unsqueeze(-1).to(self.config.device)
                
                dones = torch.FloatTensor(
                    np.array(traj["dones"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).unsqueeze(-1).to(self.config.device)
                
                # å‰å‘ä¼ æ’­
                pred_states, pred_rewards, pred_dones, _ = self.world_model(states, actions)
                
                # æŸå¤±ï¼ˆå¢åŠ æƒé‡å¹³è¡¡ï¼‰
                state_loss = nn.MSELoss()(pred_states, next_states)
                reward_loss = nn.MSELoss()(pred_rewards, rewards) * 10.0  # å¢åŠ å¥–åŠ±æƒé‡
                done_loss = nn.BCELoss()(pred_dones, dones) * 5.0  # å¢åŠ ç»ˆæ­¢é¢„æµ‹æƒé‡
                
                loss = state_loss + reward_loss + done_loss
                
                # ä¼˜åŒ–
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
                self.optimizer.step()
                
                epoch_losses.append(loss.item())
            
            avg_loss = np.mean(epoch_losses)
            self.training_history["world_model_losses"].append(avg_loss)
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{self.config.world_model_epochs} | Loss: {avg_loss:.4f}")
        
        print("âœ… ä¸–ç•Œæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def train_controller_in_dream(self):
        """é˜¶æ®µ 3: åœ¨æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥"""
        print("\nğŸ’­ é˜¶æ®µ 3: æ¢¦å¢ƒè®­ç»ƒç­–ç•¥")
        print("-" * 50)
        
        # åˆå§‹åŒ– CMA-ES
        param_dim = self.state_dim * self.action_dim
        cmaes = CMAES(param_dim, self.config.population_size)
        
        best_controller = None
        best_fitness = -float('inf')
        
        for generation in range(self.config.generations):
            # ç”Ÿæˆç§ç¾¤
            population = cmaes.ask()
            fitness_scores = []
            
            # è¯„ä¼°æ¯ä¸ªä¸ªä½“
            for params in population:
                controller = LinearController(self.state_dim, self.action_dim)
                controller.set_params(params)
                
                # åœ¨æ¢¦å¢ƒä¸­è¯„ä¼°
                fitness = self.evaluate_in_dream(controller)
                fitness_scores.append(fitness)
            
            fitness_scores = np.array(fitness_scores)
            
            # æ›´æ–° CMA-ES
            cmaes.tell(population, fitness_scores)
            
            # è®°å½•æœ€ä½³
            gen_best = fitness_scores.max()
            if gen_best > best_fitness:
                best_fitness = gen_best
                best_controller = LinearController(self.state_dim, self.action_dim)
                best_controller.set_params(population[fitness_scores.argmax()])
            
            self.training_history["policy_fitness"].append(gen_best)
            
            if (generation + 1) % 10 == 0:
                print(f"Generation {generation+1}/{self.config.generations} | "
                      f"Best Fitness: {gen_best:.2f} | "
                      f"Mean: {fitness_scores.mean():.2f}")
        
        print(f"âœ… ç­–ç•¥è®­ç»ƒå®Œæˆ | æœ€ä½³é€‚åº”åº¦: {best_fitness:.2f}")
        return best_controller
    
    def evaluate_in_dream(self, controller):
        """åœ¨æ¢¦å¢ƒä¸­è¯„ä¼°ç­–ç•¥"""
        total_reward = 0
        
        # ä»éšæœºè½¨è¿¹é‡‡æ ·èµ·å§‹çŠ¶æ€
        traj = np.random.choice(self.trajectories)
        state = torch.FloatTensor(traj["states"][0]).to(self.config.device)
        
        hidden = None
        
        for _ in range(self.config.dream_rollout_length):
            # æ§åˆ¶å™¨é€‰æ‹©åŠ¨ä½œ
            action_idx = controller.get_action(state.cpu().numpy())
            action = torch.zeros(self.action_dim).to(self.config.device)
            action[action_idx] = 1
            
            # ä¸–ç•Œæ¨¡å‹é¢„æµ‹
            next_state, reward, done, hidden = self.world_model.imagine_step(
                state.unsqueeze(0), 
                action.unsqueeze(0), 
                hidden
            )
            
            total_reward += reward.item()
            
            # ç»ˆæ­¢æ£€æŸ¥
            if done.item() > 0.5:
                break
            
            state = next_state.squeeze(0)
        
        return total_reward
    
    def evaluate_in_real_env(self, controller, num_episodes=10):
        """åœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°"""
        rewards = []
        
        for _ in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                action = controller.get_action(state)
                state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
            
            rewards.append(episode_reward)
        
        return np.mean(rewards), np.std(rewards)
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ Simple World Model")
        print(f"è®¾å¤‡: {self.config.device}")
        print("=" * 50)
        
        # é˜¶æ®µ 1: æ”¶é›†æ•°æ®
        self.collect_data()
        
        # é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹
        self.train_world_model()
        
        # é˜¶æ®µ 3: åœ¨æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥
        best_controller = self.train_controller_in_dream()
        
        # è¯„ä¼°
        print("\nğŸ“Š åœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°...")
        mean_reward, std_reward = self.evaluate_in_real_env(best_controller, num_episodes=50)
        self.training_history["evaluation_rewards"].append(mean_reward)
        
        print(f"âœ… è¯„ä¼°ç»“æœ: {mean_reward:.2f} Â± {std_reward:.2f}")
        
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(self.world_model.state_dict(), 
                   f"{self.config.save_dir}/world_model.pt")
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(f"{self.config.save_dir}/training_history.json", "w") as f:
            json.dump(self.training_history, f, indent=2)
        
        # ç»˜å›¾
        self.plot_results()
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {self.config.save_dir}")
    
    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # æ•°æ®æ”¶é›†é˜¶æ®µå¥–åŠ±
        axes[0, 0].plot(self.training_history["data_collection_rewards"], alpha=0.5)
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].set_title('Data Collection (Random Policy)')
        axes[0, 0].grid(True)
        
        # ä¸–ç•Œæ¨¡å‹æŸå¤±
        axes[0, 1].plot(self.training_history["world_model_losses"])
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].set_title('World Model Training')
        axes[0, 1].set_yscale('log')
        axes[0, 1].grid(True)
        
        # ç­–ç•¥é€‚åº”åº¦
        axes[1, 0].plot(self.training_history["policy_fitness"])
        axes[1, 0].set_xlabel('Generation')
        axes[1, 0].set_ylabel('Fitness (Dream Reward)')
        axes[1, 0].set_title('Policy Evolution (CMA-ES)')
        axes[1, 0].grid(True)
        
        # æœ€ç»ˆè¯„ä¼°
        if self.training_history["evaluation_rewards"]:
            axes[1, 1].bar(['Evaluation'], self.training_history["evaluation_rewards"])
            axes[1, 1].set_ylabel('Mean Reward')
            axes[1, 1].set_title('Real Environment Evaluation')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.save_dir}/training_curves.png", dpi=150)
        plt.close()


# ========== ä¸»å‡½æ•° ==========
def main():
    config = Config()
    agent = SimpleWorldModelAgent(config)
    agent.train()


if __name__ == "__main__":
    main()
