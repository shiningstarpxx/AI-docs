"""
CartPole-v1: Full World Model Implementation
=============================================
å®Œæ•´çš„ World Models (2018) å®ç°ï¼ŒåŒ…å« MDN-RNN

æ¶æ„ï¼š
- Vision: çŠ¶æ€ç¼–ç å™¨ï¼ˆCartPole çŠ¶æ€å·²æ˜¯ä½ç»´ï¼Œç”¨ç®€å• MLPï¼‰
- Memory: MDN-LSTM é¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„åˆ†å¸ƒ
- Controller: çº¿æ€§ç­–ç•¥ + CMA-ES

ä¸ç®€åŒ–ç‰ˆçš„åŒºåˆ«ï¼š
- ä½¿ç”¨ MDN (Mixture Density Network) è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
- æ›´æ¥è¿‘è®ºæ–‡åŸç‰ˆæ¶æ„

å‚è€ƒè®ºæ–‡ï¼š
- World Models (Ha & Schmidhuber, 2018)
- https://arxiv.org/abs/1803.10122
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import gymnasium as gym
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import json
import math
import os


# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"

    # é˜¶æ®µ 1: æ•°æ®æ”¶é›†
    random_episodes = 100  # éšæœºç­–ç•¥æ”¶é›†æ•°æ®

    # é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹
    world_model_epochs = 100
    batch_size = 32
    sequence_length = 50  # LSTM åºåˆ—é•¿åº¦
    hidden_size = 64      # LSTM éšè—å±‚å¤§å°
    latent_size = 16      # æ½œåœ¨ç©ºé—´ç»´åº¦ (çŠ¶æ€ç¼–ç )
    n_gaussians = 5       # MDN æ··åˆé«˜æ–¯åˆ†é‡æ•°
    learning_rate = 1e-3

    # é˜¶æ®µ 3: æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥ (CMA-ES)
    population_size = 50
    generations = 100
    dream_rollout_length = 200
    elite_frac = 0.2
    temperature = 1.0     # é‡‡æ ·æ¸©åº¦

    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else
                         "cuda" if torch.cuda.is_available() else "cpu")

    # æ—¥å¿—
    save_dir = "./results_world_model_full"


# ========== Vision Model: çŠ¶æ€ç¼–ç å™¨ ==========
class VisionEncoder(nn.Module):
    """
    V: Vision Model

    å¯¹äº CartPoleï¼ŒçŠ¶æ€å·²ç»æ˜¯ä½ç»´å‘é‡ (4ç»´)ï¼Œ
    ä½†ä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘ä»¬ä»ç„¶åŠ å…¥ä¸€ä¸ªç¼–ç å™¨ã€‚

    åœ¨å›¾åƒä»»åŠ¡ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ VAEã€‚
    """
    def __init__(self, state_dim, latent_size):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, 32),
            nn.ReLU(),
            nn.Linear(32, latent_size * 2)  # è¾“å‡º Î¼ å’Œ log_Ïƒ
        )
        self.latent_size = latent_size

    def forward(self, x):
        """ç¼–ç çŠ¶æ€åˆ°æ½œåœ¨ç©ºé—´"""
        h = self.encoder(x)
        mu, log_sigma = h.chunk(2, dim=-1)
        return mu, log_sigma

    def sample(self, mu, log_sigma):
        """é‡å‚æ•°åŒ–é‡‡æ ·"""
        sigma = torch.exp(log_sigma)
        eps = torch.randn_like(sigma)
        z = mu + sigma * eps
        return z

    def encode(self, x):
        """ç¼–ç å¹¶é‡‡æ ·"""
        mu, log_sigma = self.forward(x)
        z = self.sample(mu, log_sigma)
        return z, mu, log_sigma


class VisionDecoder(nn.Module):
    """è§£ç å™¨ï¼šä»æ½œåœ¨ç©ºé—´é‡å»ºçŠ¶æ€"""
    def __init__(self, latent_size, state_dim):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(latent_size, 32),
            nn.ReLU(),
            nn.Linear(32, state_dim)
        )

    def forward(self, z):
        return self.decoder(z)


# ========== MDN (Mixture Density Network) ==========
class MDN(nn.Module):
    """
    Mixture Density Network

    è¾“å‡ºæ··åˆé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼š
    - Ï€: æ··åˆæƒé‡ (Kä¸ª)
    - Î¼: å‡å€¼ (K * output_dim ä¸ª)
    - Ïƒ: æ ‡å‡†å·® (K * output_dim ä¸ª)
    """
    def __init__(self, input_size, output_size, n_gaussians):
        super().__init__()
        self.output_size = output_size
        self.n_gaussians = n_gaussians

        # è¾“å‡ºå±‚
        self.pi_layer = nn.Linear(input_size, n_gaussians)
        self.mu_layer = nn.Linear(input_size, n_gaussians * output_size)
        self.sigma_layer = nn.Linear(input_size, n_gaussians * output_size)

    def forward(self, x):
        """
        Args:
            x: (batch, input_size)
        Returns:
            pi: (batch, n_gaussians) - æ··åˆæƒé‡
            mu: (batch, n_gaussians, output_size) - å‡å€¼
            sigma: (batch, n_gaussians, output_size) - æ ‡å‡†å·®
        """
        pi = F.softmax(self.pi_layer(x), dim=-1)
        mu = self.mu_layer(x).view(-1, self.n_gaussians, self.output_size)
        sigma = torch.exp(self.sigma_layer(x)).view(-1, self.n_gaussians, self.output_size)
        # é™åˆ¶ sigma çš„èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼é—®é¢˜
        sigma = torch.clamp(sigma, min=1e-4, max=10.0)
        return pi, mu, sigma

    def sample(self, pi, mu, sigma, temperature=1.0):
        """
        ä»æ··åˆé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·

        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œ>1 å¢åŠ éšæœºæ€§ï¼Œ<1 å‡å°‘éšæœºæ€§
        """
        batch_size = pi.shape[0]

        # æŒ‰æƒé‡é€‰æ‹©é«˜æ–¯åˆ†é‡
        pi_temp = pi ** (1.0 / temperature)
        pi_temp = pi_temp / pi_temp.sum(dim=-1, keepdim=True)

        # é‡‡æ ·åˆ†é‡ç´¢å¼•
        indices = torch.multinomial(pi_temp, 1).squeeze(-1)  # (batch,)

        # è·å–å¯¹åº”çš„ Î¼ å’Œ Ïƒ
        batch_idx = torch.arange(batch_size, device=mu.device)
        mu_selected = mu[batch_idx, indices]  # (batch, output_size)
        sigma_selected = sigma[batch_idx, indices] * temperature

        # ä»é€‰ä¸­çš„é«˜æ–¯åˆ†å¸ƒé‡‡æ ·
        eps = torch.randn_like(mu_selected)
        sample = mu_selected + sigma_selected * eps

        return sample

    def log_prob(self, pi, mu, sigma, target):
        """
        è®¡ç®—ç›®æ ‡å€¼çš„å¯¹æ•°æ¦‚ç‡

        Args:
            target: (batch, output_size)
        Returns:
            log_prob: (batch,)
        """
        target = target.unsqueeze(1)  # (batch, 1, output_size)

        # è®¡ç®—æ¯ä¸ªé«˜æ–¯åˆ†é‡çš„æ¦‚ç‡å¯†åº¦
        # N(x | Î¼, Ïƒ) = (1 / sqrt(2Ï€ÏƒÂ²)) * exp(-(x-Î¼)Â² / 2ÏƒÂ²)
        var = sigma ** 2
        log_prob_per_dim = -0.5 * (
            math.log(2 * math.pi) +
            torch.log(var) +
            (target - mu) ** 2 / var
        )

        # å¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œ (å‡è®¾ç»´åº¦ç‹¬ç«‹)
        log_prob_per_gaussian = log_prob_per_dim.sum(dim=-1)  # (batch, n_gaussians)

        # æ··åˆï¼šlog(Î£ Ï€_i * exp(log_prob_i))
        # ä½¿ç”¨ log-sum-exp æŠ€å·§ä¿è¯æ•°å€¼ç¨³å®šæ€§
        log_pi = torch.log(pi + 1e-8)
        log_prob = torch.logsumexp(log_pi + log_prob_per_gaussian, dim=-1)

        return log_prob


# ========== Memory Model: MDN-LSTM ==========
class MemoryModel(nn.Module):
    """
    M: Memory Model (MDN-LSTM)

    è¾“å…¥: (z_t, a_t, h_{t-1})
    è¾“å‡º: P(z_{t+1}) = Î£ Ï€_i * N(Î¼_i, Ïƒ_iÂ²)

    åŒæ—¶é¢„æµ‹ reward å’Œ done
    """
    def __init__(self, latent_size, action_dim, hidden_size, n_gaussians):
        super().__init__()
        self.hidden_size = hidden_size
        self.latent_size = latent_size

        # è¾“å…¥ç¼–ç 
        input_size = latent_size + action_dim
        self.input_encoder = nn.Linear(input_size, hidden_size)

        # LSTM æ ¸å¿ƒ
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)

        # MDN è¾“å‡ºä¸‹ä¸€çŠ¶æ€åˆ†å¸ƒ
        self.mdn = MDN(hidden_size, latent_size, n_gaussians)

        # å¥–åŠ±å’Œç»ˆæ­¢é¢„æµ‹
        self.reward_head = nn.Linear(hidden_size, 1)
        self.done_head = nn.Linear(hidden_size, 1)

    def forward(self, z, action, hidden=None):
        """
        Args:
            z: (batch, seq_len, latent_size)
            action: (batch, seq_len, action_dim)
            hidden: LSTM éšè—çŠ¶æ€
        Returns:
            pi, mu, sigma: MDN å‚æ•°
            reward: é¢„æµ‹å¥–åŠ±
            done: é¢„æµ‹ç»ˆæ­¢æ¦‚ç‡
            hidden: æ›´æ–°åçš„éšè—çŠ¶æ€
        """
        batch_size, seq_len, _ = z.shape

        # ç¼–ç è¾“å…¥
        x = torch.cat([z, action], dim=-1)
        x = torch.relu(self.input_encoder(x))

        # LSTM
        lstm_out, hidden = self.lstm(x, hidden)

        # é‡å¡‘ç”¨äº MDN
        lstm_out_flat = lstm_out.reshape(batch_size * seq_len, -1)

        # MDN é¢„æµ‹ä¸‹ä¸€çŠ¶æ€åˆ†å¸ƒ
        pi, mu, sigma = self.mdn(lstm_out_flat)

        # é‡å¡‘å›åºåˆ—å½¢å¼
        pi = pi.view(batch_size, seq_len, -1)
        mu = mu.view(batch_size, seq_len, self.mdn.n_gaussians, self.latent_size)
        sigma = sigma.view(batch_size, seq_len, self.mdn.n_gaussians, self.latent_size)

        # å¥–åŠ±å’Œç»ˆæ­¢é¢„æµ‹
        reward = self.reward_head(lstm_out)
        done = torch.sigmoid(self.done_head(lstm_out))

        return pi, mu, sigma, reward, done, hidden

    def imagine_step(self, z, action, hidden=None, temperature=1.0):
        """
        å•æ­¥æƒ³è±¡ï¼ˆç”¨äºç­–ç•¥è®­ç»ƒï¼‰

        Args:
            z: (batch, latent_size)
            action: (batch, action_dim)
            temperature: é‡‡æ ·æ¸©åº¦
        """
        with torch.no_grad():
            z = z.unsqueeze(1)  # (batch, 1, latent_size)
            action = action.unsqueeze(1)  # (batch, 1, action_dim)

            pi, mu, sigma, reward, done, hidden = self.forward(z, action, hidden)

            # ä»åˆ†å¸ƒä¸­é‡‡æ ·ä¸‹ä¸€çŠ¶æ€
            pi = pi.squeeze(1)  # (batch, n_gaussians)
            mu = mu.squeeze(1)  # (batch, n_gaussians, latent_size)
            sigma = sigma.squeeze(1)

            next_z = self.mdn.sample(pi, mu, sigma, temperature)

            return next_z, reward.squeeze(1), done.squeeze(1), hidden


# ========== Controller: çº¿æ€§ç­–ç•¥ ==========
class LinearController:
    """
    C: Controller

    çº¿æ€§ç­–ç•¥: action = argmax(W @ [z, h])
    å‚æ•°æå°‘ï¼Œé€‚åˆè¿›åŒ–ç®—æ³•ä¼˜åŒ–

    æ³¨æ„ï¼šä½¿ç”¨æ½œåœ¨ç©ºé—´ z å’Œ LSTM éšè—çŠ¶æ€ h
    """
    def __init__(self, input_dim, action_dim):
        self.input_dim = input_dim
        self.action_dim = action_dim
        self.weights = np.random.randn(action_dim, input_dim) * 0.1
        self.bias = np.zeros(action_dim)

    def get_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        logits = self.weights @ state + self.bias
        return np.argmax(logits)

    def get_params(self):
        """è·å–å‚æ•°ï¼ˆæ‰å¹³åŒ–ï¼‰"""
        return np.concatenate([self.weights.flatten(), self.bias])

    def set_params(self, params):
        """è®¾ç½®å‚æ•°"""
        w_size = self.action_dim * self.input_dim
        self.weights = params[:w_size].reshape(self.action_dim, self.input_dim)
        self.bias = params[w_size:]

    @property
    def num_params(self):
        return self.action_dim * self.input_dim + self.action_dim


# ========== CMA-ES è¿›åŒ–ç®—æ³• ==========
class CMAES:
    """
    ç®€åŒ–çš„ CMA-ES å®ç°

    CMA-ES (Covariance Matrix Adaptation Evolution Strategy)
    æ˜¯ä¸€ç§æ— æ¢¯åº¦ä¼˜åŒ–ç®—æ³•ï¼Œé€‚åˆä¼˜åŒ–å‚æ•°è¾ƒå°‘çš„ç­–ç•¥
    """
    def __init__(self, dim, population_size=50, elite_frac=0.2, sigma=0.5):
        self.dim = dim
        self.population_size = population_size
        self.elite_size = int(population_size * elite_frac)

        # åˆå§‹åˆ†å¸ƒå‚æ•°
        self.mean = np.zeros(dim)
        self.sigma = sigma

        # åæ–¹å·®çŸ©é˜µï¼ˆç®€åŒ–ä¸ºå¯¹è§’ï¼‰
        self.cov = np.ones(dim)

    def ask(self):
        """ç”Ÿæˆå€™é€‰è§£"""
        # ä»æ­£æ€åˆ†å¸ƒé‡‡æ ·
        noise = np.random.randn(self.population_size, self.dim)
        population = self.mean + self.sigma * np.sqrt(self.cov) * noise
        return population

    def tell(self, population, fitness):
        """æ ¹æ®é€‚åº”åº¦æ›´æ–°åˆ†å¸ƒ"""
        # é€‰æ‹©ç²¾è‹±ï¼ˆé€‚åº”åº¦æœ€é«˜çš„ï¼‰
        elite_idxs = np.argsort(fitness)[-self.elite_size:]
        elite = population[elite_idxs]
        elite_fitness = fitness[elite_idxs]

        # åŠ æƒæ›´æ–°å‡å€¼
        weights = np.exp(elite_fitness - elite_fitness.max())
        weights = weights / weights.sum()
        self.mean = (weights[:, None] * elite).sum(axis=0)

        # æ›´æ–°åæ–¹å·®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        diff = elite - self.mean
        self.cov = 0.8 * self.cov + 0.2 * (weights[:, None] * diff ** 2).sum(axis=0)

        # è‡ªé€‚åº”æ›´æ–° sigma
        self.sigma = self.sigma * 0.95 + 0.05 * np.std(elite_fitness)


# ========== Full World Model Agent ==========
class FullWorldModelAgent:
    """
    å®Œæ•´çš„ World Model Agent

    åŒ…å«ï¼š
    - V: Vision Model (çŠ¶æ€ç¼–ç å™¨)
    - M: Memory Model (MDN-LSTM)
    - C: Controller (çº¿æ€§ç­–ç•¥)
    """
    def __init__(self, config):
        self.config = config
        self.env = gym.make(config.env_name)

        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n

        # V: Vision Model
        self.encoder = VisionEncoder(
            self.state_dim,
            config.latent_size
        ).to(config.device)

        self.decoder = VisionDecoder(
            config.latent_size,
            self.state_dim
        ).to(config.device)

        # M: Memory Model
        self.memory = MemoryModel(
            config.latent_size,
            self.action_dim,
            config.hidden_size,
            config.n_gaussians
        ).to(config.device)

        # ä¼˜åŒ–å™¨
        self.vae_optimizer = optim.Adam(
            list(self.encoder.parameters()) + list(self.decoder.parameters()),
            lr=config.learning_rate
        )
        self.memory_optimizer = optim.Adam(
            self.memory.parameters(),
            lr=config.learning_rate
        )

        # æ•°æ®ç¼“å†²
        self.trajectories = []

        # è®°å½•
        self.training_history = {
            "data_collection_rewards": [],
            "vae_losses": [],
            "memory_losses": [],
            "policy_fitness": [],
            "evaluation_rewards": []
        }

    def collect_data(self):
        """é˜¶æ®µ 1: éšæœºç­–ç•¥æ”¶é›†æ•°æ®"""
        print("=" * 60)
        print("ğŸ“¦ é˜¶æ®µ 1: æ”¶é›†æ•°æ®")
        print("=" * 60)

        for episode in range(self.config.random_episodes):
            trajectory = {
                "states": [],
                "actions": [],
                "rewards": [],
                "dones": []
            }

            state, _ = self.env.reset()
            episode_reward = 0
            done = False

            while not done:
                action = self.env.action_space.sample()
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated

                # One-hot encode action
                action_onehot = np.zeros(self.action_dim)
                action_onehot[action] = 1

                trajectory["states"].append(state)
                trajectory["actions"].append(action_onehot)
                trajectory["rewards"].append(reward)
                trajectory["dones"].append(float(done))

                episode_reward += reward
                state = next_state

            self.trajectories.append(trajectory)
            self.training_history["data_collection_rewards"].append(episode_reward)

            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(self.training_history["data_collection_rewards"][-20:])
                print(f"  Episode {episode+1:3d}/{self.config.random_episodes} | "
                      f"Avg Reward: {avg_reward:.1f}")

        total_steps = sum(len(t["states"]) for t in self.trajectories)
        print(f"\nâœ… æ”¶é›†äº† {len(self.trajectories)} æ¡è½¨è¿¹, å…± {total_steps} æ­¥")

    def train_vae(self):
        """è®­ç»ƒ VAE (Vision Model)"""
        print("\n" + "=" * 60)
        print("ğŸ‘ï¸  è®­ç»ƒ VAE (Vision Model)")
        print("=" * 60)

        for epoch in range(self.config.world_model_epochs // 2):
            epoch_losses = []

            for _ in range(20):
                # éšæœºé‡‡æ ·ä¸€æ‰¹çŠ¶æ€
                traj = np.random.choice(self.trajectories)
                idx = np.random.randint(0, len(traj["states"]))
                state = torch.FloatTensor(traj["states"][idx]).to(self.config.device)

                # å‰å‘ä¼ æ’­
                z, mu, log_sigma = self.encoder.encode(state.unsqueeze(0))
                state_recon = self.decoder(z)

                # æŸå¤±ï¼šé‡å»º + KL
                recon_loss = F.mse_loss(state_recon, state.unsqueeze(0))
                kl_loss = -0.5 * torch.sum(1 + 2 * log_sigma - mu.pow(2) - torch.exp(2 * log_sigma))
                loss = recon_loss + 0.001 * kl_loss  # Î² = 0.001

                # ä¼˜åŒ–
                self.vae_optimizer.zero_grad()
                loss.backward()
                self.vae_optimizer.step()

                epoch_losses.append(loss.item())

            avg_loss = np.mean(epoch_losses)
            self.training_history["vae_losses"].append(avg_loss)

            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1:3d}/{self.config.world_model_epochs//2} | "
                      f"Loss: {avg_loss:.4f}")

        print("âœ… VAE è®­ç»ƒå®Œæˆ")

    def train_memory(self):
        """è®­ç»ƒ MDN-LSTM (Memory Model)"""
        print("\n" + "=" * 60)
        print("ğŸ§  è®­ç»ƒ MDN-LSTM (Memory Model)")
        print("=" * 60)

        for epoch in range(self.config.world_model_epochs):
            epoch_losses = []

            for _ in range(20):
                traj = np.random.choice(self.trajectories)

                # å‡†å¤‡åºåˆ—æ•°æ®
                seq_len = min(len(traj["states"]) - 1, self.config.sequence_length)
                if seq_len < 5:
                    continue

                start_idx = np.random.randint(0, len(traj["states"]) - seq_len)

                # è½¬æ¢ä¸ºå¼ é‡
                states = torch.FloatTensor(
                    traj["states"][start_idx:start_idx+seq_len]
                ).to(self.config.device)

                actions = torch.FloatTensor(
                    traj["actions"][start_idx:start_idx+seq_len]
                ).to(self.config.device)

                next_states = torch.FloatTensor(
                    traj["states"][start_idx+1:start_idx+seq_len+1]
                ).to(self.config.device)

                rewards = torch.FloatTensor(
                    traj["rewards"][start_idx:start_idx+seq_len]
                ).unsqueeze(-1).to(self.config.device)

                dones = torch.FloatTensor(
                    traj["dones"][start_idx:start_idx+seq_len]
                ).unsqueeze(-1).to(self.config.device)

                # ç¼–ç çŠ¶æ€
                with torch.no_grad():
                    z, _, _ = self.encoder.encode(states)
                    next_z, _, _ = self.encoder.encode(next_states)

                # æ·»åŠ  batch ç»´åº¦
                z = z.unsqueeze(0)
                actions = actions.unsqueeze(0)
                next_z = next_z.unsqueeze(0)
                rewards = rewards.unsqueeze(0)
                dones = dones.unsqueeze(0)

                # å‰å‘ä¼ æ’­
                pi, mu, sigma, pred_reward, pred_done, _ = self.memory(z, actions)

                # MDN æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
                # é‡å¡‘ç”¨äºè®¡ç®—
                batch_size, seq_len_actual, n_g, latent_s = mu.shape
                pi_flat = pi.view(batch_size * seq_len_actual, n_g)
                mu_flat = mu.view(batch_size * seq_len_actual, n_g, latent_s)
                sigma_flat = sigma.view(batch_size * seq_len_actual, n_g, latent_s)
                next_z_flat = next_z.view(batch_size * seq_len_actual, latent_s)

                log_prob = self.memory.mdn.log_prob(pi_flat, mu_flat, sigma_flat, next_z_flat)
                mdn_loss = -log_prob.mean()

                # å¥–åŠ±å’Œç»ˆæ­¢æŸå¤±
                reward_loss = F.mse_loss(pred_reward, rewards)
                done_loss = F.binary_cross_entropy(pred_done, dones)

                # æ€»æŸå¤±
                loss = mdn_loss + reward_loss + done_loss

                # ä¼˜åŒ–
                self.memory_optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.memory.parameters(), 1.0)
                self.memory_optimizer.step()

                epoch_losses.append(loss.item())

            if epoch_losses:
                avg_loss = np.mean(epoch_losses)
                self.training_history["memory_losses"].append(avg_loss)

                if (epoch + 1) % 10 == 0:
                    print(f"  Epoch {epoch+1:3d}/{self.config.world_model_epochs} | "
                          f"Loss: {avg_loss:.4f}")

        print("âœ… Memory Model è®­ç»ƒå®Œæˆ")

    def train_controller_in_dream(self):
        """é˜¶æ®µ 3: åœ¨æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥"""
        print("\n" + "=" * 60)
        print("ğŸ’­ é˜¶æ®µ 3: æ¢¦å¢ƒè®­ç»ƒç­–ç•¥ (CMA-ES)")
        print("=" * 60)

        # Controller è¾“å…¥: z + LSTM hidden state
        controller_input_dim = self.config.latent_size + self.config.hidden_size

        # åˆå§‹åŒ– CMA-ES
        controller = LinearController(controller_input_dim, self.action_dim)
        cmaes = CMAES(controller.num_params, self.config.population_size)

        best_controller = None
        best_fitness = -float('inf')

        for generation in range(self.config.generations):
            # ç”Ÿæˆç§ç¾¤
            population = cmaes.ask()
            fitness_scores = []

            # è¯„ä¼°æ¯ä¸ªä¸ªä½“
            for params in population:
                controller.set_params(params)

                # åœ¨æ¢¦å¢ƒä¸­è¯„ä¼°
                fitness = self.evaluate_in_dream(controller)
                fitness_scores.append(fitness)

            fitness_scores = np.array(fitness_scores)

            # æ›´æ–° CMA-ES
            cmaes.tell(population, fitness_scores)

            # è®°å½•æœ€ä½³
            gen_best = fitness_scores.max()
            gen_mean = fitness_scores.mean()

            if gen_best > best_fitness:
                best_fitness = gen_best
                best_controller = LinearController(controller_input_dim, self.action_dim)
                best_controller.set_params(population[fitness_scores.argmax()])

            self.training_history["policy_fitness"].append(gen_best)

            if (generation + 1) % 10 == 0:
                print(f"  Generation {generation+1:3d}/{self.config.generations} | "
                      f"Best: {gen_best:.1f} | Mean: {gen_mean:.1f}")

        print(f"\nâœ… ç­–ç•¥è®­ç»ƒå®Œæˆ | æœ€ä½³é€‚åº”åº¦: {best_fitness:.1f}")
        return best_controller

    def evaluate_in_dream(self, controller, num_rollouts=3):
        """åœ¨æ¢¦å¢ƒä¸­è¯„ä¼°ç­–ç•¥"""
        total_reward = 0

        for _ in range(num_rollouts):
            # ä»éšæœºè½¨è¿¹é‡‡æ ·èµ·å§‹çŠ¶æ€
            traj = np.random.choice(self.trajectories)
            state = torch.FloatTensor(traj["states"][0]).to(self.config.device)

            # ç¼–ç åˆå§‹çŠ¶æ€
            z, _, _ = self.encoder.encode(state.unsqueeze(0))
            z = z.squeeze(0)

            # åˆå§‹åŒ– LSTM éšè—çŠ¶æ€
            hidden = None

            episode_reward = 0

            for _ in range(self.config.dream_rollout_length):
                # è·å– LSTM éšè—çŠ¶æ€
                if hidden is None:
                    h = torch.zeros(self.config.hidden_size).to(self.config.device)
                else:
                    h = hidden[0].squeeze(0).squeeze(0)  # (hidden_size,)

                # æ„å»º controller è¾“å…¥
                controller_input = torch.cat([z, h]).detach().cpu().numpy()

                # æ§åˆ¶å™¨é€‰æ‹©åŠ¨ä½œ
                action_idx = controller.get_action(controller_input)
                action = torch.zeros(self.action_dim).to(self.config.device)
                action[action_idx] = 1

                # ä¸–ç•Œæ¨¡å‹é¢„æµ‹
                next_z, reward, done, hidden = self.memory.imagine_step(
                    z.unsqueeze(0),
                    action.unsqueeze(0),
                    hidden,
                    temperature=self.config.temperature
                )

                episode_reward += reward.item()

                # ç»ˆæ­¢æ£€æŸ¥
                if done.item() > 0.5:
                    break

                z = next_z.squeeze(0)

            total_reward += episode_reward

        return total_reward / num_rollouts

    def evaluate_in_real_env(self, controller, num_episodes=10):
        """åœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°"""
        rewards = []

        for _ in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False

            # ç¼–ç åˆå§‹çŠ¶æ€
            state_tensor = torch.FloatTensor(state).to(self.config.device)
            z, _, _ = self.encoder.encode(state_tensor.unsqueeze(0))
            z = z.squeeze(0)

            hidden = None

            while not done:
                # è·å– LSTM éšè—çŠ¶æ€
                if hidden is None:
                    h = torch.zeros(self.config.hidden_size).to(self.config.device)
                else:
                    h = hidden[0].squeeze(0).squeeze(0)

                # æ„å»º controller è¾“å…¥
                controller_input = torch.cat([z, h]).detach().cpu().numpy()

                # é€‰æ‹©åŠ¨ä½œ
                action = controller.get_action(controller_input)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward

                # ç¼–ç æ–°çŠ¶æ€å¹¶æ›´æ–° LSTM
                if not done:
                    state_tensor = torch.FloatTensor(next_state).to(self.config.device)
                    z, _, _ = self.encoder.encode(state_tensor.unsqueeze(0))
                    z = z.squeeze(0)

                    # æ›´æ–° LSTM éšè—çŠ¶æ€
                    action_onehot = torch.zeros(self.action_dim).to(self.config.device)
                    action_onehot[action] = 1
                    _, _, _, _, _, hidden = self.memory(
                        z.unsqueeze(0).unsqueeze(0),
                        action_onehot.unsqueeze(0).unsqueeze(0),
                        hidden
                    )

            rewards.append(episode_reward)

        return np.mean(rewards), np.std(rewards)

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 60)
        print("ğŸš€ Full World Model Training")
        print("=" * 60)
        print(f"  ç¯å¢ƒ: {self.config.env_name}")
        print(f"  è®¾å¤‡: {self.config.device}")
        print(f"  æ½œåœ¨ç©ºé—´ç»´åº¦: {self.config.latent_size}")
        print(f"  MDN é«˜æ–¯åˆ†é‡æ•°: {self.config.n_gaussians}")
        print(f"  LSTM éšè—å±‚: {self.config.hidden_size}")
        print("=" * 60)

        # é˜¶æ®µ 1: æ”¶é›†æ•°æ®
        self.collect_data()

        # é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹
        self.train_vae()
        self.train_memory()

        # é˜¶æ®µ 3: åœ¨æ¢¦å¢ƒä¸­è®­ç»ƒç­–ç•¥
        best_controller = self.train_controller_in_dream()

        # è¯„ä¼°
        print("\n" + "=" * 60)
        print("ğŸ“Š åœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°")
        print("=" * 60)

        mean_reward, std_reward = self.evaluate_in_real_env(best_controller, num_episodes=50)
        self.training_history["evaluation_rewards"].append(mean_reward)

        print(f"\nâœ… æœ€ç»ˆè¯„ä¼°ç»“æœ: {mean_reward:.1f} Â± {std_reward:.1f}")

        # ä¿å­˜ç»“æœ
        self.save_results(best_controller)

        return best_controller

    def save_results(self, controller):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(self.config.save_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        torch.save({
            'encoder': self.encoder.state_dict(),
            'decoder': self.decoder.state_dict(),
            'memory': self.memory.state_dict()
        }, f"{self.config.save_dir}/world_model.pt")

        # ä¿å­˜ controller
        np.savez(f"{self.config.save_dir}/controller.npz",
                 weights=controller.weights,
                 bias=controller.bias)

        # ä¿å­˜è®­ç»ƒå†å²
        with open(f"{self.config.save_dir}/training_history.json", "w") as f:
            json.dump(self.training_history, f, indent=2)

        # ç»˜å›¾
        self.plot_results()

        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {self.config.save_dir}")

    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # æ•°æ®æ”¶é›†é˜¶æ®µå¥–åŠ±
        axes[0, 0].plot(self.training_history["data_collection_rewards"], alpha=0.7)
        axes[0, 0].axhline(y=np.mean(self.training_history["data_collection_rewards"]),
                          color='r', linestyle='--', label='Mean')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].set_title('Stage 1: Data Collection (Random Policy)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # VAE æŸå¤±
        if self.training_history["vae_losses"]:
            axes[0, 1].plot(self.training_history["vae_losses"])
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].set_title('Stage 2a: VAE Training')
            axes[0, 1].set_yscale('log')
            axes[0, 1].grid(True, alpha=0.3)

        # Memory æŸå¤±
        if self.training_history["memory_losses"]:
            axes[1, 0].plot(self.training_history["memory_losses"])
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Loss (NLL)')
            axes[1, 0].set_title('Stage 2b: MDN-LSTM Training')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True, alpha=0.3)

        # ç­–ç•¥é€‚åº”åº¦
        if self.training_history["policy_fitness"]:
            axes[1, 1].plot(self.training_history["policy_fitness"])
            axes[1, 1].set_xlabel('Generation')
            axes[1, 1].set_ylabel('Fitness (Dream Reward)')
            axes[1, 1].set_title('Stage 3: Policy Evolution (CMA-ES)')
            axes[1, 1].grid(True, alpha=0.3)

            # æ·»åŠ æœ€ç»ˆè¯„ä¼°ç»“æœ
            if self.training_history["evaluation_rewards"]:
                final_reward = self.training_history["evaluation_rewards"][-1]
                axes[1, 1].axhline(y=final_reward, color='g', linestyle='--',
                                  label=f'Real Env: {final_reward:.1f}')
                axes[1, 1].legend()

        plt.tight_layout()
        plt.savefig(f"{self.config.save_dir}/training_curves.png", dpi=150)
        plt.close()

        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜")


# ========== ä¸»å‡½æ•° ==========
def main():
    config = Config()
    agent = FullWorldModelAgent(config)
    agent.train()


if __name__ == "__main__":
    main()
