"""
CartPole-v1: Mini Dreamer
==========================
ç®€åŒ–çš„ Dreamer (2020) å®ç°

æ¶æ„ï¼š
- RSSM: ç¡®å®šæ€§ RNN + éšæœºæ½œåœ¨å˜é‡
- Actor-Critic: åœ¨æƒ³è±¡è½¨è¿¹ä¸­å­¦ä¹ ç­–ç•¥
- åœ¨çº¿å­¦ä¹ : æŒç»­æ”¹è¿›ä¸–ç•Œæ¨¡å‹å’Œç­–ç•¥

å…³é”®å¯¹æ¯”ç‚¹ï¼š
- æ¯” Simple WM æ›´å¿«æ”¶æ•›ï¼ˆ~5Ã— æ ·æœ¬æ•ˆç‡ï¼‰
- ç­–ç•¥ç½‘ç»œæ›¿ä»£è¿›åŒ–ç®—æ³•
- æ”¯æŒåœ¨çº¿å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributions as distributions
import gymnasium as gym
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import json

# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"
    
    # è®­ç»ƒ
    num_episodes = 300
    seed_episodes = 5  # åˆå§‹éšæœºæ”¶é›†
    
    # RSSM
    state_dim = 4  # CartPole çŠ¶æ€ç»´åº¦
    action_dim = 2
    hidden_size = 128  # ç¡®å®šæ€§éšè—çŠ¶æ€
    stochastic_size = 32  # éšæœºæ½œåœ¨çŠ¶æ€
    
    # ç½‘ç»œ
    learning_rate = 3e-4
    batch_size = 16
    sequence_length = 50
    imagination_horizon = 15  # æƒ³è±¡è§†é‡
    
    # Actor-Critic
    gamma = 0.99
    lambda_gae = 0.95  # GAE å‚æ•°
    
    # ç¼“å†²
    buffer_size = 5000
    
    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ—¥å¿—
    log_interval = 10
    save_dir = "./results_mini_dreamer"


# ========== RSSM (Recurrent State Space Model) ==========
class RSSM(nn.Module):
    """
    ç®€åŒ–çš„ RSSM
    h_t: ç¡®å®šæ€§è·¯å¾„ (RNN)
    s_t: éšæœºè·¯å¾„ (Gaussian)
    """
    def __init__(self, state_dim, action_dim, hidden_size, stochastic_size):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        self.stochastic_size = stochastic_size
        
        # Encoder: observation â†’ embedding
        self.obs_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size)
        )
        
        # Deterministic path: RNN
        self.rnn = nn.GRUCell(hidden_size + action_dim, hidden_size)
        
        # Prior: p(s_t | h_t)
        self.prior_fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )
        self.prior_mean = nn.Linear(hidden_size, stochastic_size)
        self.prior_std = nn.Linear(hidden_size, stochastic_size)
        
        # Posterior: q(s_t | h_t, o_t)
        self.posterior_fc = nn.Sequential(
            nn.Linear(hidden_size + hidden_size, hidden_size),
            nn.ReLU()
        )
        self.posterior_mean = nn.Linear(hidden_size, stochastic_size)
        self.posterior_std = nn.Linear(hidden_size, stochastic_size)
        
        # Decoder: (h_t, s_t) â†’ o_t
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size + stochastic_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, state_dim)
        )
        
        # Reward predictor
        self.reward_predictor = nn.Sequential(
            nn.Linear(hidden_size + stochastic_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
    
    def get_prior(self, h):
        """Prior: p(s_t | h_t)"""
        x = self.prior_fc(h)
        mean = self.prior_mean(x)
        std = nn.functional.softplus(self.prior_std(x)) + 0.1
        return mean, std
    
    def get_posterior(self, h, obs_embed):
        """Posterior: q(s_t | h_t, o_t)"""
        x = self.posterior_fc(torch.cat([h, obs_embed], dim=-1))
        mean = self.posterior_mean(x)
        std = nn.functional.softplus(self.posterior_std(x)) + 0.1
        return mean, std
    
    def imagine_step(self, h, s, action):
        """æƒ³è±¡ä¸€æ­¥ï¼ˆç”¨äºç­–ç•¥å­¦ä¹ ï¼‰"""
        # æ›´æ–°ç¡®å®šæ€§çŠ¶æ€
        x = torch.cat([s, action], dim=-1)
        h_next = self.rnn(x, h)
        
        # Prior é‡‡æ ·
        mean, std = self.get_prior(h_next)
        s_next = mean + std * torch.randn_like(std)
        
        # é¢„æµ‹å¥–åŠ±
        reward = self.reward_predictor(torch.cat([h_next, s_next], dim=-1))
        
        return h_next, s_next, reward


# ========== Actor (ç­–ç•¥ç½‘ç»œ) ==========
class Actor(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼š(h, s) â†’ action"""
    def __init__(self, hidden_size, stochastic_size, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(hidden_size + stochastic_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, h, s):
        x = torch.cat([h, s], dim=-1)
        logits = self.network(x)
        return distributions.Categorical(logits=logits)


# ========== Critic (ä»·å€¼ç½‘ç»œ) ==========
class Critic(nn.Module):
    """ä»·å€¼ç½‘ç»œï¼š(h, s) â†’ V(s)"""
    def __init__(self, hidden_size, stochastic_size):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(hidden_size + stochastic_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, h, s):
        x = torch.cat([h, s], dim=-1)
        return self.network(x)


# ========== Mini Dreamer Agent ==========
class MiniDreamerAgent:
    def __init__(self, config):
        self.config = config
        self.env = gym.make(config.env_name)
        
        # ç½‘ç»œ
        self.rssm = RSSM(
            config.state_dim,
            config.action_dim,
            config.hidden_size,
            config.stochastic_size
        ).to(config.device)
        
        self.actor = Actor(
            config.hidden_size,
            config.stochastic_size,
            config.action_dim
        ).to(config.device)
        
        self.critic = Critic(
            config.hidden_size,
            config.stochastic_size
        ).to(config.device)
        
        # ä¼˜åŒ–å™¨
        self.world_model_optimizer = optim.Adam(
            self.rssm.parameters(),
            lr=config.learning_rate
        )
        
        self.actor_optimizer = optim.Adam(
            self.actor.parameters(),
            lr=config.learning_rate
        )
        
        self.critic_optimizer = optim.Adam(
            self.critic.parameters(),
            lr=config.learning_rate
        )
        
        # ç»éªŒç¼“å†²
        self.buffer = deque(maxlen=config.buffer_size)
        
        # è®°å½•
        self.episode_rewards = []
        self.world_model_losses = []
        self.actor_losses = []
        self.critic_losses = []
    
    def collect_episode(self, random=False):
        """æ”¶é›†ä¸€æ¡è½¨è¿¹"""
        trajectory = {
            "observations": [],
            "actions": [],
            "rewards": []
        }
        
        obs, _ = self.env.reset()
        episode_reward = 0
        done = False
        
        # åˆå§‹åŒ– RSSM çŠ¶æ€
        h = torch.zeros(1, self.config.hidden_size).to(self.config.device)
        s = torch.zeros(1, self.config.stochastic_size).to(self.config.device)
        
        while not done:
            if random:
                action = self.env.action_space.sample()
            else:
                # ä½¿ç”¨ Actor
                with torch.no_grad():
                    action_dist = self.actor(h, s)
                    action = action_dist.sample().item()
            
            next_obs, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated
            
            # è®°å½•
            trajectory["observations"].append(obs)
            
            # One-hot action
            action_onehot = np.zeros(self.config.action_dim)
            action_onehot[action] = 1
            trajectory["actions"].append(action_onehot)
            
            trajectory["rewards"].append(reward)
            
            # æ›´æ–° RSSM çŠ¶æ€ï¼ˆç”¨äºä¸‹ä¸€æ­¥å†³ç­–ï¼‰
            if not random:
                with torch.no_grad():
                    obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.config.device)
                    action_tensor = torch.FloatTensor(action_onehot).unsqueeze(0).to(self.config.device)
                    
                    obs_embed = self.rssm.obs_encoder(obs_tensor)
                    h = self.rssm.rnn(torch.cat([s, action_tensor], dim=-1), h)
                    mean, std = self.rssm.get_posterior(h, obs_embed)
                    s = mean  # ä½¿ç”¨å‡å€¼ï¼ˆæµ‹è¯•æ—¶ï¼‰
            
            episode_reward += reward
            obs = next_obs
        
        # æ·»åŠ æœ€åä¸€ä¸ªè§‚æµ‹
        trajectory["observations"].append(obs)
        
        self.buffer.append(trajectory)
        return episode_reward
    
    def train_world_model(self):
        """è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼ˆRSSMï¼‰"""
        if len(self.buffer) < self.config.batch_size:
            return
        
        # éšæœºé‡‡æ ·è½¨è¿¹
        batch = np.random.choice(list(self.buffer), self.config.batch_size, replace=False)
        
        total_loss = 0
        
        for traj in batch:
            seq_len = min(len(traj["observations"]) - 1, self.config.sequence_length)
            
            observations = torch.FloatTensor(
                traj["observations"][:seq_len]
            ).to(self.config.device)
            
            actions = torch.FloatTensor(
                traj["actions"][:seq_len]
            ).to(self.config.device)
            
            next_observations = torch.FloatTensor(
                traj["observations"][1:seq_len+1]
            ).to(self.config.device)
            
            rewards = torch.FloatTensor(
                traj["rewards"][:seq_len]
            ).unsqueeze(-1).to(self.config.device)
            
            # å‰å‘ä¼ æ’­
            h = torch.zeros(1, self.config.hidden_size).to(self.config.device)
            
            reconstruction_loss = 0
            kl_loss = 0
            reward_loss = 0
            
            for t in range(seq_len):
                obs = observations[t:t+1]
                action = actions[t:t+1]
                next_obs = next_observations[t:t+1]
                reward = rewards[t:t+1]
                
                # Encode
                obs_embed = self.rssm.obs_encoder(obs)
                
                # Prior
                prior_mean, prior_std = self.rssm.get_prior(h)
                
                # Posterior
                posterior_mean, posterior_std = self.rssm.get_posterior(h, obs_embed)
                
                # é‡‡æ ·
                s = posterior_mean + posterior_std * torch.randn_like(posterior_std)
                
                # Decode
                reconstructed_obs = self.rssm.decoder(torch.cat([h, s], dim=-1))
                
                # Predict reward
                predicted_reward = self.rssm.reward_predictor(torch.cat([h, s], dim=-1))
                
                # æŸå¤±
                reconstruction_loss += nn.MSELoss()(reconstructed_obs, next_obs)
                
                # KL divergence
                kl = torch.distributions.kl_divergence(
                    distributions.Normal(posterior_mean, posterior_std),
                    distributions.Normal(prior_mean, prior_std)
                ).sum(-1).mean()
                kl_loss += kl
                
                reward_loss += nn.MSELoss()(predicted_reward, reward)
                
                # æ›´æ–° h
                h = self.rssm.rnn(torch.cat([s, action], dim=-1), h)
            
            # æ€»æŸå¤±
            loss = reconstruction_loss + kl_loss + reward_loss
            total_loss += loss.item()
            
            self.world_model_optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.rssm.parameters(), 100)
            self.world_model_optimizer.step()
        
        self.world_model_losses.append(total_loss / self.config.batch_size)
    
    def train_actor_critic(self):
        """åœ¨æƒ³è±¡ä¸­è®­ç»ƒ Actor-Critic"""
        if len(self.buffer) < self.config.batch_size:
            return
        
        # ä»ç¼“å†²ä¸­é‡‡æ ·èµ·å§‹çŠ¶æ€
        batch = np.random.choice(list(self.buffer), self.config.batch_size, replace=False)
        
        actor_loss_total = 0
        critic_loss_total = 0
        
        for traj in batch:
            # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
            start_idx = np.random.randint(0, len(traj["observations"]) - 1)
            obs = torch.FloatTensor(traj["observations"][start_idx]).unsqueeze(0).to(self.config.device)
            
            # åˆå§‹åŒ– RSSM çŠ¶æ€
            obs_embed = self.rssm.obs_encoder(obs)
            h = torch.zeros(1, self.config.hidden_size).to(self.config.device)
            s, _ = self.rssm.get_prior(h)
            
            # æƒ³è±¡å±•å¼€
            imagined_trajectory = []
            
            for _ in range(self.config.imagination_horizon):
                # Actor é‡‡æ ·åŠ¨ä½œ
                action_dist = self.actor(h, s)
                action = action_dist.sample()
                
                # One-hot
                action_onehot = torch.zeros(self.config.action_dim).to(self.config.device)
                action_onehot[action] = 1
                
                # Critic ä¼°è®¡ä»·å€¼
                value = self.critic(h, s)
                
                # RSSM æƒ³è±¡ä¸‹ä¸€æ­¥
                h, s, reward = self.rssm.imagine_step(h, s, action_onehot.unsqueeze(0))
                h = h.squeeze(0).unsqueeze(0)
                s = s.squeeze(0).unsqueeze(0)
                
                imagined_trajectory.append({
                    "h": h.detach(),
                    "s": s.detach(),
                    "action": action,
                    "reward": reward.squeeze(),
                    "value": value.squeeze(),
                    "log_prob": action_dist.log_prob(action)
                })
            
            # è®¡ç®— GAE å’Œ returns
            returns = []
            advantages = []
            
            next_value = self.critic(h, s).squeeze().detach()
            
            for t in reversed(range(len(imagined_trajectory))):
                reward = imagined_trajectory[t]["reward"]
                value = imagined_trajectory[t]["value"]
                
                # TD error
                td_error = reward + self.config.gamma * next_value - value
                
                # GAE
                if t == len(imagined_trajectory) - 1:
                    advantage = td_error
                else:
                    advantage = td_error + self.config.gamma * self.config.lambda_gae * advantages[0]
                
                advantages.insert(0, advantage)
                returns.insert(0, advantage + value)
                
                next_value = value
            
            # Actor loss (Policy Gradient)
            actor_loss = 0
            for t, step in enumerate(imagined_trajectory):
                actor_loss -= step["log_prob"] * advantages[t].detach()
            
            # Critic loss
            critic_loss = 0
            for t, step in enumerate(imagined_trajectory):
                critic_loss += (step["value"] - returns[t].detach()) ** 2
            
            # ä¼˜åŒ–
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100)
            self.actor_optimizer.step()
            
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 100)
            self.critic_optimizer.step()
            
            actor_loss_total += actor_loss.item()
            critic_loss_total += critic_loss.item()
        
        self.actor_losses.append(actor_loss_total / self.config.batch_size)
        self.critic_losses.append(critic_loss_total / self.config.batch_size)
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ Mini Dreamer")
        print(f"è®¾å¤‡: {self.config.device}")
        print("-" * 50)
        
        # åˆå§‹éšæœºæ”¶é›†
        print("ğŸ“¦ åˆå§‹æ•°æ®æ”¶é›†...")
        for _ in range(self.config.seed_episodes):
            self.collect_episode(random=True)
        
        # ä¸»å¾ªç¯
        for episode in range(self.config.num_episodes):
            # æ”¶é›†æ•°æ®
            episode_reward = self.collect_episode(random=False)
            self.episode_rewards.append(episode_reward)
            
            # è®­ç»ƒä¸–ç•Œæ¨¡å‹
            for _ in range(5):  # å¤šæ¬¡æ›´æ–°
                self.train_world_model()
            
            # è®­ç»ƒ Actor-Critic
            for _ in range(5):
                self.train_actor_critic()
            
            # æ—¥å¿—
            if (episode + 1) % self.config.log_interval == 0:
                avg_reward = np.mean(self.episode_rewards[-self.config.log_interval:])
                print(f"Episode {episode+1}/{self.config.num_episodes} | "
                      f"Avg Reward: {avg_reward:.2f} | "
                      f"Buffer Size: {len(self.buffer)}")
        
        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            "rssm": self.rssm.state_dict(),
            "actor": self.actor.state_dict(),
            "critic": self.critic.state_dict()
        }, f"{self.config.save_dir}/models.pt")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        results = {
            "episode_rewards": self.episode_rewards,
            "world_model_losses": self.world_model_losses,
            "actor_losses": self.actor_losses,
            "critic_losses": self.critic_losses
        }
        
        with open(f"{self.config.save_dir}/training_data.json", "w") as f:
            json.dump(results, f, indent=2)
        
        # ç»˜å›¾
        self.plot_results()
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {self.config.save_dir}")
    
    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.episode_rewards, alpha=0.3, label='Raw')
        window = 30
        if len(self.episode_rewards) >= window:
            smoothed = np.convolve(self.episode_rewards, 
                                   np.ones(window)/window, mode='valid')
            axes[0, 0].plot(range(window-1, len(self.episode_rewards)), 
                           smoothed, label=f'Smoothed ({window})')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # World Model Loss
        if self.world_model_losses:
            axes[0, 1].plot(self.world_model_losses)
            axes[0, 1].set_xlabel('Update Step')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].set_title('World Model Loss')
            axes[0, 1].set_yscale('log')
            axes[0, 1].grid(True)
        
        # Actor Loss
        if self.actor_losses:
            axes[1, 0].plot(self.actor_losses)
            axes[1, 0].set_xlabel('Update Step')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].set_title('Actor Loss')
            axes[1, 0].grid(True)
        
        # Critic Loss
        if self.critic_losses:
            axes[1, 1].plot(self.critic_losses)
            axes[1, 1].set_xlabel('Update Step')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].set_title('Critic Loss')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{self.config.save_dir}/training_curves.png", dpi=150)
        plt.close()


# ========== ä¸»å‡½æ•° ==========
def main():
    config = Config()
    agent = MiniDreamerAgent(config)
    agent.train()


if __name__ == "__main__":
    main()
