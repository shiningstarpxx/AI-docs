"""
å¯è§†åŒ–è®­ç»ƒå‰åçš„æ•ˆæœå¯¹æ¯”
========================
å±•ç¤ºå››ä¸ªéƒ¨åˆ†ï¼š
1. è®­ç»ƒå‰çš„éšæœºç­–ç•¥ (Before Training)
2. DQN è®­ç»ƒå (After DQN)
3. Simple WM è®­ç»ƒå (After Simple WM)
4. Mini Dreamer è®­ç»ƒå (After Mini Dreamer)
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from pathlib import Path
import torch
import json


# ========== æ¸²æŸ“ Episode åˆ°å›¾åƒ ==========
def render_episode(env, policy=None, max_steps=500, title="Episode"):
    """
    è¿è¡Œä¸€ä¸ª episode å¹¶æ”¶é›†å¸§
    
    Args:
        env: Gym ç¯å¢ƒ
        policy: ç­–ç•¥å‡½æ•° (state -> action)ï¼ŒNone è¡¨ç¤ºéšæœºç­–ç•¥
        max_steps: æœ€å¤§æ­¥æ•°
        title: æ˜¾ç¤ºæ ‡é¢˜
    
    Returns:
        frames: å¸§åˆ—è¡¨
        total_reward: æ€»å¥–åŠ±
        steps: æ­¥æ•°
    """
    frames = []
    state, _ = env.reset()
    total_reward = 0
    
    for step in range(max_steps):
        # æ¸²æŸ“å½“å‰å¸§
        frame = env.render()
        frames.append(frame)
        
        # é€‰æ‹©åŠ¨ä½œ
        if policy is None:
            action = env.action_space.sample()  # éšæœºç­–ç•¥
        else:
            action = policy(state)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        total_reward += reward
        state = next_state
        
        if done:
            break
    
    return frames, total_reward, step + 1


# ========== åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ ==========
def load_dqn_policy(model_path):
    """åŠ è½½ DQN ç­–ç•¥"""
    if not Path(model_path).exists():
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")
        return None
    
    # ç®€åŒ–ç‰ˆ DQN ç½‘ç»œ
    class DQN(torch.nn.Module):
        def __init__(self, state_dim=4, action_dim=2, hidden_size=128):
            super().__init__()
            self.network = torch.nn.Sequential(
                torch.nn.Linear(state_dim, hidden_size),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_size, hidden_size),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_size, action_dim)
            )
        
        def forward(self, x):
            return self.network(x)
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model = DQN().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    def policy(state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            q_values = model(state_tensor)
            return q_values.argmax(1).item()
    
    return policy


def load_simple_wm_policy(model_path):
    """åŠ è½½ Simple WM ç­–ç•¥ï¼ˆçº¿æ€§ï¼‰"""
    if not Path(model_path).exists():
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")
        return None
    
    # çº¿æ€§ç­–ç•¥
    weights = np.load(model_path)
    
    def policy(state):
        # state: [4,] -> action: 0 or 1
        action_scores = state @ weights  # [2,]
        return int(np.argmax(action_scores))
    
    return policy


def load_mini_dreamer_policy(model_path):
    """åŠ è½½ Mini Dreamer Actor"""
    if not Path(model_path).exists():
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")
        return None
    
    # ç®€åŒ–ç‰ˆ Actor ç½‘ç»œ
    class Actor(torch.nn.Module):
        def __init__(self, state_dim=4, action_dim=2, hidden_size=64):
            super().__init__()
            self.network = torch.nn.Sequential(
                torch.nn.Linear(state_dim, hidden_size),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_size, action_dim)
            )
        
        def forward(self, x):
            return self.network(x)
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model = Actor().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    def policy(state):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            logits = model(state_tensor)
            action = torch.distributions.Categorical(logits=logits).sample()
            return action.item()
    
    return policy


# ========== å››é¢æ¿å¯è§†åŒ– ==========
def visualize_comparison(save_path="before_after_comparison.png"):
    """
    ç”Ÿæˆ 2Ã—2 ç½‘æ ¼å¯¹æ¯”å›¾
    """
    print("=" * 60)
    print("ğŸ¬ ç”Ÿæˆè®­ç»ƒå‰åå¯¹æ¯”å¯è§†åŒ–")
    print("=" * 60)
    print()
    
    # åˆ›å»ºç¯å¢ƒï¼ˆRGB æ¸²æŸ“æ¨¡å¼ï¼‰
    env = gym.make("CartPole-v1", render_mode="rgb_array")
    
    # ========== 1. æ”¶é›†å››ä¸ªåœºæ™¯çš„æ•°æ® ==========
    scenarios = []
    
    # (1) è®­ç»ƒå‰ï¼šéšæœºç­–ç•¥
    print("ğŸ“¹ åœºæ™¯ 1/4: è®­ç»ƒå‰ (éšæœºç­–ç•¥)...")
    frames_before, reward_before, steps_before = render_episode(
        env, policy=None, max_steps=500, title="Before Training"
    )
    scenarios.append({
        "title": "è®­ç»ƒå‰ (éšæœºç­–ç•¥)",
        "frames": frames_before,
        "reward": reward_before,
        "steps": steps_before,
        "color": "red"
    })
    print(f"  âœ“ å¥–åŠ±: {reward_before:.1f}, æ­¥æ•°: {steps_before}")
    print()
    
    # (2) DQN è®­ç»ƒå
    print("ğŸ“¹ åœºæ™¯ 2/4: DQN è®­ç»ƒå...")
    dqn_policy = load_dqn_policy("./results_dqn/model_final.pth")
    if dqn_policy:
        frames_dqn, reward_dqn, steps_dqn = render_episode(
            env, policy=dqn_policy, max_steps=500, title="After DQN"
        )
        scenarios.append({
            "title": "DQN è®­ç»ƒå",
            "frames": frames_dqn,
            "reward": reward_dqn,
            "steps": steps_dqn,
            "color": "blue"
        })
        print(f"  âœ“ å¥–åŠ±: {reward_dqn:.1f}, æ­¥æ•°: {steps_dqn}")
    else:
        print("  âš ï¸ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨éšæœºç­–ç•¥")
        scenarios.append(scenarios[0])  # å¤ç”¨éšæœºç­–ç•¥
    print()
    
    # (3) Simple WM è®­ç»ƒå
    print("ğŸ“¹ åœºæ™¯ 3/4: Simple WM è®­ç»ƒå...")
    swm_policy = load_simple_wm_policy("./results_simple_wm/controller_best.npy")
    if swm_policy:
        frames_swm, reward_swm, steps_swm = render_episode(
            env, policy=swm_policy, max_steps=500, title="After Simple WM"
        )
        scenarios.append({
            "title": "Simple WM è®­ç»ƒå",
            "frames": frames_swm,
            "reward": reward_swm,
            "steps": steps_swm,
            "color": "green"
        })
        print(f"  âœ“ å¥–åŠ±: {reward_swm:.1f}, æ­¥æ•°: {steps_swm}")
    else:
        print("  âš ï¸ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨éšæœºç­–ç•¥")
        scenarios.append(scenarios[0])
    print()
    
    # (4) Mini Dreamer è®­ç»ƒå
    print("ğŸ“¹ åœºæ™¯ 4/4: Mini Dreamer è®­ç»ƒå...")
    dreamer_policy = load_mini_dreamer_policy("./results_mini_dreamer/actor_final.pth")
    if dreamer_policy:
        frames_dreamer, reward_dreamer, steps_dreamer = render_episode(
            env, policy=dreamer_policy, max_steps=500, title="After Mini Dreamer"
        )
        scenarios.append({
            "title": "Mini Dreamer è®­ç»ƒå",
            "frames": frames_dreamer,
            "reward": reward_dreamer,
            "steps": steps_dreamer,
            "color": "purple"
        })
        print(f"  âœ“ å¥–åŠ±: {reward_dreamer:.1f}, æ­¥æ•°: {steps_dreamer}")
    else:
        print("  âš ï¸ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨éšæœºç­–ç•¥")
        scenarios.append(scenarios[0])
    print()
    
    env.close()
    
    # ========== 2. åˆ›å»º 2Ã—2 ç½‘æ ¼å›¾ ==========
    print("ğŸ¨ ç”Ÿæˆå¯¹æ¯”å›¾...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    # æ‰¾åˆ°æ‰€æœ‰åœºæ™¯ä¸­çš„æœ€å¤§å¸§æ•°ï¼ˆç”¨äºç»Ÿä¸€æ—¶é—´è½´ï¼‰
    max_frames = max(len(s["frames"]) for s in scenarios)
    
    for idx, (ax, scenario) in enumerate(zip(axes, scenarios)):
        # æ˜¾ç¤ºä¸­é—´å¸§ï¼ˆçº¦ä¸€åŠä½ç½®ï¼‰
        frame_idx = len(scenario["frames"]) // 2
        if frame_idx >= len(scenario["frames"]):
            frame_idx = len(scenario["frames"]) - 1
        
        frame = scenario["frames"][frame_idx]
        
        # æ˜¾ç¤ºå¸§
        ax.imshow(frame)
        ax.axis('off')
        
        # æ ‡é¢˜ï¼ˆåŒ…å«æ€§èƒ½æŒ‡æ ‡ï¼‰
        title_text = f"{scenario['title']}\n"
        title_text += f"æ€»å¥–åŠ±: {scenario['reward']:.1f} | æŒç»­æ­¥æ•°: {scenario['steps']}"
        
        # æ ¹æ®æ€§èƒ½ç€è‰²æ ‡é¢˜
        if scenario['reward'] >= 450:
            title_color = 'green'
            title_weight = 'bold'
        elif scenario['reward'] >= 200:
            title_color = 'orange'
            title_weight = 'normal'
        else:
            title_color = 'red'
            title_weight = 'normal'
        
        ax.set_title(title_text, fontsize=14, fontweight=title_weight, 
                     color=title_color, pad=10)
        
        # æ·»åŠ è¾¹æ¡†
        for spine in ax.spines.values():
            spine.set_edgecolor(scenario['color'])
            spine.set_linewidth(3)
    
    plt.suptitle('CartPole-v1: è®­ç»ƒå‰åæ•ˆæœå¯¹æ¯”', 
                 fontsize=18, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    print()
    
    # ========== 3. ç”Ÿæˆæ€§èƒ½æ€»ç»“ ==========
    print("=" * 60)
    print("ğŸ“Š æ€§èƒ½æ€»ç»“")
    print("=" * 60)
    print()
    
    summary = "| åœºæ™¯ | æ€»å¥–åŠ± | æŒç»­æ­¥æ•° | vs éšæœºç­–ç•¥ |\n"
    summary += "|:---|---:|---:|:---|\n"
    
    baseline_reward = scenarios[0]["reward"]
    
    for scenario in scenarios:
        reward = scenario["reward"]
        steps = scenario["steps"]
        improvement = ((reward - baseline_reward) / baseline_reward * 100) if baseline_reward > 0 else 0
        
        summary += f"| {scenario['title']} | {reward:.1f} | {steps} | "
        if improvement > 0:
            summary += f"+{improvement:.0f}% âœ… |\n"
        else:
            summary += "Baseline |\n"
    
    print(summary)
    
    # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶
    with open("performance_summary.md", "w") as f:
        f.write("# CartPole-v1 è®­ç»ƒå‰åæ€§èƒ½å¯¹æ¯”\n\n")
        f.write(summary)
        f.write("\n## å¯è§†åŒ–\n\n")
        f.write(f"![å¯¹æ¯”å›¾]({save_path})\n")
    
    print("âœ“ æ€»ç»“å·²ä¿å­˜: performance_summary.md")
    print()
    
    return scenarios


# ========== ç”ŸæˆåŠ¨ç”»ï¼ˆå¯é€‰ï¼‰==========
def create_animation(scenarios, save_path="comparison_animation.gif", fps=30):
    """
    ç”Ÿæˆå››é¢æ¿åŠ¨ç”» GIF
    """
    print("ğŸ¬ ç”ŸæˆåŠ¨ç”»ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    # åˆå§‹åŒ–å›¾åƒ
    images = []
    for ax, scenario in zip(axes, scenarios):
        img = ax.imshow(scenario["frames"][0])
        ax.axis('off')
        
        title_text = f"{scenario['title']}\n"
        title_text += f"æ€»å¥–åŠ±: {scenario['reward']:.1f} | æ­¥æ•°: {scenario['steps']}"
        ax.set_title(title_text, fontsize=14, pad=10)
        
        images.append(img)
    
    plt.suptitle('CartPole-v1: è®­ç»ƒå‰åæ•ˆæœå¯¹æ¯”', 
                 fontsize=18, fontweight='bold', y=0.98)
    
    # æ‰¾åˆ°æœ€å¤§å¸§æ•°
    max_frames = max(len(s["frames"]) for s in scenarios)
    
    def update(frame):
        for img, scenario in zip(images, scenarios):
            # å¦‚æœè¯¥åœºæ™¯å·²ç»“æŸï¼Œæ˜¾ç¤ºæœ€åä¸€å¸§
            idx = min(frame, len(scenario["frames"]) - 1)
            img.set_data(scenario["frames"][idx])
        return images
    
    anim = FuncAnimation(fig, update, frames=max_frames, 
                         interval=1000/fps, blit=True)
    
    anim.save(save_path, writer='pillow', fps=fps)
    plt.close()
    
    print(f"âœ“ åŠ¨ç”»å·²ä¿å­˜: {save_path}")


# ========== ä¸»å‡½æ•° ==========
def main():
    """ä¸»å‡½æ•°"""
    
    # ç”Ÿæˆé™æ€å¯¹æ¯”å›¾
    scenarios = visualize_comparison(save_path="before_after_comparison.png")
    
    # è¯¢é—®æ˜¯å¦ç”ŸæˆåŠ¨ç”»ï¼ˆå¯é€‰ï¼‰
    print()
    print("ğŸ’¡ æç¤º: å¯ä»¥ç”ŸæˆåŠ¨ç”» GIF æŸ¥çœ‹å®Œæ•´è¿‡ç¨‹")
    print("   ä½†è¿™éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆ~5-10åˆ†é’Ÿï¼‰")
    print()
    
    # å¦‚æœéœ€è¦åŠ¨ç”»ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # create_animation(scenarios, save_path="comparison_animation.gif", fps=30)
    
    print("=" * 60)
    print("âœ… å¯è§†åŒ–å®Œæˆï¼")
    print("=" * 60)
    print()
    print("è¾“å‡ºæ–‡ä»¶:")
    print("  - before_after_comparison.png (é™æ€å¯¹æ¯”å›¾)")
    print("  - performance_summary.md (æ€§èƒ½æ€»ç»“)")
    print()
    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹å¯¹æ¯”å›¾ï¼Œäº†è§£è®­ç»ƒæ•ˆæœ")
    print("  2. è¿è¡ŒçœŸå®è®­ç»ƒ: python 1_baseline_dqn.py")
    print("  3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬æŸ¥çœ‹çœŸå®æ•ˆæœ")


if __name__ == "__main__":
    main()
