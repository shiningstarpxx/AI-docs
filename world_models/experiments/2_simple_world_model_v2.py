"""
CartPole-v1: Simple World Model v2 (æ”¹è¿›ç‰ˆ)
================================
æ”¹è¿›ç‚¹ï¼š
1. æ•°æ®æ”¶é›†ï¼šç”¨ Îµ-greedy DQN ä»£æ›¿çº¯éšæœº
2. ä¸–ç•Œæ¨¡å‹ï¼šæ›´å¤§å®¹é‡ (256 hidden, 2å±‚LSTM)
3. æ§åˆ¶å™¨ï¼šç¥ç»ç½‘ç»œ + æ¢¯åº¦ä¼˜åŒ–ä»£æ›¿çº¿æ€§+CMA-ES
"""

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import json

# ========== é…ç½® ==========
class Config:
    # ç¯å¢ƒ
    env_name = "CartPole-v1"
    
    # é˜¶æ®µ 1: æ”¹è¿›çš„æ•°æ®æ”¶é›†
    pretrain_episodes = 100  # DQN é¢„è®­ç»ƒ
    data_collection_episodes = 200  # ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥æ”¶é›†
    epsilon_start = 0.5  # Îµ-greedy
    epsilon_end = 0.1
    
    # é˜¶æ®µ 2: æ›´å¼ºçš„ä¸–ç•Œæ¨¡å‹
    world_model_epochs = 200  # æ›´å¤šè®­ç»ƒ
    batch_size = 64
    sequence_length = 20
    hidden_size = 256  # 64 â†’ 256
    num_lstm_layers = 2  # 1 â†’ 2
    learning_rate = 3e-4
    
    # é˜¶æ®µ 3: ç¥ç»ç½‘ç»œæ§åˆ¶å™¨ + æ¢¯åº¦ä¼˜åŒ–
    dream_training_steps = 5000  # æ¢¯åº¦ä¼˜åŒ–æ­¥æ•°
    dream_batch_size = 32
    dream_horizon = 50  # æƒ³è±¡é•¿åº¦
    controller_lr = 1e-3
    
    # è®¾å¤‡
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ—¥å¿—
    save_dir = "./results_simple_wm_v2"


# ========== ç®€å• DQN (ç”¨äºæ•°æ®æ”¶é›†) ==========
class SimpleDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)
    
    def get_action(self, state, epsilon=0.0, device='cpu'):
        if np.random.random() < epsilon:
            return np.random.randint(0, 2)
        with torch.no_grad():
            q_values = self.forward(torch.FloatTensor(state).to(device))
            return q_values.argmax().item()


# ========== æ”¹è¿›çš„ä¸–ç•Œæ¨¡å‹ ==========
class ImprovedWorldModel(nn.Module):
    """
    æ›´å¼ºçš„ LSTM ä¸–ç•Œæ¨¡å‹
    - 2å±‚ LSTM
    - 256 hidden
    - Residual connections
    """
    def __init__(self, state_dim, action_dim, hidden_size=256, num_layers=2):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥ç¼–ç 
        self.input_encoder = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size)
        )
        
        # å¤šå±‚ LSTM
        self.lstm = nn.LSTM(
            hidden_size, 
            hidden_size, 
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1 if num_layers > 1 else 0
        )
        
        # è¾“å‡ºè§£ç 
        self.state_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, state_dim)
        )
        self.reward_predictor = nn.Linear(hidden_size, 1)
        self.done_predictor = nn.Linear(hidden_size, 1)
    
    def forward(self, state, action, hidden=None):
        # ç¼–ç 
        x = torch.cat([state, action], dim=-1)
        x = self.input_encoder(x)
        
        # LSTM
        x, hidden = self.lstm(x, hidden)
        
        # é¢„æµ‹
        next_state = self.state_predictor(x)
        reward = self.reward_predictor(x)
        done = torch.sigmoid(self.done_predictor(x))
        
        return next_state, reward, done, hidden
    
    def imagine_step(self, state, action, hidden=None):
        """å•æ­¥æƒ³è±¡"""
        with torch.no_grad():
            if state.dim() == 1:
                state = state.unsqueeze(0)
            if action.dim() == 1:
                action = action.unsqueeze(0)
            
            state = state.unsqueeze(1)
            action = action.unsqueeze(1)
            
            next_state, reward, done, hidden = self.forward(state, action, hidden)
            return next_state.squeeze(1), reward.squeeze(1), done.squeeze(1), hidden


# ========== ç¥ç»ç½‘ç»œæ§åˆ¶å™¨ ==========
class NeuralController(nn.Module):
    """
    ç¥ç»ç½‘ç»œç­–ç•¥ï¼ˆæ¯”çº¿æ€§å¼ºå¾—å¤šï¼‰
    """
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, state):
        return self.network(state)
    
    def get_action(self, state, device='cpu', deterministic=False):
        with torch.no_grad():
            logits = self.forward(torch.FloatTensor(state).to(device))
            if deterministic:
                return logits.argmax().item()
            probs = torch.softmax(logits, dim=-1)
            return torch.multinomial(probs, 1).item()


# ========== æ”¹è¿›çš„ Agent ==========
class ImprovedWorldModelAgent:
    def __init__(self, config):
        self.config = config
        self.env = gym.make(config.env_name)
        
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n
        
        # DQN (ç”¨äºæ•°æ®æ”¶é›†)
        self.dqn = SimpleDQN(self.state_dim, self.action_dim).to(config.device)
        self.dqn_optimizer = optim.Adam(self.dqn.parameters(), lr=1e-3)
        self.dqn_memory = deque(maxlen=10000)
        
        # ä¸–ç•Œæ¨¡å‹
        self.world_model = ImprovedWorldModel(
            self.state_dim,
            self.action_dim,
            config.hidden_size,
            config.num_lstm_layers
        ).to(config.device)
        self.wm_optimizer = optim.Adam(
            self.world_model.parameters(),
            lr=config.learning_rate
        )
        
        # æ§åˆ¶å™¨
        self.controller = NeuralController(
            self.state_dim,
            self.action_dim
        ).to(config.device)
        self.controller_optimizer = optim.Adam(
            self.controller.parameters(),
            lr=config.controller_lr
        )
        
        # æ•°æ®
        self.trajectories = []
        
        # è®°å½•
        self.training_history = {
            "dqn_pretrain_rewards": [],
            "data_collection_rewards": [],
            "world_model_losses": [],
            "controller_dream_rewards": [],
            "evaluation_rewards": []
        }
    
    def pretrain_dqn(self):
        """é¢„è®­ç»ƒ DQN ç”¨äºæ•°æ®æ”¶é›†"""
        print("ğŸ¯ é˜¶æ®µ 0: é¢„è®­ç»ƒ DQN (ç”¨äºæ•°æ®æ”¶é›†)")
        print("-" * 50)
        
        epsilon = 1.0
        epsilon_decay = 0.995
        
        for episode in range(self.config.pretrain_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                # Îµ-greedy
                action = self.dqn.get_action(state, epsilon, self.config.device)
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # å­˜å‚¨ç»éªŒ
                self.dqn_memory.append((state, action, reward, next_state, done))
                episode_reward += reward
                state = next_state
                
                # è®­ç»ƒ DQN
                if len(self.dqn_memory) > 64:
                    batch = np.random.choice(len(self.dqn_memory), 64, replace=False)
                    states = torch.FloatTensor([self.dqn_memory[i][0] for i in batch]).to(self.config.device)
                    actions = torch.LongTensor([self.dqn_memory[i][1] for i in batch]).to(self.config.device)
                    rewards = torch.FloatTensor([self.dqn_memory[i][2] for i in batch]).to(self.config.device)
                    next_states = torch.FloatTensor([self.dqn_memory[i][3] for i in batch]).to(self.config.device)
                    dones = torch.FloatTensor([self.dqn_memory[i][4] for i in batch]).to(self.config.device)
                    
                    current_q = self.dqn(states).gather(1, actions.unsqueeze(1))
                    next_q = self.dqn(next_states).max(1)[0].detach()
                    target_q = rewards + 0.99 * next_q * (1 - dones)
                    
                    loss = nn.MSELoss()(current_q.squeeze(), target_q)
                    self.dqn_optimizer.zero_grad()
                    loss.backward()
                    self.dqn_optimizer.step()
            
            epsilon = max(0.1, epsilon * epsilon_decay)
            self.training_history["dqn_pretrain_rewards"].append(episode_reward)
            
            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(self.training_history["dqn_pretrain_rewards"][-20:])
                print(f"Episode {episode+1}/{self.config.pretrain_episodes} | "
                      f"Avg Reward: {avg_reward:.2f} | Îµ: {epsilon:.3f}")
        
        final_avg = np.mean(self.training_history["dqn_pretrain_rewards"][-20:])
        print(f"âœ… DQN é¢„è®­ç»ƒå®Œæˆ | æœ€ç»ˆå¹³å‡: {final_avg:.2f}")
    
    def collect_data_with_policy(self):
        """ç”¨è®­ç»ƒå¥½çš„ DQN æ”¶é›†é«˜è´¨é‡æ•°æ®"""
        print("\nğŸ“¦ é˜¶æ®µ 1: ç”¨ç­–ç•¥æ”¶é›†æ•°æ®")
        print("-" * 50)
        
        epsilon_schedule = np.linspace(
            self.config.epsilon_start,
            self.config.epsilon_end,
            self.config.data_collection_episodes
        )
        
        for episode in range(self.config.data_collection_episodes):
            trajectory = {
                "states": [],
                "actions": [],
                "rewards": [],
                "dones": []
            }
            
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            epsilon = epsilon_schedule[episode]
            
            while not done:
                action = self.dqn.get_action(state, epsilon, self.config.device)
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # One-hot encode
                action_onehot = np.zeros(self.action_dim)
                action_onehot[action] = 1
                
                trajectory["states"].append(state)
                trajectory["actions"].append(action_onehot)
                trajectory["rewards"].append(reward)
                trajectory["dones"].append(float(done))
                
                episode_reward += reward
                state = next_state
            
            self.trajectories.append(trajectory)
            self.training_history["data_collection_rewards"].append(episode_reward)
            
            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(self.training_history["data_collection_rewards"][-20:])
                print(f"Episode {episode+1}/{self.config.data_collection_episodes} | "
                      f"Avg Reward: {avg_reward:.2f}")
        
        avg_reward = np.mean(self.training_history["data_collection_rewards"])
        print(f"âœ… æ”¶é›†äº† {len(self.trajectories)} æ¡è½¨è¿¹ | å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    
    def train_world_model(self):
        """è®­ç»ƒä¸–ç•Œæ¨¡å‹"""
        print("\nğŸŒ é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹")
        print("-" * 50)
        
        for epoch in range(self.config.world_model_epochs):
            epoch_losses = []
            
            for _ in range(100):  # æ¯è½®æ›´å¤šæ›´æ–°
                traj = np.random.choice(self.trajectories)
                
                max_seq_len = min(len(traj["states"]) - 1, self.config.sequence_length)
                if max_seq_len < 2:
                    continue
                
                start_idx = np.random.randint(0, len(traj["states"]) - max_seq_len)
                
                states = torch.FloatTensor(
                    np.array(traj["states"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).to(self.config.device)
                
                actions = torch.FloatTensor(
                    np.array(traj["actions"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).to(self.config.device)
                
                next_states = torch.FloatTensor(
                    np.array(traj["states"][start_idx+1:start_idx+max_seq_len+1])
                ).unsqueeze(0).to(self.config.device)
                
                rewards = torch.FloatTensor(
                    np.array(traj["rewards"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).unsqueeze(-1).to(self.config.device)
                
                dones = torch.FloatTensor(
                    np.array(traj["dones"][start_idx:start_idx+max_seq_len])
                ).unsqueeze(0).unsqueeze(-1).to(self.config.device)
                
                # å‰å‘
                pred_states, pred_rewards, pred_dones, _ = self.world_model(states, actions)
                
                # æŸå¤±
                state_loss = nn.MSELoss()(pred_states, next_states)
                reward_loss = nn.MSELoss()(pred_rewards, rewards)
                done_loss = nn.BCELoss()(pred_dones, dones)
                
                loss = state_loss + reward_loss * 10.0 + done_loss * 5.0
                
                # ä¼˜åŒ–
                self.wm_optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 1.0)
                self.wm_optimizer.step()
                
                epoch_losses.append(loss.item())
            
            avg_loss = np.mean(epoch_losses)
            self.training_history["world_model_losses"].append(avg_loss)
            
            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{self.config.world_model_epochs} | Loss: {avg_loss:.4f}")
        
        print("âœ… ä¸–ç•Œæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def train_controller_in_dream(self):
        """åœ¨æ¢¦å¢ƒä¸­ç”¨æ¢¯åº¦è®­ç»ƒæ§åˆ¶å™¨"""
        print("\nğŸ’­ é˜¶æ®µ 3: æ¢¦å¢ƒè®­ç»ƒæ§åˆ¶å™¨")
        print("-" * 50)
        
        for step in range(self.config.dream_training_steps):
            # é‡‡æ ·èµ·å§‹çŠ¶æ€
            batch_trajs = np.random.choice(self.trajectories, self.config.dream_batch_size)
            start_states = []
            for traj in batch_trajs:
                idx = np.random.randint(0, len(traj["states"]))
                start_states.append(traj["states"][idx])
            
            start_states = torch.FloatTensor(np.array(start_states)).to(self.config.device)
            
            # æƒ³è±¡ rollout
            total_reward = 0
            states = start_states
            hidden = None
            
            for t in range(self.config.dream_horizon):
                # ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                logits = self.controller(states)
                action_probs = torch.softmax(logits, dim=-1)
                actions = torch.multinomial(action_probs, 1).squeeze(-1)
                
                # One-hot
                action_onehot = torch.zeros(self.config.dream_batch_size, self.action_dim).to(self.config.device)
                action_onehot.scatter_(1, actions.unsqueeze(1), 1)
                
                # ä¸–ç•Œæ¨¡å‹é¢„æµ‹ï¼ˆéœ€è¦ä¿ç•™æ¢¯åº¦ï¼‰
                states_input = states.unsqueeze(1)
                actions_input = action_onehot.unsqueeze(1)
                
                # ä¸´æ—¶å¯ç”¨æ¢¯åº¦
                self.world_model.eval()
                with torch.enable_grad():
                    next_states, rewards, dones, hidden = self.world_model(
                        states_input, 
                        actions_input, 
                        hidden
                    )
                self.world_model.train()
                
                next_states = next_states.squeeze(1)
                rewards = rewards.squeeze(1)
                
                total_reward = total_reward + rewards
                states = next_states.detach()  # æ–­å¼€æ¢¯åº¦ï¼ˆåªä¼˜åŒ–ç­–ç•¥ï¼‰
                
                # æå‰ç»ˆæ­¢
                if dones.mean() > 0.5:
                    break
            
            # ä¼˜åŒ–ç­–ç•¥ï¼ˆæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼‰
            loss = -total_reward.mean()
            
            self.controller_optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.controller.parameters(), 1.0)
            self.controller_optimizer.step()
            
            if (step + 1) % 500 == 0:
                avg_dream_reward = -loss.item()
                self.training_history["controller_dream_rewards"].append(avg_dream_reward)
                print(f"Step {step+1}/{self.config.dream_training_steps} | "
                      f"Dream Reward: {avg_dream_reward:.2f}")
        
        print("âœ… æ§åˆ¶å™¨è®­ç»ƒå®Œæˆ")
    
    def evaluate(self, num_episodes=50):
        """åœ¨çœŸå®ç¯å¢ƒè¯„ä¼°"""
        print("\nğŸ“Š çœŸå®ç¯å¢ƒè¯„ä¼°...")
        rewards = []
        
        for _ in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                action = self.controller.get_action(state, self.config.device, deterministic=True)
                state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
            
            rewards.append(episode_reward)
        
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        self.training_history["evaluation_rewards"].append(mean_reward)
        
        print(f"âœ… è¯„ä¼°ç»“æœ: {mean_reward:.2f} Â± {std_reward:.2f}")
        return mean_reward, std_reward
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ Improved Simple World Model")
        print(f"è®¾å¤‡: {self.config.device}")
        print("=" * 50)
        
        # é˜¶æ®µ 0: é¢„è®­ç»ƒ DQN
        self.pretrain_dqn()
        
        # é˜¶æ®µ 1: ç”¨ç­–ç•¥æ”¶é›†æ•°æ®
        self.collect_data_with_policy()
        
        # é˜¶æ®µ 2: è®­ç»ƒä¸–ç•Œæ¨¡å‹
        self.train_world_model()
        
        # é˜¶æ®µ 3: åœ¨æ¢¦å¢ƒä¸­è®­ç»ƒæ§åˆ¶å™¨
        self.train_controller_in_dream()
        
        # è¯„ä¼°
        self.evaluate()
        
        # ä¿å­˜
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(self.world_model.state_dict(),
                   f"{self.config.save_dir}/world_model.pt")
        torch.save(self.controller.state_dict(),
                   f"{self.config.save_dir}/controller.pt")
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(f"{self.config.save_dir}/training_history.json", "w") as f:
            json.dump(self.training_history, f, indent=2)
        
        # ç»˜å›¾
        self.plot_results()
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {self.config.save_dir}")
    
    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        
        # DQN é¢„è®­ç»ƒ
        axes[0, 0].plot(self.training_history["dqn_pretrain_rewards"], alpha=0.5)
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].set_title('DQN Pretraining')
        axes[0, 0].grid(True)
        
        # æ•°æ®æ”¶é›†
        axes[0, 1].plot(self.training_history["data_collection_rewards"], alpha=0.5)
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Reward')
        axes[0, 1].set_title('Data Collection')
        axes[0, 1].grid(True)
        
        # ä¸–ç•Œæ¨¡å‹æŸå¤±
        axes[0, 2].plot(self.training_history["world_model_losses"])
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('Loss')
        axes[0, 2].set_title('World Model Training')
        axes[0, 2].set_yscale('log')
        axes[0, 2].grid(True)
        
        # æ¢¦å¢ƒå¥–åŠ±
        if self.training_history["controller_dream_rewards"]:
            axes[1, 0].plot(self.training_history["controller_dream_rewards"])
            axes[1, 0].set_xlabel('Step (x500)')
            axes[1, 0].set_ylabel('Dream Reward')
            axes[1, 0].set_title('Dream Training')
            axes[1, 0].grid(True)
        
        # æœ€ç»ˆè¯„ä¼°
        if self.training_history["evaluation_rewards"]:
            axes[1, 1].bar(['Evaluation'], self.training_history["evaluation_rewards"])
            axes[1, 1].set_ylabel('Mean Reward')
            axes[1, 1].set_title('Real Environment')
            axes[1, 1].grid(True)
        
        # å¯¹æ¯”
        axes[1, 2].text(0.5, 0.5, 
                       f"Data Collection Avg:\n{np.mean(self.training_history['data_collection_rewards']):.2f}\n\n"
                       f"Final Evaluation:\n{self.training_history['evaluation_rewards'][0]:.2f}",
                       ha='center', va='center', fontsize=12)
        axes[1, 2].set_title('Summary')
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(f"{self.config.save_dir}/training_curves.png", dpi=150)
        plt.close()


# ========== ä¸»å‡½æ•° ==========
def main():
    config = Config()
    agent = ImprovedWorldModelAgent(config)
    agent.train()


if __name__ == "__main__":
    main()
