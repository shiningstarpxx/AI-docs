# World Models 实验进度报告

📅 **更新时间**: 2025-12-09 14:50

---

## 🎯 任务目标

对比三种强化学习方法在 CartPole-v1 上的表现：
1. **DQN** (Baseline, Model-Free)
2. **Simple World Model** (LSTM + CMA-ES)
3. **Mini Dreamer** (RSSM + Actor-Critic)

**关键指标**：样本效率、最终性能、训练稳定性

---

## ✅ 已完成

### 1. DQN (Baseline) ✅

**训练状态**: 已完成 (800 episodes)

**性能结果**:
- 平均奖励: **193.3 ± 128.9**
- 最佳性能: **500** (满分)
- 达到满分概率: ~15%

**文件位置**:
```
results_dqn/
├── dqn_model.pt              # 模型权重
├── model_final.pth           # 最终模型
├── training_curves.png       # 训练曲线
└── training_data.json        # 训练历史 (23KB)
```

**问题分析**:
- ⚠️ 性能不稳定，未能持续达到 475+ 收敛标准
- 原因：epsilon_decay 初始设置过快 (0.995 → 0.998 后改善)
- CartPole 需要 1000-2000 episodes 才能真正稳定

**结论**: 可作为 baseline，展示 Model-Free 方法的样本效率

---

### 2. Simple World Model ✅

**训练状态**: 已完成 (200 episodes 数据收集 + 100 epochs 模型训练 + 150 代进化)

**性能结果**:
- **梦境适应度**: 103.16 (rollout 长度 100)
- **真实环境评估**: **17.06 ± ?** ⚠️⚠️⚠️
- **严重的模型偏差问题**

**文件位置**:
```
results_simple_wm/
├── world_model.pt            # LSTM 世界模型 (540KB)
├── training_curves.png       # 训练曲线
└── training_history.json     # 训练历史 (8KB)
```

**问题分析**:
- 🔴 **致命问题**: 梦境性能 (103) vs 真实性能 (17)，差距 6 倍！
- 根本原因：世界模型学习失败
  - 数据收集阶段：随机策略平均仅 20 分
  - 世界模型损失：收敛到 0.005，但预测质量差
  - 策略在错误的世界模型中优化 → 负迁移

**需要解决**:
1. 提高数据收集质量（使用稍好的策略，不是纯随机）
2. 增加世界模型容量和训练时间
3. 调整损失权重（状态预测 vs 奖励预测）
4. 考虑简化：直接在真实环境中用进化策略？

---

### 3. Mini Dreamer ❌

**训练状态**: 未开始

**原因**: 在 Simple World Model 调试成功前，先不进行更复杂的实验

**预计时间**: 实现 + 调试需要 4-6 小时

---

## 📊 当前对比

| 方法 | 数据收集 | 最终性能 | 状态 |
|:---|:---|:---|:---|
| **DQN** | 800 episodes | 193.3 ± 128.9 | ✅ 完成，性能一般 |
| **Simple WM** | 200 episodes | 17.06 ± ? | ⚠️ 完成，性能极差 |
| **Mini Dreamer** | - | - | ❌ 未开始 |

---

## 🔍 核心洞察

### 问题 1: DQN 性能为何不稳定？

**回答**: CartPole 的特性
- 需要精确的平衡控制
- 小的策略偏差导致大的性能差异
- 通常需要 1000-2000 episodes 才能稳定收敛
- **我们的 800 episodes 是不够的**

**解决方案**:
- 继续训练到 1500-2000 episodes
- 或接受当前性能作为 baseline（因为对比重点是样本效率，不是绝对性能）

### 问题 2: Simple World Model 为何失败？

**回答**: 低质量数据 + 模型容量不足
- 随机策略平均 20 分 → 数据分布很差
- LSTM(64) 太小，无法准确建模动态
- 线性控制器太简单，表达能力不足

**World Models 论文的成功案例**:
- Car Racing: 状态空间平滑，易建模
- VizDoom: 视觉输入，VAE 压缩有效
- **CartPole: 状态空间小但动态敏感，反而难建模**

**教训**: World Model 不是银弹，对环境有要求

---

## 🎯 下一步计划

### 方案 A: 修复 Simple World Model (推荐)

**目标**: 让 Simple WM 至少达到 100+ 性能

**改进策略**:
1. **改进数据收集**:
   ```python
   # 不用纯随机，用 ε-greedy 策略
   # 先训练一个简单的策略网络收集数据
   ```

2. **增强世界模型**:
   ```python
   hidden_size = 256  # 64 → 256
   num_layers = 2     # 1 → 2 层 LSTM
   ```

3. **改进控制器**:
   ```python
   # 线性 → 小型神经网络
   # 梯度优化 → CMA-ES
   ```

**预计时间**: 2-3 小时调试

---

### 方案 B: 简化实验，聚焦对比 (备选)

**理由**:
- CartPole 可能不适合 World Model 演示
- 样本效率对比才是核心

**新策略**:
1. 接受 DQN 的 193.3 作为 baseline
2. 创建一个"理想化" Simple WM：
   - 直接用 DQN 收集高质量数据
   - 训练准确的世界模型
   - 展示"如果模型准确"的样本效率提升
3. 跳过 Mini Dreamer，专注讲清楚核心思想

**预计时间**: 1-2 小时

---

### 方案 C: 换任务到 MountainCar (激进)

**理由**:
- MountainCar 更适合 World Model（物理直觉强）
- 稀疏奖励问题更能体现 World Model 的价值

**缺点**:
- 需要重新实现三种方法
- 时间成本 +1 天

---

## 💡 建议

**推荐执行顺序**:

1. **先快速尝试方案 A** (2 小时)
   - 如果 Simple WM 能达到 100+，说明思路对了
   - 继续完整对比实验

2. **如果方案 A 失败，转方案 B** (1 小时)
   - 承认 CartPole 不是最佳演示任务
   - 但完成教学目标（理解 World Model 原理）

3. **不推荐方案 C**
   - 时间成本太高
   - 当前已有的工作可以讲清楚核心思想

---

## 📈 实验价值

尽管性能不理想，这个实验过程本身就很有价值：

### 学到了什么？

1. **World Model 的挑战**:
   - 模型准确性是瓶颈
   - 数据质量比数量更重要
   - 不是所有任务都适合 World Model

2. **工程实践**:
   - 超参数调优的重要性 (epsilon_decay)
   - 训练曲线分析技巧
   - 模型偏差诊断

3. **对比分析**:
   - Model-Free (DQN) 的稳定性问题
   - Model-Based 的模型误差累积问题
   - 样本效率 vs 渐近性能 tradeoff

### 可以输出的内容

1. **技术文档**: 详细的实验设计和问题分析
2. **代码实现**: 三种方法的清晰实现（即使性能不完美）
3. **经验总结**: World Model 的适用场景和局限性

---

## 🚦 下一步行动

**请选择**:
- [ ] 方案 A: 修复 Simple World Model
- [ ] 方案 B: 简化实验，聚焦教学
- [ ] 方案 C: 换任务到 MountainCar
- [ ] 其他建议？

---

**更新记录**:
- 2025-12-08 17:35: DQN 训练完成
- 2025-12-08 20:28: DQN 优化版完成
- 2025-12-09 11:49: Simple WM 训练完成（性能差）
- 2025-12-09 14:50: 创建进度报告
