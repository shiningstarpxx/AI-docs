# Genie 与 JEPA：新一代世界模型路线深度解析

> 从 Dreamer 到 Genie 到 JEPA，三条世界模型路线的哲学分歧

## 目录

1. [三条路线概览](#三条路线概览)
2. [Genie：从无标签视频学习可控世界](#genie从无标签视频学习可控世界)
3. [JEPA：不预测像素，预测表示](#jepa不预测像素预测表示)
4. [三方对比：核心分歧](#三方对比核心分歧)
5. [总结与展望](#总结与展望)

---

## 三条路线概览

### 核心问题

三种方法都在回答同一个问题：**如何让 AI 理解世界的运行规律？**

但它们的回答截然不同：

```
Dreamer: "给我标注好的交互数据，我学完整的世界动态"
Genie:   "只给视频就行，但我只能学有限种动作"
JEPA:    "我不关心动作，只学习状态的抽象表示"
```

### 预测目标对比

```
┌─────────────────────────────────────────────────────────────┐
│                    预测目标对比                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Dreamer:                                                   │
│    输入: 潜在状态 s_t + 动作 a_t                            │
│    预测: 潜在状态 s_{t+1} + 重建像素 x_{t+1}               │
│    需要解码器重建像素                                        │
│                                                             │
│  Genie:                                                     │
│    输入: 像素 token x_t + 潜在动作 a_t                      │
│    预测: 像素 token x_{t+1}                                │
│    直接在像素空间预测                                        │
│                                                             │
│  JEPA:                                                      │
│    输入: 表示 z_t                                          │
│    预测: 表示 z_{t+1}                                      │
│    永远不碰像素！                                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Genie：从无标签视频学习可控世界

> Google DeepMind, 2024
> "Genie: Generative Interactive Environments"

### 核心问题

传统世界模型的数据困境：

```
Dreamer/World Models 需要:
  (观测 o_t, 动作 a_t, 奖励 r_t, 观测 o_{t+1})

  必须有明确的动作标签！
  数据来源：自己在环境中交互收集

互联网上的视频:
  海量游戏视频、教学视频、生活视频
  只有画面，没有动作标签
  玩家按了什么键？不知道！
```

**Genie 的核心问题：能否从纯视频中，自动发现"动作"是什么？**

### 架构：三大组件

```
┌─────────────────────────────────────────────────────────────┐
│                         Genie                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. Video Tokenizer (ST-ViViT)                             │
│     视频帧 (256×256) → 离散 token (16×16 = 256 个)         │
│     时空 Vision Transformer + 向量量化                      │
│                                                             │
│  2. Latent Action Model (LAM) — 核心创新！                  │
│     从连续帧推断潜在动作                                     │
│     a_t = LAM(x_t, x_{t+1})                                │
│     输出：8 个离散选项之一                                   │
│                                                             │
│  3. Dynamics Model (Transformer)                           │
│     给定当前帧 + 动作，预测下一帧                            │
│     x_{t+1} = DM(x_t, a_t)                                 │
│     MaskGIT 风格并行解码                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 核心创新：Latent Action Model

**"鸡和蛋"问题：**

```
看起来像循环依赖：
- LAM 需要 DM 的反馈才能知道什么是"好的"动作
- DM 需要 LAM 提供动作才能学习预测

谁先训练？
```

**答案：联合训练 + 信息瓶颈**

```
┌─────────────────────────────────────────────────────────────┐
│                      训练过程                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   x_t ─────────────────────────────┐                       │
│    │                               │                       │
│    │         x_{t+1} ──────┐       │                       │
│    │              │        │       │                       │
│    └──────┬───────┘        │       │                       │
│           │                │       │                       │
│           ▼                │       │                       │
│    ┌─────────────┐         │       │                       │
│    │     LAM     │         │       │                       │
│    │  (看到两帧) │         │       │                       │
│    └──────┬──────┘         │       │                       │
│           │                │       │                       │
│           ▼                │       │                       │
│     a_t ∈ {1..8}          │       │                       │
│      (离散！)              │       │                       │
│           │                │       │                       │
│           └────────────────┼───────┘                       │
│                            │                               │
│                            ▼                               │
│                     ┌─────────────┐                        │
│                     │     DM      │                        │
│                     │ (只看x_t和a)│  ← 看不到 x_{t+1}！     │
│                     └──────┬──────┘                        │
│                            │                               │
│                            ▼                               │
│                       x̂_{t+1}                              │
│                            │                               │
│                            ▼                               │
│               Loss = CE(x̂_{t+1}, x_{t+1})                 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**关键洞察：信息不对称**

```
LAM 看到: x_t 和 x_{t+1} （知道发生了什么）
DM  看到: x_t 和 a_t    （不知道 x_{t+1}！）

LAM 必须通过 a_t 这个"瓶颈"告诉 DM 关键信息
```

### 为什么必须是离散瓶颈？

```
如果 a_t 是连续向量（如 256 维）：

问题：信息泄露
  LAM 可以直接把 x_{t+1} 的信息编码进 a_t
  DM 只需要解码，不需要真正理解动态

  极端情况：
    a_t = Encoder(x_{t+1})
    x̂_{t+1} = Decoder(a_t)
    Loss = 0，完美！但 a_t 毫无语义

离散瓶颈（8 选 1）：

  a_t 只有 3 bit 信息
  LAM 无法把像素细节塞进去
  只能传递"最本质的变化是什么"

  结果：
    a_t = 1 → "角色向左"
    a_t = 2 → "角色向右"
    a_t = 3 → "角色跳跃"
    ...
```

### 训练与推理的对齐

```
训练时：
  LAM(x_t, x_{t+1}) → a_t    # 看两帧，推断动作（后验）

推理时：
  用户给 a_t                  # 用户选择动作
  DM(x_t, a_t) → x_{t+1}     # 模型生成下一帧

对齐是"隐式"发生的：
  如果 LAM 学到 "向右移动" → a_t = 2
  那么 DM 必须学到 x_t + a_t=2 → 生成"向右移动"的画面
  因为这是唯一能最小化重建误差的方式
```

### Genie 的 Trade-off

```
获得：
  + 可以用海量无标签视频（YouTube、网络视频）
  + 自动发现"有意义"的动作
  + 从草图生成可玩游戏

付出：
  - 动作空间被限制为 K 个离散选项
  - K 太小 → 表达力不足（无法表达复杂动作）
  - K 太大 → 回到信息泄露问题

本质：用"动作空间的先验假设"换"不需要标签"
```

### 模型规模与结果

```
规模：
  参数量: 11B (110 亿)
  训练数据: 200,000+ 小时视频

结果：
  - 学到了有意义的动作空间（左、右、跳等）
  - 可交互控制生成的环境
  - 输入草图 → 生成可玩游戏
```

---

## JEPA：不预测像素，预测表示

> Yann LeCun, 2022-2024
> "A Path Towards Autonomous Machine Intelligence"
> I-JEPA (2023), V-JEPA (2024)

### LeCun 的批评

```
对 Dreamer 和 Genie 的批评：

"你们都在预测像素（或像素的压缩表示）"
"为什么要预测草地上每片叶子的位置？"
"这些细节对决策有用吗？"

论点：
1. 像素包含大量无关信息
2. 预测像素浪费计算资源
3. 模型被迫学习"纹理生成"而不是"因果理解"
```

### 核心思想：预测表示，不预测像素

```
方案 A（生成式，Dreamer/Genie）：
  预测下一帧的每个像素
  - 球的位置、颜色
  - 桌子的纹理
  - 背景、光影...

方案 B（判别式，JEPA）：
  预测下一帧的"抽象表示"
  - 不生成具体像素
  - 只在表示空间预测"状态会怎么变"
  - 忽略纹理、色彩等冗余信息
```

### 架构

```
┌─────────────────────────────────────────────────────────────┐
│                        JEPA                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   x_t (当前帧)              x_{t+1} (下一帧)                │
│       │                         │                          │
│       ▼                         ▼                          │
│   ┌────────┐               ┌────────┐                      │
│   │Encoder │               │Encoder │  ← EMA 更新          │
│   │(在线)   │               │ (目标) │  （慢慢跟随）         │
│   └───┬────┘               └───┬────┘                      │
│       │                        │                           │
│       ▼                        ▼                           │
│      z_t                     z_{t+1}  ← 目标（stop grad）  │
│       │                        ↑                           │
│       ▼                        │                           │
│   ┌──────────┐                 │                           │
│   │Predictor │─────────────────┘                           │
│   └──────────┘                                             │
│       ↓                                                    │
│     ẑ_{t+1}  ← 预测                                        │
│                                                             │
│   Loss = ||z_{t+1} - ẑ_{t+1}||²                           │
│                                                             │
│   关键：没有 Decoder！永远不重建像素                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 避免表示坍缩

**问题：如果 Encoder 学到常数输出？**

```
z_t = [0, 0, 0, ...]
z_{t+1} = [0, 0, 0, ...]
ẑ_{t+1} = [0, 0, 0, ...]

Loss = 0，完美！
但表示毫无信息 — 表示坍缩 (Representation Collapse)
```

**解决方案：不对称设计 (EMA)**

```
在线 Encoder: 正在被训练，可以快速变化
目标 Encoder: EMA 更新，慢慢跟随在线 Encoder

θ_target = β × θ_target + (1-β) × θ_online
           (如 β = 0.99)

为什么能防止坍缩？
  如果在线 Encoder 想输出常数
  但目标 Encoder 是旧版本的移动平均
  它的输出不会立刻变成常数
  Loss ≠ 0，在线 Encoder 被迫继续学习

类比：
  在线 Encoder = 领跑者
  EMA Encoder = 跟随者（慢慢追）
  领跑者停下来 → 跟随者还在追 → 领跑者必须继续跑
```

### JEPA vs Dreamer：分布假设

```
Dreamer:
  先验 p(z|h) = Categorical(32×32)  ← 必须指定分布形式
  后验 q(z|h,o) = Categorical(32×32)
  用 KL 散度约束

  需要：
  - 假设分布形式（高斯/离散）
  - 闭式 KL 或采样估计
  - 调节 KL 权重、free bits 等

JEPA:
  z_t = Encoder(x_t)  ← 就是一个向量，不是分布
  用 MSE 直接比较

  简洁：
  - 不假设任何分布
  - MSE 简单直接
  - 没有 KL balancing
```

### JEPA 不建模不确定性

```
Dreamer:
  "下一个状态可能是 A，也可能是 B"
  p(z_{t+1}) 是一个分布，可以采样多种可能

JEPA:
  "下一个状态的表示是 ẑ_{t+1}"
  一个确定性的预测

LeCun 的观点：

  两种不确定性：

  1. 低层不确定性（像素级）
     "树叶往左还是往右飘？"
     → 对决策无关，不需要建模

  2. 高层不确定性（语义级）
     "对手会攻击还是防守？"
     → 可以在更高层处理

  JEPA 的表示空间会"抹掉"低层不确定性
  只保留语义信息
```

### JEPA 用于 RL 的挑战

```
JEPA 当前主要用途：
  - 自监督预训练（I-JEPA 图像，V-JEPA 视频）
  - 学习好的表示，下游任务 fine-tune

JEPA + RL 的开放问题：
  - 如何加入动作条件？ → 不像 Dreamer 有现成方案
  - 如何做规划？ → 需要展开多步，但没有生成能力
  - 如何处理奖励？ → 不建模不确定性，如何评估期望回报？

  这些还在探索中！
```

---

## 三方对比：核心分歧

### 功能对比表

| 维度 | Dreamer | Genie | JEPA |
|:---|:---|:---|:---|
| **预测目标** | 潜在状态分布 + 重建像素 | 像素 token | 抽象表示 |
| **需要解码器** | 是 | 是 | **否** |
| **建模不确定性** | 显式分布 (KL) | 隐式（生成多样性） | **不建模** |
| **需要动作标签** | 是 | **否**（自动发现） | N/A（不涉及动作） |
| **动作空间** | 环境定义（灵活） | **预设离散（如 8 类）** | N/A |
| **分布假设** | 需要（高斯/离散） | 不需要 | **不需要** |
| **RL 应用成熟度** | 高 | 中 | 低（探索中） |
| **主要应用** | 游戏、机器人控制 | 视频生成、游戏 | 自监督预训练 |

### 数据需求对比

```
Dreamer:
  需要: (观测, 动作, 奖励) 完整交互数据
  来源: 自己收集或模拟器
  规模: 通常 < 1B 帧

Genie:
  需要: 纯视频（无标签）
  来源: YouTube、网络视频
  规模: 200k+ 小时
  限制: 动作空间被预设

JEPA:
  需要: 图像/视频（无标签）
  来源: 任意图像/视频数据
  规模: 可以很大
  限制: 不涉及动作和交互
```

### 核心 Trade-off

```
┌─────────────────────────────────────────────────────────────┐
│                      没有免费的午餐                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Dreamer:                                                   │
│    获得: 灵活的动作空间，完整的世界动态                       │
│    付出: 需要动作标签，需要假设分布                          │
│                                                             │
│  Genie:                                                     │
│    获得: 不需要动作标签，可用海量视频                        │
│    付出: 动作空间受限（预设离散选项）                        │
│                                                             │
│  JEPA:                                                      │
│    获得: 不假设分布，计算高效，表示抽象                      │
│    付出: 不建模不确定性，不涉及动作，RL 应用不成熟           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 哲学分歧

```
关于"预测什么"：

  Dreamer: 预测完整的潜在状态分布
           "世界是随机的，我要建模所有可能性"

  Genie:   预测像素级的下一帧
           "我要生成逼真的未来画面"

  JEPA:    只预测抽象表示
           "像素细节不重要，理解语义规律才重要"


关于"不确定性"：

  Dreamer: 显式建模（分布 + KL 散度）
  Genie:   隐式包含（生成多样性）
  JEPA:    不建模（认为低层不确定性不重要）


关于"动作"：

  Dreamer: 需要标签，但空间灵活
  Genie:   不需要标签，但空间受限
  JEPA:    不涉及动作（目前）
```

### 适用场景

```
选择 Dreamer 当：
  - 有交互环境（模拟器、游戏）
  - 需要精确控制
  - 追求 RL 性能
  - 例：Atari、机器人控制、DMControl

选择 Genie 当：
  - 有大量无标签视频
  - 动作空间相对简单（2D 游戏）
  - 想从视频创建可交互环境
  - 例：从 YouTube 学习、草图生成游戏

选择 JEPA 当：
  - 主要目标是学习好的表示
  - 下游任务是分类、检测等
  - 追求计算效率
  - 例：视觉预训练、视频理解
```

---

## 总结与展望

### 演进脉络

```
2018: World Models
      证明"在梦中学习"可行

2020-2023: Dreamer 系列
      端到端训练，RSSM，离散潜在空间
      RL 世界模型的标杆

2022: LeCun 发布 JEPA 路线图
      批评生成式方法
      提出表示预测替代像素预测

2024: Genie
      从无标签视频学习
      自动发现潜在动作
      世界模型 + 视频生成的交汇

2024+: 融合？
      三种方法能否结合？
      - Dreamer 的 RL 能力
      - Genie 的无监督动作发现
      - JEPA 的抽象表示
```

### 开放问题

1. **JEPA 如何用于 RL？**
   - 加入动作条件的最佳方式？
   - 不建模不确定性如何做规划？

2. **Genie 如何扩展动作空间？**
   - 从 8 个离散到连续动作？
   - 如何处理更复杂的交互？

3. **能否融合三种方法？**
   - JEPA 的表示 + Dreamer 的 RL？
   - Genie 的无监督 + Dreamer 的灵活动作空间？

4. **与大语言模型的关系？**
   - LLM 是否已经是某种世界模型？
   - 视觉世界模型 + 语言模型如何结合？

### 核心洞察

```
1. 预测目标的选择决定了方法的特性
   像素 → 计算昂贵，细节丰富
   分布 → 建模不确定性，需要假设
   表示 → 高效抽象，但丢失细节

2. 监督信号与约束的 trade-off
   标签多 → 约束少，灵活
   标签少 → 约束多（如离散动作空间）

3. 没有银弹
   每种方法都有其适用场景
   未来可能是融合而非替代
```

---

## 参考文献

1. Hafner et al. "DreamerV3: Mastering Diverse Domains through World Models" (2023)
2. Bruce et al. "Genie: Generative Interactive Environments" (2024)
3. LeCun. "A Path Towards Autonomous Machine Intelligence" (2022)
4. Assran et al. "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture" (I-JEPA, 2023)
5. Bardes et al. "Revisiting Feature Prediction for Learning Visual Representations from Video" (V-JEPA, 2024)

---

> 世界模型的终极目标：让机器像人一样理解世界、想象未来、做出决策。
> 三条路线各有所长，未来的突破可能来自它们的融合。
