# 因果世界模型 (Causal World Models) 深度解析

## 1. 引言：相关性 vs 因果性

### 1.1 一个经典例子

```
观察数据：
  - 雨伞销量增加 → 冰淇淋销量增加
  - 相关系数很高！

相关性模型会学到：
  "多卖雨伞 → 多卖冰淇淋"

但真正的因果关系是：
  天气炎热 → {雨伞销量↑ (防晒), 冰淇淋销量↑}
  天气炎热是共同原因 (Confounder)

如果你根据相关性模型决策：
  "让我们多进雨伞来带动冰淇淋销量" → 失败！
```

### 1.2 为什么世界模型需要因果性？

```
当前的世界模型 (Dreamer, Genie 等):
  学习 P(s_{t+1} | s_t, a_t)  ← 条件概率（相关性）

问题：
  - 不能区分 "观察到 A 后 B 发生" vs "A 导致 B"
  - 无法回答反事实问题："如果当时选 B 会怎样？"
  - 分布变化时泛化能力差

因果世界模型:
  学习 P(s_{t+1} | do(a_t), s_t)  ← 干预分布（因果性）

好处：
  - 支持反事实推理
  - 更好的分布外泛化
  - 更高效的规划
```

---

## 2. 因果推断基础

### 2.1 结构因果模型 (SCM)

#### 定义

结构因果模型 $\mathcal{M} = (U, V, F)$：
- $U$: 外生变量（背景噪声）
- $V$: 内生变量（系统状态）
- $F$: 结构方程 $V_i = f_i(pa_i, U_i)$

#### 示例：简单机器人

```
┌──────────────────────────────────────────┐
│  结构因果模型                             │
├──────────────────────────────────────────┤
│  外生变量:                                │
│    U_motor: 电机噪声                      │
│    U_sensor: 传感器噪声                   │
│                                          │
│  结构方程:                                │
│    position = f(action, U_motor)         │
│    observation = g(position, U_sensor)   │
│                                          │
│  因果图:                                  │
│    action → position → observation       │
│              ↑            ↑              │
│           U_motor     U_sensor           │
└──────────────────────────────────────────┘
```

### 2.2 Pearl 的因果层次

```
┌─────────────────────────────────────────────────────────────┐
│                    因果之梯 (Ladder of Causation)            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Level 3: 反事实 (Counterfactual)                          │
│  ─────────────────────────────────────────                  │
│  问题: "如果当时 X=x'，Y 会是多少？"                         │
│  形式: P(Y_x' | X=x, Y=y)                                  │
│  例子: "如果我当时没刹车，会不会撞上？"                      │
│  能力: 需要完整 SCM                                         │
│        ↑                                                    │
│        │                                                    │
│  Level 2: 干预 (Intervention)                              │
│  ─────────────────────────────────────────                  │
│  问题: "如果我强制 X=x，Y 会是多少？"                        │
│  形式: P(Y | do(X=x))                                      │
│  例子: "如果我踩刹车，车会减速多少？"                        │
│  能力: 需要因果图 + 观察数据                                │
│        ↑                                                    │
│        │                                                    │
│  Level 1: 观察/关联 (Association)                          │
│  ─────────────────────────────────────────                  │
│  问题: "看到 X=x 时，Y 通常是多少？"                         │
│  形式: P(Y | X=x)                                          │
│  例子: "踩刹车的人通常速度如何？"                           │
│  能力: 只需要观察数据                                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘

当前的深度学习模型基本都在 Level 1！
```

### 2.3 do-Calculus: 从观察到干预

#### 观察 vs 干预

```
P(Y | X=x)     观察条件概率
              "在 X=x 的人群中，Y 的分布"
              混淆因素的影响仍然存在

P(Y | do(X=x)) 干预分布
              "强制设定 X=x 后，Y 的分布"
              切断了所有指向 X 的边
```

#### 图形操作

```
原始因果图:           do(X=x) 后:
    Z                     Z
   ╱ ╲                   ╱ ╲
  ↓   ↓                 ↓   ↓
  X → Y                 X → Y
                        ↑
                      固定为 x
                      (切断指向 X 的边)
```

#### 后门准则 (Back-door Criterion)

如果变量集 $Z$ 满足：
1. $Z$ 阻断所有从 $X$ 到 $Y$ 的后门路径
2. $Z$ 中没有 $X$ 的后代

则：
$$P(Y | do(X=x)) = \sum_z P(Y | X=x, Z=z) P(Z=z)$$

**这允许我们从纯观察数据估计干预效果！**

### 2.4 反事实推理

#### 三步过程

```
反事实问题: "如果当时选了 a' 而不是 a，结果会怎样？"

Step 1: Abduction（溯因）
  给定观察 (a, s, s')，推断外生变量 U
  U* = argmax P(U | a, s, s')

Step 2: Action（行动）
  修改 SCM，将 a 替换为 a'

Step 3: Prediction（预测）
  用修改后的 SCM 和 U* 预测 s'_{a'}
```

#### 示例

```
实际发生: 我踩了刹车 (a=brake)，车停了 (s'=stopped)

反事实问题: 如果我没踩刹车，会怎样？

Step 1: 推断 U
  路面摩擦系数 U_friction = 0.8 (从停车距离推断)
  初始速度 U_v0 = 30 km/h

Step 2: 修改动作
  a' = no_brake

Step 3: 预测
  使用相同的 U_friction, U_v0
  s'_{a'} = 继续前进，可能撞上前车
```

---

## 3. 当前世界模型的因果局限

### 3.1 Causal Confusion: 更多信息反而更差 (de Haan et al., 2019)

**核心发现**：在模仿学习/世界模型中，获取更多信息可能导致更差的性能！

```
实验设计 (Causal Confusion in Imitation Learning):

场景 A: 完整信息 (CONFOUNDED)
  - 输入包含仪表盘（刹车灯）+ 挡风玻璃
  - 模型可以看到刹车指示灯

场景 B: 不完整信息 (ORIGINAL)
  - 输入只有挡风玻璃，仪表盘被遮挡
  - 模型看不到刹车灯

结果令人惊讶：
  场景 A (更多信息): 训练误差低，但实际驾驶性能差
  场景 B (更少信息): 训练误差略高，但实际驾驶性能好！

原因：
  模型 A 学到了 "刹车灯亮 → 踩刹车" 的相关性
  但刹车灯是踩刹车的结果，不是原因！
  这就是 "因果混淆" (Causal Confusion)
```

#### 实验结果

| 任务 | ORIGINAL (少信息) | CONFOUNDED (多信息) |
|:---|:---|:---|
| MountainCar | 收敛到专家水平 | 需要更多数据/失败 |
| Hopper | 良好 | 性能显著下降 |
| Pong | 良好 | 严重下降 |

**关键洞察**：分布偏移 (distributional shift) 会暴露因果混淆问题
- 训练时：数据来自专家分布
- 测试时：数据来自学习策略分布
- 相关性在训练分布成立，但在测试分布可能不成立

### 3.2 相关性建模的其他问题

```
问题 1: 混淆变量

  真实因果:     学到的"因果":
    U
   ╱ ╲
  ↓   ↓
  A   B            A ──?──→ B

  模型无法区分 A→B 还是 A←U→B


问题 2: 虚假相关

  训练分布:
    白天训练 → 场景明亮 → 易识别障碍物 → 高性能

  测试分布:
    夜晚测试 → 场景昏暗 → 模型依赖亮度特征 → 失败

  模型学到了 "亮度" 和 "性能" 的相关性，而非因果关系


问题 3: 无法泛化

  Dreamer 在 Atari 上训练:
    学到: 特定颜色/形状 → 特定奖励

  换一个主题/皮肤:
    颜色/形状变了 → 预测失败

  因为模型学的是表面特征的相关性，不是游戏机制的因果结构
```

### 3.2 案例：自动驾驶中的混淆

```
训练数据观察:
  - 前方有行人 AND 刹车 → 没撞
  - 前方有行人 AND 没刹车 → 撞了

相关性模型学到:
  P(没撞 | 前方有行人) ≈ P(刹车 | 前方有行人) × P(没撞 | 刹车)

问题：模型没有学到 "刹车导致不撞"
而是学到 "看到行人的场景，通常不撞"

部署时:
  如果遇到一个 "之前没见过的行人出现方式"
  模型可能不刹车（因为不匹配训练分布）
```

---

## 4. 因果世界模型方法

### 4.1 因果结构发现

#### 从数据学习因果图

```
方法 1: 基于约束 (Constraint-based)
  - PC 算法、FCI 算法
  - 通过条件独立性测试推断因果结构
  - 例：如果 X ⊥ Y | Z，则 X 和 Y 之间没有直接因果边

方法 2: 基于分数 (Score-based)
  - GES、NOTEARS
  - 搜索最优的 DAG 结构
  - 优化某种评分函数（如 BIC）

方法 3: 函数因果模型 (FCM)
  - 假设特定的函数形式
  - 利用非高斯性或非线性识别因果方向
```

#### 在世界模型中的应用

```
┌─────────────────────────────────────────────────────────┐
│  Causal World Model Training                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. 收集交互数据                                        │
│     {(s_t, a_t, s_{t+1})} from environment             │
│                                                         │
│  2. 因果发现                                            │
│     从数据学习状态变量间的因果图 G                      │
│     s_i → s_j 表示 s_i 因果影响 s_j                    │
│                                                         │
│  3. 因果动态模型                                        │
│     对每个变量 s_i: s_i^{t+1} = f_i(pa_i^t, a_t)       │
│     pa_i 是 s_i 在因果图中的父节点                     │
│                                                         │
│  4. 干预预测                                            │
│     P(s' | do(a)) 而非 P(s' | a)                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 4.2 因果表示学习

#### 问题：潜在空间的因果解纠缠

```
VAE 学到的 z:
  z_1 可能混合了 "位置" 和 "颜色"
  z_2 可能混合了 "形状" 和 "大小"

  这种纠缠的表示难以进行因果推理

因果解纠缠的 z:
  z_1 = 位置
  z_2 = 颜色
  z_3 = 形状
  ...

  每个 z_i 对应一个独立的因果机制
```

#### 方法

```
1. 弱监督因果解纠缠
   - 利用时间结构：相邻帧之间的变化暗示因果关系
   - 利用干预信号：动作作为已知的干预

2. 对比学习 + 因果正则化
   - 鼓励表示对非因果变化不变
   - 例：背景变化不影响物体表示

3. 因果注意力
   - 注意力机制学习因果相关的特征
   - 忽略虚假相关的特征
```

### 4.3 反事实世界模型

#### 架构

```
┌─────────────────────────────────────────────────────────┐
│  Counterfactual World Model                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  输入: 历史轨迹 τ = (s_0, a_0, ..., s_T)               │
│        反事实动作 a'_k (在时刻 k 选择不同动作)         │
│                                                         │
│  Step 1: 编码 + 外生变量推断                           │
│          τ → Encoder → z, U_inferred                   │
│                                                         │
│  Step 2: 反事实 rollout                                │
│          s_0, ..., s_{k-1} (保持不变)                  │
│          s'_k = f(s_{k-1}, a'_k, U_inferred)           │
│          s'_{k+1} = f(s'_k, a_{k+1}, U_inferred)       │
│          ...                                            │
│                                                         │
│  输出: 反事实轨迹 τ' = (s_0, ..., s_{k-1}, s'_k, ...)  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### 关键挑战

```
挑战 1: 外生变量的推断
  - U 是隐变量，难以从数据中恢复
  - 需要强假设或额外信息

挑战 2: 可识别性
  - 多个 SCM 可能产生相同的观察分布
  - 需要额外约束保证唯一性

挑战 3: 高维状态
  - 图像/视频状态下，因果结构更复杂
  - 需要层次化的因果建模
```

---

## 5. 代表工作

### 5.1 Causal Dynamics Learning

**核心思想**：学习状态变量间的因果结构

```
方法:
  1. 用 VAE 学习潜在表示 z
  2. 在 z 空间发现因果图
  3. 用因果图约束的动态模型预测

优势:
  - 更好的分布外泛化
  - 支持干预预测
  - 可解释的状态表示
```

### 5.2 CausalCity

**应用**：自动驾驶场景的因果分析

```
目标:
  分析自动驾驶决策的因果后果
  "如果 ego 车没让行，会发生什么？"

方法:
  1. 场景因果图建模
     ego_action → ego_position → collision
           ↘           ↗
         other_car_position

  2. 反事实场景生成
     保持其他车辆轨迹，改变 ego 决策
     预测反事实结果

  3. 责任归因
     通过反事实分析确定事故责任
```

### 5.3 Causal Reinforcement Learning

```
因果 RL 的几个方向:

1. 因果模型辅助的 RL
   - 利用因果结构加速学习
   - 识别关键因果变量

2. 反事实策略评估
   - Off-policy 评估的因果视角
   - "如果用策略 π' 会怎样？"

3. 因果探索
   - 设计干预来发现因果结构
   - 比随机探索更高效
```

---

## 6. 因果视角对世界模型设计的启示

### 6.1 架构设计

```
启示 1: 模块化设计
  每个模块对应一个独立的因果机制
  模块间的连接对应因果关系

  传统: 一个大网络学习所有
  因果: 多个小网络，每个学习一个机制

启示 2: 不变性正则化
  鼓励模型学习因果不变的特征
  惩罚对虚假相关特征的依赖

启示 3: 干预数据
  除了观察数据，收集干预数据
  例：随机动作探索提供干预信号
```

### 6.2 训练目标

```
传统目标:
  min E[(s' - f(s, a))^2]  预测误差

因果增强目标:
  min E[(s' - f(s, a))^2]  预测误差
    + λ₁ · 因果结构正则化  (鼓励稀疏因果图)
    + λ₂ · 不变性损失      (跨环境一致性)
    + λ₃ · 干预一致性      (预测应满足 do-calculus)
```

### 6.3 评估指标

```
传统指标:
  - 预测误差 (MSE, likelihood)
  - RL 性能 (episode return)

因果增强指标:
  - 分布外泛化性能
  - 反事实预测准确率
  - 干预效果估计误差
  - 因果结构恢复准确率
```

---

## 7. 开放问题与未来方向

### 7.1 开放问题

```
1. 可扩展性
   因果发现算法在高维状态下计算复杂度高
   如何在大规模视觉世界模型中应用？

2. 可识别性
   什么条件下可以从观察数据恢复因果结构？
   深度学习模型如何保证可识别性？

3. 评估难题
   反事实的 ground truth 难以获取
   如何评估反事实预测的准确性？

4. 与现有方法的整合
   如何将因果模块整合到 Dreamer 等成熟框架？
```

### 7.2 未来方向

```
1. 因果表示学习 + 世界模型
   - 学习因果解纠缠的潜在表示
   - 在因果表示上建立动态模型

2. 大规模因果预训练
   - 类似 GPT 的因果世界模型预训练
   - 从大规模视频学习因果结构

3. 人机协作因果发现
   - 人类提供先验因果知识
   - 模型从数据中精炼

4. 因果世界模型 + 安全 RL
   - 利用因果推理保证安全性
   - 预测干预的副作用
```

---

## 8. 参考资料

### 书籍

- Pearl, J. "Causality: Models, Reasoning, and Inference" (2009)
- Peters, J., Janzing, D., & Scholkopf, B. "Elements of Causal Inference" (2017)
- Pearl, J. & Mackenzie, D. "The Book of Why" (2018)

### 论文

- "Causal Confusion in Imitation Learning" (de Haan et al., 2019)
- "Discovering Causal Structure from Observations" (Spirtes et al., 2000)
- "Invariant Risk Minimization" (Arjovsky et al., 2019)
- "Causal Reasoning from Meta-reinforcement Learning" (Dasgupta et al., 2019)

---

## 9. 总结

### 核心要点

1. **因果 vs 相关**：当前世界模型主要学习相关性，缺乏因果理解
2. **因果层次**：观察 → 干预 → 反事实，能力递增
3. **关键技术**：因果发现、因果表示学习、反事实推理
4. **设计启示**：模块化、不变性、干预数据

### 一句话总结

> **因果世界模型追求理解"为什么"而非仅仅预测"是什么"，这是通向真正世界理解的关键。**
