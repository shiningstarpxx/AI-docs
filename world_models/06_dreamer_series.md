# Dreamer ç³»åˆ—ï¼šä»Ž PlaNet åˆ° DreamerV3

> æ·±å…¥ç†è§£åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ æ¼”è¿›

## ç›®å½•

1. [æ¦‚è¿°ï¼šä¸ºä»€ä¹ˆéœ€è¦ Dreamerï¼Ÿ](#æ¦‚è¿°ä¸ºä»€ä¹ˆéœ€è¦-dreamer)
2. [PlaNet (2019)ï¼šçº¯è§„åˆ’æ–¹æ³•](#planet-2019çº¯è§„åˆ’æ–¹æ³•)
3. [Dreamer (2020)ï¼šæƒ³è±¡ä¸­çš„ç­–ç•¥æ¢¯åº¦](#dreamer-2020æƒ³è±¡ä¸­çš„ç­–ç•¥æ¢¯åº¦)
4. [DreamerV2 (2021)ï¼šç¦»æ•£æ½œåœ¨å˜é‡](#dreamerv2-2021ç¦»æ•£æ½œåœ¨å˜é‡)
5. [DreamerV3 (2023)ï¼šé€šç”¨ä¸–ç•Œæ¨¡åž‹](#dreamerv3-2023é€šç”¨ä¸–ç•Œæ¨¡åž‹)
6. [æ ¸å¿ƒæŠ€æœ¯ï¼šRSSM è¯¦è§£](#æ ¸å¿ƒæŠ€æœ¯rssm-è¯¦è§£)
7. [ä¸Ž World Models å¯¹æ¯”](#ä¸Ž-world-models-å¯¹æ¯”)
8. [å®žè·µæŒ‡å—](#å®žè·µæŒ‡å—)

---

## æ¦‚è¿°ï¼šä¸ºä»€ä¹ˆéœ€è¦ Dreamerï¼Ÿ

### World Models çš„å±€é™æ€§

å›žé¡¾ World Models (Ha & Schmidhuber, 2018) çš„é—®é¢˜ï¼š

| é—®é¢˜ | æè¿° |
|:---|:---|
| **åˆ†é˜¶æ®µè®­ç»ƒ** | VAE â†’ MDN-RNN â†’ Controller åˆ†å¼€è®­ç»ƒï¼Œæ— æ³•è”åˆä¼˜åŒ– |
| **ç®€å•æŽ§åˆ¶å™¨** | çº¿æ€§ç­–ç•¥ä»… 867 å‚æ•°ï¼Œè¡¨è¾¾èƒ½åŠ›æœ‰é™ |
| **CMA-ES ä¼˜åŒ–** | æ— æ¢¯åº¦æ–¹æ³•ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤æ‚ç­–ç•¥ |
| **è¯¯å·®ç´¯ç§¯** | é•¿è½¨è¿¹ rollout æ—¶æ¨¡åž‹è¯¯å·®é€æ­¥æ”¾å¤§ |

### Dreamer ç³»åˆ—çš„æ”¹è¿›æ€è·¯

```
World Models (2018)          Dreamer ç³»åˆ—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åˆ†é˜¶æ®µè®­ç»ƒ       â”€â”€â”€â”€â”€â”€â–º     ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ
VAE + MDN-RNN    â”€â”€â”€â”€â”€â”€â–º     RSSM (æ›´å¼ºçš„åŠ¨æ€æ¨¡åž‹)
çº¿æ€§ Controller   â”€â”€â”€â”€â”€â”€â–º     ç¥žç»ç½‘ç»œ Actor-Critic
CMA-ES (æ— æ¢¯åº¦)   â”€â”€â”€â”€â”€â”€â–º     åå‘ä¼ æ’­ (æœ‰æ¢¯åº¦)
```

---

## PlaNet (2019)ï¼šçº¯è§„åˆ’æ–¹æ³•

> ðŸ“„ "Learning Latent Dynamics for Planning from Pixels" - Hafner et al.

### æ ¸å¿ƒæ€æƒ³

**ä¸å­¦ä¹ ç­–ç•¥ï¼Œç›´æŽ¥åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­è§„åˆ’ï¼**

```
ä¼ ç»Ÿæ–¹æ³•: å­¦ä¹  Ï€(a|s) ç­–ç•¥ç½‘ç»œ
PlaNet:   åœ¨æƒ³è±¡ä¸­æœç´¢æœ€ä¼˜åŠ¨ä½œåºåˆ—
```

### æž¶æž„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PlaNet                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  è§‚æµ‹ o_t â”€â”€â–º [Encoder] â”€â”€â–º æ½œåœ¨çŠ¶æ€ s_t                    â”‚
â”‚                                â”‚                            â”‚
â”‚                                â–¼                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚              â”‚         RSSM åŠ¨æ€æ¨¡åž‹            â”‚            â”‚
â”‚              â”‚                                 â”‚            â”‚
â”‚              â”‚  ç¡®å®šæ€§: h_t = f(h_{t-1}, s_t, a_t)          â”‚
â”‚              â”‚  éšæœºæ€§: s_t ~ p(s_t | h_t)     â”‚            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                â”‚                            â”‚
â”‚                                â–¼                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚              â”‚       CEM è§„åˆ’å™¨                 â”‚            â”‚
â”‚              â”‚  åœ¨æƒ³è±¡ä¸­å±•å¼€å¤šæ¡è½¨è¿¹            â”‚            â”‚
â”‚              â”‚  é€‰æ‹©ç´¯ç§¯å¥–åŠ±æœ€é«˜çš„åŠ¨ä½œ          â”‚            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                â”‚                            â”‚
â”‚                                â–¼                            â”‚
â”‚                           åŠ¨ä½œ a_t                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RSSM: Recurrent State-Space Model

**PlaNet å¼•å…¥çš„æ ¸å¿ƒåˆ›æ–°ï¼**

```
World Models:  çº¯éšæœº z_t
               z_t â”€â”€â–º LSTM â”€â”€â–º z_{t+1}

RSSM:          ç¡®å®šæ€§ h_t + éšæœºæ€§ s_t
               h_t â”€â”€â–º GRU â”€â”€â–º h_{t+1}  (ç¡®å®šæ€§è·¯å¾„ï¼Œè®°å¿†)
                 â”‚              â”‚
                 â–¼              â–¼
               s_t ~ p(s|h_t)  s_{t+1} ~ p(s|h_{t+1})  (éšæœºæ€§)
```

**ä¸ºä»€ä¹ˆéœ€è¦åŒè·¯å¾„ï¼Ÿ**

| è·¯å¾„ | ä½œç”¨ | è§£å†³çš„é—®é¢˜ |
|:---|:---|:---|
| ç¡®å®šæ€§ h | é•¿æœŸè®°å¿†ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤± | RNN é—å¿˜é—®é¢˜ |
| éšæœºæ€§ s | å»ºæ¨¡ä¸ç¡®å®šæ€§ã€å¤šæ¨¡æ€ | çŽ¯å¢ƒéšæœºæ€§ |

### CEM (Cross-Entropy Method) è§„åˆ’

```python
def cem_planning(world_model, current_state, horizon=12):
    """
    äº¤å‰ç†µæ–¹æ³•ï¼šåœ¨æƒ³è±¡ä¸­æœç´¢æœ€ä¼˜åŠ¨ä½œåºåˆ—
    """
    # åˆå§‹åŒ–åŠ¨ä½œåˆ†å¸ƒ
    mean = zeros(horizon, action_dim)
    std = ones(horizon, action_dim)

    for iteration in range(10):  # è¿­ä»£ä¼˜åŒ–
        # 1. é‡‡æ · N æ¡åŠ¨ä½œåºåˆ—
        action_sequences = sample_gaussian(mean, std, n_samples=1000)

        # 2. åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­å±•å¼€ï¼Œè®¡ç®—ç´¯ç§¯å¥–åŠ±
        rewards = []
        for actions in action_sequences:
            total_reward = imagine_rollout(world_model, current_state, actions)
            rewards.append(total_reward)

        # 3. é€‰æ‹© top-K ç²¾è‹±
        elite_idx = argsort(rewards)[-100:]  # top 10%
        elite_actions = action_sequences[elite_idx]

        # 4. æ›´æ–°åˆ†å¸ƒ
        mean = elite_actions.mean(axis=0)
        std = elite_actions.std(axis=0)

    # è¿”å›žç¬¬ä¸€ä¸ªåŠ¨ä½œ
    return mean[0]
```

### PlaNet çš„ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**
- ä¸éœ€è¦å­¦ä¹ ç­–ç•¥ç½‘ç»œ
- å¯ä»¥é€‚åº”å¥–åŠ±å‡½æ•°å˜åŒ–
- MPC é£Žæ ¼ï¼Œå®žæ—¶è§„åˆ’

**ç¼ºç‚¹ï¼š**
- æ¯æ­¥éƒ½éœ€è¦è§„åˆ’ï¼Œè®¡ç®—é‡å¤§
- è§„åˆ’è§†é‡Žæœ‰é™ (é€šå¸¸ 12-50 æ­¥)
- éš¾ä»¥å¤„ç†é•¿æœŸä»»åŠ¡

---

## Dreamer (2020)ï¼šæƒ³è±¡ä¸­çš„ç­–ç•¥æ¢¯åº¦

> ðŸ“„ "Dream to Control: Learning Behaviors by Latent Imagination" - Hafner et al.

### æ ¸å¿ƒæ”¹è¿›

**ä»Ž "è§„åˆ’" åˆ° "å­¦ä¹ "ï¼šåœ¨æƒ³è±¡ä¸­è®­ç»ƒ Actor-Criticï¼**

```
PlaNet:   æ¯æ­¥è§„åˆ’ â”€â”€â–º åŠ¨ä½œ
Dreamer:  å­¦ä¹ ç­–ç•¥ Ï€(a|s) å’Œä»·å€¼å‡½æ•° V(s)
          åœ¨æƒ³è±¡è½¨è¿¹ä¸Šç”¨æ¢¯åº¦æ›´æ–°
```

### æž¶æž„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Dreamer                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ World Model â”‚  RSSM + Reward Model + Continue Model      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚         â”‚                                                   â”‚
â”‚         â”‚ æƒ³è±¡è½¨è¿¹                                           â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚        Imagination Rollout          â”‚                    â”‚
â”‚  â”‚  s_t â†’ s_{t+1} â†’ ... â†’ s_{t+H}      â”‚                    â”‚
â”‚  â”‚  ç”¨ä¸–ç•Œæ¨¡åž‹å±•å¼€ H æ­¥                 â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Actor     â”‚      â”‚   Critic    â”‚                       â”‚
â”‚  â”‚  Ï€(a|s)     â”‚      â”‚   V(s)      â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â”‚                    â”‚                              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                â”‚                                            â”‚
â”‚                â–¼                                            â”‚
â”‚         ç­–ç•¥æ¢¯åº¦æ›´æ–° (åœ¨æƒ³è±¡ä¸­ï¼)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®­ç»ƒæµç¨‹

```python
def dreamer_training_step():
    """Dreamer è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µ"""

    # ========== é˜¶æ®µ 1: ä¸–ç•Œæ¨¡åž‹å­¦ä¹  ==========
    # ä»Ž replay buffer é‡‡æ ·çœŸå®žè½¨è¿¹
    batch = replay_buffer.sample()

    # è®­ç»ƒ RSSMã€å¥–åŠ±æ¨¡åž‹ã€è§£ç å™¨
    world_model_loss = (
        reconstruction_loss +      # é‡å»ºè§‚æµ‹
        kl_divergence +           # æ­£åˆ™åŒ–æ½œåœ¨ç©ºé—´
        reward_prediction_loss +   # é¢„æµ‹å¥–åŠ±
        continue_prediction_loss   # é¢„æµ‹æ˜¯å¦ç»ˆæ­¢
    )
    world_model.update(world_model_loss)

    # ========== é˜¶æ®µ 2: æƒ³è±¡è½¨è¿¹ç”Ÿæˆ ==========
    # ä»ŽçœŸå®žçŠ¶æ€å‡ºå‘ï¼Œåœ¨ä¸–ç•Œæ¨¡åž‹ä¸­å±•å¼€
    imagined_states = []
    imagined_rewards = []

    state = world_model.encode(batch.observations[0])
    for t in range(imagination_horizon):  # é€šå¸¸ 15 æ­¥
        action = actor(state)
        next_state = world_model.imagine_step(state, action)
        reward = reward_model(next_state)

        imagined_states.append(state)
        imagined_rewards.append(reward)
        state = next_state

    # ========== é˜¶æ®µ 3: Actor-Critic æ›´æ–° ==========
    # è®¡ç®— Î»-returns (TD(Î») ç›®æ ‡)
    values = critic(imagined_states)
    returns = compute_lambda_returns(imagined_rewards, values, gamma, lambda_)

    # Critic æŸå¤±ï¼šé¢„æµ‹å›žæŠ¥
    critic_loss = (values - returns.detach()).pow(2).mean()

    # Actor æŸå¤±ï¼šæœ€å¤§åŒ–å›žæŠ¥
    actor_loss = -returns.mean()  # ç®€åŒ–ç‰ˆï¼Œå®žé™…è¿˜æœ‰ç†µæ­£åˆ™åŒ–

    actor.update(actor_loss)
    critic.update(critic_loss)
```

### Î»-Returns è®¡ç®—

Dreamer ä½¿ç”¨ TD(Î») æ¥å¹³è¡¡åå·®å’Œæ–¹å·®ï¼š

$$V_t^\lambda = r_t + \gamma \left[ (1-\lambda) V(s_{t+1}) + \lambda V_{t+1}^\lambda \right]$$

```python
def compute_lambda_returns(rewards, values, gamma=0.99, lambda_=0.95):
    """
    è®¡ç®— Î»-returns

    Î»=0: çº¯ TD(0)ï¼Œé«˜åå·®ä½Žæ–¹å·®
    Î»=1: çº¯ Monte Carloï¼Œä½Žåå·®é«˜æ–¹å·®
    Î»=0.95: å¹³è¡¡ç‚¹
    """
    returns = []
    last_value = values[-1]

    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_return = last_value
        else:
            next_return = returns[-1]

        td_target = rewards[t] + gamma * values[t + 1]
        current_return = (1 - lambda_) * td_target + lambda_ * (rewards[t] + gamma * next_return)
        returns.append(current_return)

    return list(reversed(returns))
```

### ä¸Ž World Models çš„å…³é”®åŒºåˆ«

| æ–¹é¢ | World Models | Dreamer |
|:---|:---|:---|
| åŠ¨æ€æ¨¡åž‹ | MDN-RNN (çº¯éšæœº) | RSSM (ç¡®å®š+éšæœº) |
| æŽ§åˆ¶å™¨ | çº¿æ€§ç­–ç•¥ | ç¥žç»ç½‘ç»œ Actor |
| ä»·å€¼ä¼°è®¡ | æ—  | Critic ç½‘ç»œ |
| ä¼˜åŒ–æ–¹æ³• | CMA-ES (æ— æ¢¯åº¦) | åå‘ä¼ æ’­ (æœ‰æ¢¯åº¦) |
| è®­ç»ƒæ–¹å¼ | åˆ†é˜¶æ®µ | ç«¯åˆ°ç«¯è”åˆ |

---

## DreamerV2 (2021)ï¼šç¦»æ•£æ½œåœ¨å˜é‡

> ðŸ“„ "Mastering Atari with Discrete World Models" - Hafner et al.

### æ ¸å¿ƒåˆ›æ–°ï¼šç¦»æ•£æ½œåœ¨ç©ºé—´

**ç”¨åˆ†ç±»åˆ†å¸ƒæ›¿ä»£é«˜æ–¯åˆ†å¸ƒï¼**

```
Dreamer V1:  s_t ~ N(Î¼, ÏƒÂ²)     è¿žç»­é«˜æ–¯
DreamerV2:   s_t ~ Categorical  ç¦»æ•£åˆ†ç±» (32 ä¸ª one-hot å‘é‡ï¼Œæ¯ä¸ª 32 ç±»)
```

### ä¸ºä»€ä¹ˆç¦»æ•£æ›´å¥½ï¼Ÿ

```
è¿žç»­æ½œåœ¨ç©ºé—´çš„é—®é¢˜:
- KL æ•£åº¦éš¾ä»¥å¹³è¡¡
- åŽéªŒåç¼© (posterior collapse)
- è¡¨è¾¾ç¦»æ•£æ¦‚å¿µå›°éš¾ (å¦‚"æœ‰æ•Œäºº/æ²¡æ•Œäºº")

ç¦»æ•£æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿:
- è‡ªç„¶è¡¨è¾¾ç¦»æ•£æ¦‚å¿µ
- KL æ•£åº¦æ›´ç¨³å®š
- é¿å…åŽéªŒåç¼©
```

### æž¶æž„æ”¹è¿›

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DreamerV2 çŠ¶æ€è¡¨ç¤º                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  çŠ¶æ€ s_t = [h_t, z_t]                                      â”‚
â”‚                                                             â”‚
â”‚  h_t: ç¡®å®šæ€§éšè—çŠ¶æ€ (GRU)                                   â”‚
â”‚  z_t: ç¦»æ•£éšæœºå˜é‡                                          â”‚
â”‚       = [z_t^1, z_t^2, ..., z_t^32]                         â”‚
â”‚       æ¯ä¸ª z_t^i æ˜¯ 32 ç±»çš„ one-hot                          â”‚
â”‚       æ€»å…±: 32 Ã— 32 = 1024 ç§ç»„åˆ                            â”‚
â”‚                                                             â”‚
â”‚  Straight-Through æ¢¯åº¦:                                     â”‚
â”‚  å‰å‘: argmax (ç¦»æ•£é‡‡æ ·)                                     â”‚
â”‚  åå‘: ç›´æŽ¥ä¼ é€’æ¢¯åº¦åˆ° logits                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### KL Balancing

DreamerV2 å¼•å…¥äº† KL å¹³è¡¡æŠ€å·§ï¼š

$$\mathcal{L}_{KL} = \alpha \cdot KL[sg(q) \| p] + (1-\alpha) \cdot KL[q \| sg(p)]$$

- $sg()$: stop gradient
- $\alpha = 0.8$: åå‘è®© prior æ‹Ÿåˆ posterior

```python
def kl_balance(posterior, prior, alpha=0.8):
    """
    KL å¹³è¡¡: è®©æ¨¡åž‹å­¦ä¹ æ›´å¥½çš„ prior
    """
    kl_to_prior = kl_divergence(posterior.detach(), prior)  # æ›´æ–° prior
    kl_to_posterior = kl_divergence(posterior, prior.detach())  # æ›´æ–° posterior

    return alpha * kl_to_prior + (1 - alpha) * kl_to_posterior
```

### æ€§èƒ½çªç ´

| çŽ¯å¢ƒ | DreamerV1 | DreamerV2 | äººç±» |
|:---|:---|:---|:---|
| Atari (55 games) | 115% | **200%** | 100% |
| æ ·æœ¬æ•ˆçŽ‡ | 200M æ­¥ | **200M æ­¥** | - |

**é¦–æ¬¡åœ¨ Atari ä¸Šè¶…è¶Šäººç±»çš„ Model-Based æ–¹æ³•ï¼**

---

## DreamerV3 (2023)ï¼šé€šç”¨ä¸–ç•Œæ¨¡åž‹

> ðŸ“„ "Mastering Diverse Domains through World Models" - Hafner et al.

### ç›®æ ‡ï¼šä¸€ä¸ªç®—æ³•ï¼Œæ‰€æœ‰ä»»åŠ¡

```
ä¹‹å‰: æ¯ä¸ªä»»åŠ¡éœ€è¦è°ƒå‚
DreamerV3: å›ºå®šè¶…å‚æ•°ï¼Œé€‚ç”¨äºŽæ‰€æœ‰ä»»åŠ¡
           - Atari æ¸¸æˆ
           - DMControl è¿žç»­æŽ§åˆ¶
           - Minecraft
           - æœºå™¨äºº
```

### å…³é”®æŠ€æœ¯

#### 1. Symlog é¢„æµ‹

å¤„ç†ä¸åŒå°ºåº¦çš„å¥–åŠ±ï¼š

$$\text{symlog}(x) = \text{sign}(x) \cdot \ln(|x| + 1)$$

```python
def symlog(x):
    """å¯¹ç§°å¯¹æ•°å˜æ¢ï¼Œå¤„ç†æ­£è´Ÿå€¼"""
    return torch.sign(x) * torch.log(torch.abs(x) + 1)

def symexp(x):
    """é€†å˜æ¢"""
    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)
```

#### 2. å½’ä¸€åŒ–æŠ€æœ¯

```python
# å±‚å½’ä¸€åŒ–æ›¿ä»£æ‰¹å½’ä¸€åŒ–
# æ›´ç¨³å®šï¼Œä¸ä¾èµ– batch ç»Ÿè®¡

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(parameters, max_norm=100)

# å›žæŠ¥å½’ä¸€åŒ–
returns = (returns - returns.mean()) / (returns.std() + 1e-8)
```

#### 3. Free Bits

é˜²æ­¢ KL æ•£åº¦è¿‡åº¦æ­£åˆ™åŒ–ï¼š

$$\mathcal{L}_{KL} = \max(KL, \text{free\_bits})$$

```python
def free_bits_kl(kl, free_bits=1.0):
    """åªæœ‰è¶…è¿‡ free_bits çš„ KL æ‰ä¼šè¢«æƒ©ç½š"""
    return torch.max(kl, torch.tensor(free_bits))
```

### å®Œæ•´æž¶æž„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DreamerV3                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    World Model                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚   Encoder   â”‚  â”‚    RSSM     â”‚  â”‚   Decoder   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  CNN/MLP    â”‚  â”‚ ç¡®å®š+ç¦»æ•£   â”‚  â”‚  CNN/MLP    â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â”‚         â”‚                â”‚                â”‚               â”‚   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â”‚                          â”‚                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚Reward Model â”‚  â”‚Continue Predâ”‚  â”‚  Dynamics   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  symlog     â”‚  â”‚  Bernoulli  â”‚  â”‚   Prior     â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â”‚ æƒ³è±¡è½¨è¿¹                          â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Actor-Critic                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
â”‚  â”‚  â”‚   Actor     â”‚              â”‚   Critic    â”‚            â”‚   â”‚
â”‚  â”‚  â”‚  Ï€(a|s)     â”‚              â”‚   V(s)      â”‚            â”‚   â”‚
â”‚  â”‚  â”‚ + Entropy   â”‚              â”‚  symlog     â”‚            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ€§èƒ½è¡¨çŽ°

| é¢†åŸŸ | ä»»åŠ¡ | DreamerV3 |
|:---|:---|:---|
| **Atari 100k** | 26 æ¸¸æˆ | SOTA |
| **DMControl** | è¿žç»­æŽ§åˆ¶ | SOTA |
| **Minecraft** | æ”¶é›†é’»çŸ³ | **é¦–æ¬¡æˆåŠŸ** |
| **BSuite** | è¯Šæ–­ä»»åŠ¡ | é«˜åˆ† |

---

## æ ¸å¿ƒæŠ€æœ¯ï¼šRSSM è¯¦è§£

### RSSM æ•°å­¦å½¢å¼

$$
\begin{aligned}
\text{ç¡®å®šæ€§è·¯å¾„:} \quad & h_t = f_\theta(h_{t-1}, z_{t-1}, a_{t-1}) \\
\text{å…ˆéªŒ (Prior):} \quad & p_\theta(z_t | h_t) \\
\text{åŽéªŒ (Posterior):} \quad & q_\phi(z_t | h_t, o_t) \\
\text{è§‚æµ‹è§£ç :} \quad & p_\theta(o_t | h_t, z_t) \\
\text{å¥–åŠ±é¢„æµ‹:} \quad & p_\theta(r_t | h_t, z_t)
\end{aligned}
$$

### å›¾è§£

```
æ—¶é—´æ­¥ t-1              æ—¶é—´æ­¥ t                æ—¶é—´æ­¥ t+1
    â”‚                      â”‚                       â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
    â”‚   â”‚                  â”‚                       â”‚
    â–¼   â–¼                  â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”  a_{t-1}    â”Œâ”€â”€â”€â”€â”€â”€â”€â”   a_t        â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ h_{t-1}â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  h_t  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚h_{t+1}â”‚  ç¡®å®šæ€§
â””â”€â”€â”€â”¬â”€â”€â”€â”˜            â””â”€â”€â”€â”¬â”€â”€â”€â”˜               â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚                    â”‚                       â”‚
    â”‚                    â”‚                       â”‚
    â–¼                    â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚z_{t-1}â”‚            â”‚  z_t  â”‚               â”‚z_{t+1}â”‚  éšæœºæ€§
â””â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”¬â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ æ¡ä»¶äºŽ o_t (åŽéªŒ)
                         â”‚ æˆ–ä¸æ¡ä»¶ (å…ˆéªŒ)
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Decoder â”‚ â”€â”€â–º é‡å»º o_t
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PyTorch å®žçŽ°

```python
class RSSM(nn.Module):
    """Recurrent State-Space Model"""

    def __init__(self, hidden_size=256, state_size=32, num_categories=32, action_size=6):
        super().__init__()
        self.hidden_size = hidden_size
        self.state_size = state_size
        self.num_categories = num_categories

        # ç¡®å®šæ€§è·¯å¾„: GRU
        self.gru = nn.GRUCell(
            input_size=state_size * num_categories + action_size,
            hidden_size=hidden_size
        )

        # å…ˆéªŒç½‘ç»œ: h_t -> z_t çš„åˆ†å¸ƒ
        self.prior_net = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ELU(),
            nn.Linear(256, state_size * num_categories)
        )

        # åŽéªŒç½‘ç»œ: (h_t, encoded_obs) -> z_t çš„åˆ†å¸ƒ
        self.posterior_net = nn.Sequential(
            nn.Linear(hidden_size + 256, 256),  # 256 æ˜¯ç¼–ç å™¨è¾“å‡º
            nn.ELU(),
            nn.Linear(256, state_size * num_categories)
        )

    def imagine_step(self, prev_state, action):
        """
        åœ¨æƒ³è±¡ä¸­å‰è¿›ä¸€æ­¥ (åªç”¨å…ˆéªŒï¼Œä¸éœ€è¦è§‚æµ‹)
        """
        h_prev, z_prev = prev_state['h'], prev_state['z']

        # ç¡®å®šæ€§è·¯å¾„
        gru_input = torch.cat([z_prev.flatten(-2), action], dim=-1)
        h = self.gru(gru_input, h_prev)

        # å…ˆéªŒé‡‡æ ·
        prior_logits = self.prior_net(h).view(-1, self.state_size, self.num_categories)
        z = self.sample_categorical(prior_logits)

        return {'h': h, 'z': z, 'logits': prior_logits}

    def observe_step(self, prev_state, action, encoded_obs):
        """
        è§‚æµ‹ä¸€æ­¥ (ç”¨åŽéªŒï¼Œéœ€è¦çœŸå®žè§‚æµ‹)
        """
        h_prev, z_prev = prev_state['h'], prev_state['z']

        # ç¡®å®šæ€§è·¯å¾„
        gru_input = torch.cat([z_prev.flatten(-2), action], dim=-1)
        h = self.gru(gru_input, h_prev)

        # å…ˆéªŒ
        prior_logits = self.prior_net(h).view(-1, self.state_size, self.num_categories)

        # åŽéªŒ (æ¡ä»¶äºŽè§‚æµ‹)
        posterior_input = torch.cat([h, encoded_obs], dim=-1)
        posterior_logits = self.posterior_net(posterior_input).view(-1, self.state_size, self.num_categories)
        z = self.sample_categorical(posterior_logits)

        return {
            'h': h,
            'z': z,
            'prior_logits': prior_logits,
            'posterior_logits': posterior_logits
        }

    def sample_categorical(self, logits, straight_through=True):
        """
        ä»Žåˆ†ç±»åˆ†å¸ƒé‡‡æ ·
        ä½¿ç”¨ Straight-Through æ¢¯åº¦
        """
        probs = F.softmax(logits, dim=-1)

        if straight_through:
            # å‰å‘: argmax (ç¦»æ•£)
            indices = probs.argmax(dim=-1)
            one_hot = F.one_hot(indices, self.num_categories).float()
            # åå‘: ä½¿ç”¨ probs çš„æ¢¯åº¦
            z = one_hot + probs - probs.detach()
        else:
            # Gumbel-Softmax (å¯å¾®è¿‘ä¼¼)
            z = F.gumbel_softmax(logits, hard=True)

        return z
```

---

## ä¸Ž World Models å¯¹æ¯”

### æž¶æž„å¯¹æ¯”

```
World Models (2018)              Dreamer ç³»åˆ—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VAE Encoder   â”‚             â”‚   CNN Encoder   â”‚
â”‚   (ç‹¬ç«‹è®­ç»ƒ)    â”‚             â”‚   (è”åˆè®­ç»ƒ)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MDN-RNN      â”‚             â”‚      RSSM       â”‚
â”‚  çº¯éšæœºçŠ¶æ€ z   â”‚             â”‚  h (ç¡®å®š) + z   â”‚
â”‚  é«˜æ–¯æ··åˆè¾“å‡º   â”‚             â”‚  ç¦»æ•£/è¿žç»­ z    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear Controllerâ”‚             â”‚  Actor-Critic  â”‚
â”‚   (867 å‚æ•°)    â”‚             â”‚  (ç¥žç»ç½‘ç»œ)     â”‚
â”‚    CMA-ES       â”‚             â”‚  ç­–ç•¥æ¢¯åº¦       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ€§èƒ½å¯¹æ¯”

| æ–¹é¢ | World Models | Dreamer V1 | DreamerV2 | DreamerV3 |
|:---|:---|:---|:---|:---|
| åŠ¨æ€æ¨¡åž‹ | MDN-RNN | RSSM (è¿žç»­) | RSSM (ç¦»æ•£) | RSSM (ç¦»æ•£) |
| æŽ§åˆ¶å™¨ | çº¿æ€§ | ç¥žç»ç½‘ç»œ | ç¥žç»ç½‘ç»œ | ç¥žç»ç½‘ç»œ |
| Atari | æœªæµ‹è¯• | 115% | 200% | SOTA |
| DMControl | - | SOTA | SOTA | SOTA |
| è®­ç»ƒæ–¹å¼ | åˆ†é˜¶æ®µ | ç«¯åˆ°ç«¯ | ç«¯åˆ°ç«¯ | ç«¯åˆ°ç«¯ |
| è¶…å‚æ•° | éœ€è°ƒæ•´ | éœ€è°ƒæ•´ | éœ€è°ƒæ•´ | **å›ºå®š** |

### æ ¸å¿ƒåŒºåˆ«æ€»ç»“

| ç»´åº¦ | World Models | Dreamer |
|:---|:---|:---|
| **çŠ¶æ€è¡¨ç¤º** | çº¯éšæœº z | ç¡®å®šæ€§ h + éšæœº z |
| **åŠ¨æ€æ¨¡åž‹** | LSTM + MDN | GRU + å…ˆéªŒ/åŽéªŒç½‘ç»œ |
| **ç­–ç•¥ä¼˜åŒ–** | æ— æ¢¯åº¦ (CMA-ES) | æœ‰æ¢¯åº¦ (ç­–ç•¥æ¢¯åº¦) |
| **ä»·å€¼ä¼°è®¡** | æ—  | Critic ç½‘ç»œ |
| **è®­ç»ƒæµç¨‹** | V â†’ M â†’ C | è”åˆä¼˜åŒ– |
| **æƒ³è±¡é•¿åº¦** | æ•°ç™¾æ­¥ | 15-50 æ­¥ |

---

## å®žè·µæŒ‡å—

### çŽ¯å¢ƒé€‰æ‹©

**å…¥é—¨:**
- DMControl Suite (cartpole, cheetah, walker)
- Atari 100k benchmark

**è¿›é˜¶:**
- Minecraft (å¼€æ”¾ä¸–ç•Œ)
- çœŸå®žæœºå™¨äºº (DayDreamer)

### å®žçŽ°å»ºè®®

```python
# æŽ¨èçš„ DreamerV3 è¶…å‚æ•° (è®ºæ–‡å›ºå®šå€¼)
config = {
    # ä¸–ç•Œæ¨¡åž‹
    'hidden_size': 512,
    'state_size': 32,
    'num_categories': 32,

    # Actor-Critic
    'actor_lr': 3e-5,
    'critic_lr': 3e-5,
    'imagination_horizon': 15,

    # è®­ç»ƒ
    'batch_size': 16,
    'sequence_length': 64,
    'lambda_': 0.95,
    'gamma': 0.997,

    # æ­£åˆ™åŒ–
    'kl_free_bits': 1.0,
    'kl_scale': 0.1,
}
```

### è°ƒè¯•æŠ€å·§

1. **å…ˆéªŒè¯ä¸–ç•Œæ¨¡åž‹è´¨é‡**
   - å¯è§†åŒ–é‡å»ºå›¾åƒ
   - æ£€æŸ¥ KL æ•£åº¦æ›²çº¿
   - æµ‹è¯•é¢„æµ‹å‡†ç¡®åº¦

2. **ç›‘æŽ§æƒ³è±¡è½¨è¿¹**
   - å¯è§†åŒ–æƒ³è±¡ä¸­çš„çŠ¶æ€
   - æ£€æŸ¥å¥–åŠ±é¢„æµ‹
   - éªŒè¯é•¿æœŸä¸€è‡´æ€§

3. **Actor-Critic ç¨³å®šæ€§**
   - ç›‘æŽ§ Actor/Critic loss
   - æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
   - å¯è§†åŒ–ä»·å€¼ä¼°è®¡

### å‚è€ƒå®žçŽ°

- **å®˜æ–¹ DreamerV3**: https://github.com/danijar/dreamerv3
- **PyTorch å¤çŽ°**: https://github.com/NM512/dreamerv3-torch
- **JAX å®žçŽ°**: https://github.com/google-deepmind/dreamer

---

## å‚è€ƒæ–‡çŒ®

1. **PlaNet** (2019): "Learning Latent Dynamics for Planning from Pixels"
   - https://arxiv.org/abs/1811.04551

2. **Dreamer** (2020): "Dream to Control: Learning Behaviors by Latent Imagination"
   - https://arxiv.org/abs/1912.01603

3. **DreamerV2** (2021): "Mastering Atari with Discrete World Models"
   - https://arxiv.org/abs/2010.02193

4. **DreamerV3** (2023): "Mastering Diverse Domains through World Models"
   - https://arxiv.org/abs/2301.04104

5. **World Models** (2018): "World Models"
   - https://arxiv.org/abs/1803.10122

---

> **ä¸‹ä¸€æ­¥**: RSSM æ•°å­¦æŽ¨å¯¼è¯¦è§£ â†’ `07_rssm_math.md`
